element_idx,keywords,summarized_text
4,"adversar-, generative, perceptrons, ial, process, multilayer, models",We propose a new framework for estimating generative models via an adversarial process . We train two models: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G . This framework corresponds to a
6,"deep, learning, sensory, input, rich","The promise of deep learning is to discover rich, hierarchical models that represent probability distributions over the kinds of data encountered in artificial intelligence applications . These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units which have a particularly well-behaved gradient Deep "
7,"nets, generative, model, genuine, adversarial","the generative model can be thought of as analogous to a team of counterfeiters trying to produce fake currency and use it without detection . the discriminative model is similar to the police, trying to detect the counterfeit currency ."
11,"perceptron, nets, generative, model, multilayer, adversarial","In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron . In this case, we can train both models using only the highly successful backpropagation and dropout algorithms ."
13,"graphical, carlo, markov, variables, models, monte, machines, latent, boltzmann, chain, with","an alternative to directed graphical models with latent variables are undirected models with restricted Boltzmann machines . The interactions within such models are represented as the product of unnormalized potential functions, normalized by a global summation/integration over all states of the random variables ."
15,"matching, score, estimation, noise-contrastive, nce","In many interesting generative models with several layers of latent variables, it is not even possible to derive a tractable unnormalized probability density . In NCE, as in this work, a discriminative training criterion is employed to fit a generative model . However, rather than fitting a"
16,"stochastic, generative, gsn, network, backpropagation","some techniques do not involve defining a probability distribution explicitly, but rather train a generative machine to draw samples from the desired distribution . This approach has the advantage that such machines can be designed to be trained by back-propagation . Prominent recent work in this area includes the generative stochastic network framework"
18,"perceptron, modeling, framework, multilayer, adversarial","The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons . To learn the generator's distribution Pg over data x, we define a prior on input noise variables pz, then represent a mapping to data space as G, where G is a differentiable"
21,"nets, criterion, d, training, adversarial, optimizing","In the next section, we present a theoretical analysis of adversarial nets . In practice, we must implement the game using an iterative, numerical approach . Optimizing D to completion in the inner loop of training is computationally prohibitive . Instead, we alternate between k steps of optimizing D and one"
22,"d, g, g-d","Equation 1 may not provide sufficient gradient for G to learn well . Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data ."
26,"generator, density, g, probability, distribution","The generator G defines a probability distribution Pg as the distribution of the samples G obtained when z  Pz . Therefore, we would like Algorithm 1 to converge to a good estimator of Pdata, if given enough capacity and training time ."
52,"convex, gradient, update, descent, proof, function",Consider V = U as a function of Pg as done in the above criterion . Note that U is convex in Pg . The subderivatives include the derivative of the function at the point where the maximum is attained.
53,"perceptron, nets, multilayer, adversarial","In practice, adversarial nets represent a limited family of Pg distributions . Using a multilayer perceptron to define G introduces multiple critical points in parameter space ."
55,"nets, activations, dropout, adversarial, noise","We trained adversarial nets an array of datasets including MNIST, the Toronto Face Database , and CIFAR-10 . The discriminator net used maxout activations and rectifier linear activations ."
60,"gaussians, generative, validation, set, models",this procedure was introduced in Breuleux et al. and used for various generative models for which the exact likelihood is not tractable . This method of estimating the likelihood has somewhat high variance .
70,"nets, modeling, adversarial, generative",This new framework comes with advantages and disadvantages relative to previous modeling frameworks . The disadvantages are primarily that there is no explicit representation of pg .
71,"models, adversarial, generator, network","Adversarial models may gain some statistical advantage from the generator network not being updated directly with data examples . This means that components of the input are not copied directly into the generator's parameters . Another advantage of adversarial networks is that they can represent very sharp, even degenerate distributions ."
74,"inference, wake-sleep, algorithm, network, approximate",Learned approximate inference can be performed by training an auxiliary network to predict z given x . This is similar to the inference net trained by the wake-sleep algorithm .
76,"conditional, training, model","one can approximately model conditionals p where S is a subset of the indices of x . Essentially, one can use adversarial nets to implement a stochastic extension of the MP-DBM . features from the discriminator or inference net could improve performance of classifiers when"
79,"latex, theano, pylearn2, feature",Yann Dauphin shared his Parzen window evaluation code with us . We would like to thank the developers of Pylearn2 and Theano . Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning.
82,"acoustic, improvement, backprop, modeling, speed, speed-contrastive, estimation","Bengio, Y., Yao, L., Alain, G., and Vincent, P. . Deep generative stochastic networks trainable by backprop . In Proceedings of the 30th International Conference on Machine Learning ."
84,"deep, belief, nets, dynamical, features, systems, robust","Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. . Improving neural networks by preventing co-adaptation of feature detectors . Technical report, University of Toronto."
