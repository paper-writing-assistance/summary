element_idx,keywords,summarized_text
6,"of, language, evaluation, natural, human, llm, texts, pro-cessing, quality","Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans . However, human evaluation is very difficult to reproduce and its quality is notoriously unstable . Recently, large language models have demonstrated exceptional performance on unseen tasks when only the task instructions are provided . We present the L"
8,"nlp, evaluation, human, metrics",human evaluation is an important method to understand the performance of an NLP model or algorithm . Human evaluation is prevalent and indispensable in NLP . researchers use humans to rate the quality of output of NLP models .
9,"nlp, workforces, low-quality, evaluation, human, systems","Reproducibility is another issue in human evaluation since itis hard to recruit the same human evaluators and rerun the same evaluation . Even if the same workers are recruited, the workers that have already done the task are likely to produce a different evaluation result ."
10,"instructives, natural, llm, large, language, models","To resolve some of the drawbacks, we take advantage of large language models . The ability to perform a task just given the task instructions motivates us to ask if these LLMs can perform what humans do in human evaluation . To answer this question, we feed in the LLM with the same instruction, sample,"
20,"corpora, natural, llm, language, models","Large language models have bulk parameter sizes, typically on the scale of a few billion . These LLMs show exceptional performance on unseen tasks . This kind of ability is called zero-shot in-context learning ."
22,"feedback, human, llm",InstructGPT is fine-tuned from GPT-3 using reinforcement learning from human feedback . ChatGPT can interact with users in a conversational way . We ask whether LLMs can be used as an alternative to human evaluation .
25,"of, samples, the, llm, quality",Different tasks use different sets of task instructions . Each task uses different questions to evaluate the quality of samples . The instructions and questions used in LLM evaluation are not tailored for the LLMs .
26,"evaluation, human, llm","LLM evaluation compares results with human evaluation conducted by English teachers . To make a fair and meaningful comparison, the instructions, samples, and questions are formatted similarly to those in human evaluation . In human evaluation, the human evaluators answer the question by choosing the answer from a pre-defined set of options "
34,"human-written, generation, model, stories, gpt-2, sampling, nucleus","The story generation model is GPT-2 medium model fine-tuned on the WritingPrompts training dataset . After the model is trained, we randomly select 200 prompts from the testing set of Wri tingPromptes . For the human-written stories to be compared, we postprocess the human"
42,"instructgpt, openai, chatgpt, questions",We query the InstructGPT using the official API provided by OpenAI . We sample three answers from LLMs to stimulate the result of asking the model to rate the same story three times.
43,"human-written, amt, evaluation, human, stories","Karpinska et al. has already shown that the results obtained using AMT are highly questionable . We hire three certified English teachers using an online freelancer platform, UpWork . Each LLM and each English teacher rates the 200 human-written stories and 200 GPT-2-generated stories ."
47,"likability, evaluators, evaluation, human, llm, texts, model-generated","Human evaluation result serves as some kind of ground truth of the LLM evaluation . For all four attributes, teachers rate the humanwritten stories higher than GPT-2-generated stories . This indicates that experts are able to distinguish the quality difference between model-generated and human-written stories. Based on the IAA, we also find"
50,"re, model-generated, stories, relevance","Text-curie-001 do not rate humanwritten stories higher than model-generated stories . It can also be observed that for T0, the IAA in terms of the percentage of exact agreement among three different sampled answers is overall very low . The result implies that T0 does not assign a high probability to a"
51,"amt, text-curie-001, text-davinci-003",text-davinci-003 rates human-written stories much higher than model-generated stories on all four attributes . This is in accordance with the result produced by human experts .
53,"likability, rating, chatgpt","ChatGPT is able to provide a detailed explanation of why it gives a certain rating . It will reference the sentences in the stories and prompts to support its rating. In such cases, we regenerate the response until it gives the rating."
54,"explanation, rating, grammaticality, on, chatgpt",Experts mostly agree with the ratings and explanations of ChatGPT3 . We randomly select the answers on four stories by chatGPT and ask the English teachers if they agree with their reasoning and rating . One teacher told us she cannot agree with ChatGPt's rating on grammaticality . This shows that
55,"text-davinci-003, rating, chatgpt",text-davinci-003 tends to give higher ratings compared with human rating . ChatGPT is more fastidious and prone to give lower scores . The absolute number reflects the bias or belief of the evaluator.
61,"score, coefficient, t, text-davinci-003, inter-annotator, kendall's, correlation, agreement",We calculate Kendall's T correlation coefficient between the ratings of text-davinci-003 and English teachers . We choose to use the correlation coefficient instead of the inter-annotator agreement score because IAA mainly cares if two annotators agree on the exact ratings . For each story and each rating
62,"t, text-davinci-003, llm, kendall's, ratings","Kendall's T between teacher ratings and LLM ratings is shown in Table 2.4 . We find that for all four attributes and for both human-written and GPT-2-generated stories, we observe weak to strong positive correlations between teachers' ratings and text-davinci-003's ratings . All the"
64,"t, rating, grammaticality, kendall's","We also calculate the average Kendall's T between a pair of English teachers . We find a weak correlation on grammaticality between the rating of two teachers, while the correlation of the rating on relevance is much stronger."
66,"questions, task, llm, instructions",LLMs have been shown to be sensitive to the instructions used to query the LLM sometimes . We experiment with two different instructions by changing the instruction or question in Figure 1 . This is inspired by previous work that reported GPT-3 can yield different results when giving them a persona .
67,"human-written, gpt-2-generated, rates, llm, stories","the scores obtained from different instructions are less than 0.1 . For the other two attributes, the score changes are slightly larger but still in the range of 0.25 . Different instructions do not change the relative ranking of GPT-2-generated and human-written stories ."
71,"generating, answers, llm, sampling, nucleus","We must choose a set of hyperparameters for generation, including the temperature T and probability p used in nucleus sampling . To understand whether different sampling parameters change the LLM evaluation result, we modify the temperature used for sampling and keep the p in nucular sampling fixed to 0.9 when generating the answers from"
72,"rating, lower, score, llm, block",The results of varying T from 1 to 0 are shown in the lower block in Table 3 . The LLM consistently rates human-written stories higher than GPT-2-generated stories . This is expected since lower temperature means less diversity during the LLM sampling .
77,"substitution, testing, synonym, adversarial, attacks","A special type of adversarial attack is called synonym substitution attacks . By replacing words with their synonyms, the semantics of the benign sample should be preserved . In our experiment here, we would like to see if the LLMs can rate the quality of adversaries like human experts ."
81,"fluency, meaning, preservation, llm, sample, strong, base, adversarial","For each SSA, we randomly select 100 pairs of benign and adversarial samples and use LLMs to evaluate their quality . For fluency, we present the LLM with one news title and the following question: How natural and fluent is the text of the news title? The exact instruction and formatting are presented in Append"
84,"benign, fluency, samples, check, preserving, sanity, meaning","We ask the LLM to rate the meaning preserving of two benign samples that are exactly the same . The result of this sanity check is the entry with 1 in Table 4 which is a perfect 5.00 . ChatGPT often says that ""the two titles are identical SO I rate a 5"""
85,"chatgpt, text-davinci-003, adversarial, samples",ChatGPT tends to rate adversarial samples higher than English teachers . We conduct the same experiment using text-davinci-003 and find similar results .
86,"of, benign, activity, texts, quality, adversarial, chatgpt","ChatGPT rates PWWS to be more natural than Textfooler . Interestingly, we find that the rating difference is not seen in the expert human evaluation . Overall, LLM can rank the quality of adversarial texts like most human experts ."
90,"re-, cruitment, evaluation, platform, human, llm","LLM evaluation is more reproducible than human evaluation results . Human evaluation results are hard to reproduce as it is difficult to hire the same group of evaluators . On the contrary, LLM Evaluation does not have such a drawback ."
91,"of, rating, the, evaluation, current, human, llm, example",the evaluation of each sample is independent of each other in LLM evaluation . Humans tend to compare the current sample to the ones they have previously seen and this affects their ratings . The English teachers say it took them some time to calibrate their rating .
95,"llm, ethics, limitations","Limitations and Ethical Considerations of LLM evaluation . First, LLM may possess incorrect factual knowledge, SO it is not suitable to use them . Next, an LLM that is trained to be safe and non-harmful can result in LLMs preferring to generate more positive and upbeat responses ."
96,"evaluation, human, llm",LLM evaluation has the potential to transform the NLP community . Our paper's goal is not to replace human evaluation but to present an alternative option . Both human and human evaluation have their own advantages and disadvantages .
97,"content, inputs, llm, policy, 2023, cer-tain, acl","LLM may decline to assess certain inputs that violate the content policy of the LLM provider . 6We are the first to propose this idea since when we submitted this paper to ACL 2023 on January 13, 2023, we do not find any other paper that explores this idea ."
100,"pre-trained, openai, evaluation, llm, blog, language, models",ChatGPT sometimes generates answers that sound right and plausible but are totally nonsense . OpenAI admits that the model's response may be sensitive to the prompt used to query the model .
101,"tasks, openai, emotion-related, blog, emotion","Whether AI models have emotion is a philosophical question and is controversial . ChatGPT often replies ""I am an AI system and I do not have emotions like a human"" the results of using such models may be strongly challenged and violate research ethics ."
102,"llm, visual, cues","LLMs lack the ability to process visual cues in task instructions, unlike human evaluation . Human evaluators can use formattings such as special fonts or text styles to focus on important parts of instructions ."
104,"of, ethics, evaluation, human, llm",Is it ethical to replace human evaluation with LLM evaluation? Some may question if this paper is suggesting that LLMs are now ready to replace humans . We aim to offer an alternative option to human evaluation .
105,"evaluation, human, llm","Human evaluation is still essential as the ultimate goal of NLP systems is to be used by human users . We do not recommend that future researchers completely eliminate human evaluation; rather, we believe that human evaluation should be used in conjunction with LLM evaluation . Both methods have their own advantages and disadvantages, making them both necessary for evaluating"
106,"code, of, statements, board, ethics, ethical, review, acl","Ethical statements on the experiments in the paper All the experiments strictly follow the ACL Code of Ethics . We include the exact instructions and screenshots of the interface in the human evaluation, and we report how the evaluators are recruited ."
109,"appendix, feedback, a, valuable",We want to thank reviews for providing detailed feedback and actionable suggestions . We list the modification based on the reviewers' suggestions in Appendix A .
120,"analysis, text, generated, evaluation, natural, language",All that's 'human' is not gold: Evaluating human evaluation of generated text . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.
121,"deep, understanding, transformers, bert, language, bidirectional","BERT: Pre-training of deep bidirectional transformers for language understanding . Association for Computational Linguistics: Human Language Technologies, Volume 1 ."
137,"story, grounding, generation, neural, sense, common, emnlp",Improving neural story generation by targeted common sense grounding . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing .
145,"prompted, multitask, analysis, training","Multitask prompted training enables zero-shot task generalization . In International Conference on Learning Representations, 2022 . ."
149,"language, natural, processing","Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz ."
156,"individual, evaluators, human, llm, stories",We add Section 3.3.1 to discuss whether LLM and human evaluators agree on the rating of individual stories . We refine the wordings in Section 5 and add relevant references .
164,"writingprompts, stories, model-generated, truncation","Once the model is trained, we randomly select 200 prompts from the testing set of WritingPrompts . We feed the prompts to the trained model and ask the model to generate stories based on the given prompts. After this process, we have 200 pairs of prompts and model-generated stories."
165,"writingprompts, stories, tokenization, model-generated, dataset","We select the same 200 prompts used for generating model-generated stories . For these human-written stories, we also truncate the stories to less than 150 words and end with a full sentence ."
168,"teacher, esl, certificates, education",Each teacher is asked to rate 200 GPT-2-generated stories and 200 human-written stories . This makes the hourly wage at least US$28 .
172,"human-written, preference, toward, gpt-2-generated, stories, grammaticality, clear","The reason we do not mix human-written and GPT-2-generated stories for rating is that in Karpinska et al. their observation is that when AMT workers rate model-generated and Human-written stories separately, their ratings do not show preference toward human-writing stories . We conduct the same experiment by randomly mixing"
182,"wood, bug's, way, bug, appendages","The bug sat on a hill, in a region named Alola, a place that had never been visited by any new human inhabitants . It inhabited an entire continent, six miles wide, four miles high, and the most densely populated patch of land on Earth ."
191,"google, forms, evaluation, quality, adversarial, attacks","We create two different Google Forms, one is used to evaluate the fluency . Each page of the Google Form contains one news title ."
212,"likability, value","some teachers say they try not to be affected by personal preference . One teacher asks herself: Did I personally enjoy it based on the amount of sense it made and whether or not it had stylistic flair, humor, or engaging plotting or characterization? Overall, the teachers all try to use a fair and objective view to"
215,"appendix, a, chatgpt, c.3.1, model, ai",We ask the teachers to check the ratings and explanations of ChatGPT . We do not tell the teacher that the rating is done by an AI model .
217,"loneliness, space","The more I focused on it, the more it consumed me . Years passed, and I got more lost in it . A quiet part of me liked to think that they had passed me by . The loneliness. It never felt like such a loss ."
220,son,"Story 2 ""My son is gone. He has been"" People stare at you as you give your explanation and say ""yes. You cant just do it"" ""He is my son!"" ""You "" ""No. No can't trust me! He told me he loves me. He doesnt. ButI can trust you"
223,"valhalla, cubed, box",Story 3 I held the little black box in the palm of my hand . It continued to hold my gaze regardless as if there were some deep importance about it . My friend Valhalla appeared out of no where and sat next to me .
227,"sniper, telescopic, rifle, sight, 3d","Story 4 I stared down the telescopic sight of my 196 sniper rifle . I slowly moved my gaze into each window in the hotel, Many displays of various vice . One couple was violently pleasuring each other . Another was an old man, watching a younger woman strip in front of him"
230,"rating, cohesiveness, stories, chatgpt",Overall Comments from Teachers on ChatGPT's Rating . Teachers are not informed that the ratings are done by an AI model . They find that the attributes they do not agree with are mainly Likability and Cohesiveness .
233,"openai, model, t0, gui, chatgpt",The T0 model we use is called TOpp . We query ChatGPT using the OpenAI GUI . The InstructGPT model we queried is the Dec. 15 and Jan. 9 version .
237,"answer, llm","After the T0 and InstructGPT generate the answer, we parse the generated sentence to get the model's score . First, we remove the string 1-5 from the output, out of 5 since LLM sometimes says it ""give a score of x out of 5"" . Finally, the string "
259,"fragment, prompt, story",Please rate the story fragment The goal of this task is to rate story fragment . We will reject submissions from workers that are clearly spamming the task .
273,"adversarial, samples, text, bert-base-uncased, classifier, textfooler, attacks",We use the adversarial samples generated against a bert-base-uncased text classifier trained on AG-News . The intent of the dataset is to facilitate the research in SSA .
274,"samples, evaluation, text-davinci-003, llm, adversarial",Text-davinci-003 is the LLM evaluation for evaluating the quality of adversarial samples in Table 11 . We can see that the result of using it is similar to ChatGPT .
