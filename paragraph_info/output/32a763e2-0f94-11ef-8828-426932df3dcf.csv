element_idx,keywords,summarized_text
7,"computer, natural, vision, language, describing, processing, image","In this paper, we present a generative model based on a deep recurrent architecture that combines computer vision and machine translation . The model is trained to maximize the likelihood of the target description sentence given the training image . Experiments on several datasets show the accuracy of the model and the fluency of the"
9,"english, understanding, computer, vision, visual, image","a description must capture not only the objects contained in an image, but also how they relate to each other as well as their attributes and the activities they are involved in . Moreover, the above semantic knowledge has to be expressed in a natural language like English, which means that a language model is needed in addition to visual understanding"
15,"likelihood, description, image","we would like to present in this work a single joint model that takes an image I as input, and is trained to maximize the likelihood p of producing a target sequence of words S = S1, S2,   where each word St comes from a given dictionary ."
17,"translation, source, rnn, sentence, machine","The main inspiration of our work comes from recent advances in machine translation . The task is to transform a sentence S written in a source language into its translation T in the target language . For many years, machine translation was also achieved by a series of separate tasks ."
22,"net, nic","We present an end-to-end system for the problem . It is a neural net which is fully trainable using stochastic gradient descent . Second, our model combines state-of-art subnetworks for vision and language models ."
24,"language, natural, data, visual","generating natural language descriptions from visual data has long been studied in computer vision, but mostly for video . These systems are heavily hand-designed, relatively brittle and have been demonstrated only on limited domains, e.g. traffic scenes or sports ."
25,"text, detections, generation, description, image",Farhadi et al. use detections to infer a triplet of scene elements . A more complex graph of detections beyond triplets is used by Kulkani . More powerful language models based on language parsing have been used .
26,"descriptions, crops, query, text, image","For an image query, descriptions are retrieved which lie close to the image . Most closely, neural networks are used to co-embed images and sentences together . In general, the above approaches cannot generate novel descriptions ."
28,"embedding, space, network, classification, multimodal, convolutional, image","The RNN is trained in the context of this single ""end-to-end"" network . The closest works are by Kiros et al. who use a neural net, but a feedforward one, to predict the next word given the image ."
30,"translation, image","In this paper, we propose a neural and probabilistic framework to generate descriptions from images . Recent advances in statistical machine translation have shown that it is possible to achieve state-of-the-art results . These models make use of a recurrent neural network which encodes the variable length input into a fixed dimensional"
37,"cnn, representation, scene, classification, image","For the representation of images, we use a Convolutional Neural Network . They have been widely used and studied for image tasks . The words are represented with an embedding model ."
40,"of, ing, knowledge, c, encod-, cell, model, value, the, lstm","the core of the LSTM model is a memory cell c encoding knowledge at every time step of what inputs have been observed up to this step . The behavior of the cell is controlled by ""gates"" - layers which are applied multiplicatively and thus can either keep a value from the gated"
44,"of, state, product, value, gate, lstm"," represents the product with a gate value, and the various W matrices are trained parameters . Such multiplicative gates make it possible to train the LSTM robustly ."
48,"unrolling, lstm, image","image and each sentence word such that all LSTMs share the same parameters and the output mt-1 is fed to the unrolled version . In more detail, the unrolling procedure reads:"
49,"cnn, word, vision, network, lstm, image","The image I is only input once, at t = -1, to inform the LSTM about the image contents . We empirically verified that feeding the image at each time step as an extra input yields inferior results, as the network can explicitly exploit noise in the image and overfits more easily ."
52,"sentence, beamsearch, beam, image",BeamSearch: iteratively consider the set of the k best sentences up to time t as candidates to generate sentences of size t + 1 . The second approach is Sampling where we just sample the first word according to P1 then provide the corresponding embedding as input and sample p2 
56,"metrics, subjective, score, evaluation, automatic",prior art has proposed several evaluation metrics . The most reliable is to ask for raters to give a subjective score on the usefulness of each description given the image .
57,"mechanical, metric, transport, turk, amazon, agreement",Each image was rated by 2 workers. The typical level of agreement between workers is 65% . In case of disagreement we simply average the scores and record the average as the score .
62,"tuning, bleu, hyperparameter",The perplexity is the geometric mean of the inverse probability for each predicted word . We used this metric to perform choices regarding model selection and hyperparameter tuning in our held-out set .
63,"acoustic, sequence, ranking, task, description, image","transforming the description generation task into a ranking task is unsatisfactory . as the complexity of images to describe grows, together with its dictionary, the number of possible sentences grows exponentially with the size of the dictionary . the likelihood that a predefined sentence will fit a new image will go down unless the"
73,"transfer, data, driven, learning","Since our model is data driven and trained end-to-end, we wanted to answer questions such as ""how dataset size affects generalization"" As a result, we performed experiments on five different datasets ."
75,"imagenet, sbu, models, overfitting","purely supervised approaches require large amounts of data, but the datasets that are of high quality have less than 100000 images . As a result, the advantage of our method versus most current human-engineered approaches will only increase in the next few years as training set sizes will grow ."
77,"ensembling, long-term, bleu, model, dropout","We, the word embeddings . We tried initializing them from a large news corpus . No significant gains were observed, and we decided to just leave them uninitialized ."
81,"flickr, sbu, features, dataset, image","We report our main results on all relevant datasets in Tables 1 and 2. Since PASCAL does not have a training set, we used the system trained using MSCOCO . Flickr datasets have been used recently , but mostly evaluated in a retrieval framework ."
82,"bleu, five, score, raters","Human scores in Table 2 were computed by comparing one of the human captions against the other four . We do this for each of the five raters, and average their BLEU scores ."
83,"metrics, raters, better, evaluation, human","BLEU-4 is the standard in machine translation moving forward . Additionally, we report metrics shown to correlate better with human evaluations in Table 14 ."
92,"learning, transfer, mscoco, size, data, flickr8k","the most obvious case for transfer learning and data size is between Flickr30k and Flickr8k . In this case, we see gains by adding more training data since the whole process is data-driven and overfitting prone ."
98,"descriptions, bleu, score, generative, model, captions, novel",Table 3 shows some samples when returning the N-best list from our beam search decoder instead of the best hypothesis . Notice how the samples are diverse and may show different aspects from the same image . The agreement in BLEU score between the top 15 generated sentences is 58 .
102,"mnlm, annotation, ranking, image","NIC is doing surprisingly well on both ranking tasks . Note that for the Image Annotation task, we normalized our scores similar to what used ."
110,"system, nic, bleu, groundtruth, reference","Figure 4 shows the result of the human evaluations of the descriptions provided by NIC, as well as a reference system and groundtruth on various datasets . We can see that BLEU is not a perfect metric, as it does not capture well the difference between NIC and human descriptions assessed by raters"
114,"lstm, st-1","In order to represent the previous word St-1 as input to the decoding LSTM producing St, we use word embedding vectors . These vectors can be jointly trained with the rest of the model ."
115,"component, vision, cnn, bag-of-words","""horse"" , ""pony"" and ""donkey"" close to each other will encourage the CNN to extract features that are relevant to horse-looking animals."
120,"nic, reasonable, description, image",NIC is based on a convolution neural network that encodes an image into a compact representation . The model is trained to maximize the likelihood of the sentence given the image . Experiments on several datasets show the robustness of NIC in terms of qualitative results and quantitative evaluations .
124,"descriptions, patterns, dependency, relational, visual, image","In ACL, 2010. D. Bahdanau, K. Cho, and Y. Bengio. Generating image descriptions using dependency relational patterns . In EMNLP, 2014. J. Donahue, J. Jia, 0. Vinyals, M. Hoffman, N. Zhang, E."
125,"recognition, amazon, visual, description, image","ACL, 2, 2014. S. Li, G. Kulkarni, T. L. Berg, A. C. Berg and Y. Choi. Treetalk: Composition and compression of trees for image descriptions . In Conference on Computational Natural Language Learning, 2011. T.-Y. Lin, M. Mai"
