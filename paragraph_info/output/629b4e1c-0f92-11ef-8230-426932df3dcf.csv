element_idx,summarized_text,keywords
3,"ImageNet classification is the de facto pretraining task for these models . Yet, ImageNet is now nearly ten years old and is by modern standards ""small"" In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags .","imagenet, hashtag, learning, large-scale, transfer"
5,"Pretrain a convolutional network on a large, manually annotated image classification dataset . This formula has been in wide use for several years . In fact, it is SO effective that it would now be considered foolhardy not to use supervised pretraining .","image, perception, classification, visual"
6,the ImageNet dataset is the de facto pretraining dataset . There are studies analyzing the effects of various ImageNet pretraining factors on transfer learning or the use of different datasets that are of the same size magnitude magnitude as ImageNet .,"imagenet, transfer, dataset, learning"
10,data source has potential disadvantages: hashtags may be too noisy to serve as an effective supervisory signal . it is not obvious that training on this data will yield good transfer learning results .,"source, distribution, image, data"
11,"Models trained on billions of Instagram images using thousands of distinct hashtags exhibit excellent transfer learning performance . For example, we observe improvements over the state-of-the-art for image classification and object detection . Our primary goal is to contribute novel experimental data about this previously unexplored regime .","phisticated, imagenet-1k, images, collecting, data, state-of-the-art, cleaning"
15,The canonical hashtags are used as labels for training and evaluation . We select a set of hashtags that are tagged with at least one of these hashtags .,"wordnet, synsets"
21,We use approximate image and label counts for convenience . We omit the role and image count when it is clear from context or not useful to present .,"i, instagram, i-l"
22,The hashtag set sizes are measured after merging the hashtags into their canonical forms . We hypothesize that the first set has a visual distribution similar to IN-1k . The other two represent more general visual distributions covering fine-grained visual categories .,"instagram, distribution, data, visual"
23,5% of the images in the val-CUB-6k200 set also appear in train-IN-1M-1k . 1.78% of images in VAL-IN-50k-1k set are in the JFT-300M training set . We compute R-MAC features for all candidate images using a,"deduplication, features, training, image, r-mac"
26,"Our datasets have two nice properties: public visibility and simplicity . To see what it looks like, the images are browsable by hashtag at https : 1 . Our data is also taken from the ""wild"" .","brownbear, public, visibility"
28,"despite our efforts to make the dataset content transparent, we acknowledge that, similar to JFT-300M, it is not possible for other research groups to know exactly which images we used nor to download them en masse . However, we believe that it is better if we undertake this study and share the results with the community","content, jft-300m, trans-parent, process, dataset"
30,"In addition to the standard IN-1k dataset, we experiment with larger subsets of the full ImageNet 2011 release that contains 14.2M images and 22k labels . For the 9k label set, we follow the same protocol used to construct IN-5k . In all cases, we use 50 images per class for validation ","in-1k, validation, sets"
34,Train-IG-1B-17k contains 2 hashtags per image . The target is a vector with k non-zero entries each set to 1/k .,"softmax, activation, distribution, imagenet"
37,"Our models are trained by synchronous stochastic gradient descent on 336 GPUs across 42 machines with minibatches of 8,064 images . Each GPU processes 24 images at a time and batch normalization statistics are computed on these 24 image sets . Our ResNeXt-101 32x16d networks took","stochastic, (sgd), number-of-images-processed, synchronous, descent, gfd, gradient"
38,"To set the learning rate, we follow the linear scaling rule with gradual warmup described in . We use a warm-up from 0.1 up to 0.1 /256 x 8064, where 0.1 and 256 are canonical learning rate and minibatch sizes . The same settings are used when training on","warm-up, learning, rate, reduction, schedule"
40,"In our experiments, we pretrain convolutional networks for hashtag prediction and transfer those networks to a variety of tasks . There are two established protocols for judging the quality of a pretrained model .","hashtag, task, source, prediction"
42,Full network finetuning views pretraining as sophisticated weight initialization . The success of pretraining is judged by its impact on the target task . feature transfer uses pretrained network as a feature extractor .,"finetuning, weight, sophisticated, initialization"
43,"Full network finetuning is performed by removing the hashtag-specific fully connected classification layer from the network and replacing it with a randomly initialized classification layer with one output per class in the target task . This modified network is then trained using SGD with momentum . To do this, we randomly hold out a small portion","finetuning, validation, network, layer, set, classification"
47,"Our first experiment varies the Instagram hashtag sets used in pretraining whilst keeping other factors constant . We compute transfer learning results as top-1 classification accuracy on five target datasets . For baseline models, we use train-IN-1k as the baseline source task . Full network finetuning of ResNeXt","val-places-365, imagenet, val-cub-200, val-in-9k, val-in-1k, classification"
53,Model pretrained with IN-1k-aligned 1.5k hashtag set outperforms source networks trained on larger hashtag sets . This trend reverses as the number of target ImageNet classes increases . Source models trained with the largest hashtag sets perform the best .,"source, imagenet, architecture, networks"
54,We measure the rectified classification accuracy of this model on val-IN-1k . We present all incorrect classifications to five human annotators .,"ig-3.5b, ig-3.5b-17k"
55,"This experiment studies the relationship between the number of images used in Instagram pretraining and classification accuracy on the target task . For these experiments, we train a linear classifier and keep the pretrained network weights fixed . We make this choice because the effect of pretraining is small .","task, image, accuracy, target, pretraining, classification"
60,Figure 2 shows the classification accuracy on ImageNet validation sets as a function of the number of Instagram training images ranging from 3.5M to 3.5B images . The figures correspond to ImageNet target tasks with three different capacities and CUB2011 .,"validation, imagenet, sets, accuracy, classification"
65,the lines corresponding to ResNeXt-101 32x 16d networks are steeper . This result suggests that current network architectures are prone to underfitting . We also observe log-linear scaling break down in two regimes .,"log-linear, scaling, accuracy, network, architecture"
66,"In-1k, networks pretrained on the target-taskaligned 1.5k hashtags outperform those trained using a larger hashtag vocabulary . However, as the matching between hashtag vocabulary and target classes disappears and the visual variety in the transfer task increases . on IN-9k, the difference in accuracy between networks trained on","task, imagenet, hashtag, synsets, transfer, vocabulary"
67,The highest accuracies on val-IN-1k are 83.3% . The results are obtained by training a linear classifier on fixed features . These results are nearly as good as full network finetuning .,"ig-940m-1k, ig-940m"
68,"To test whether the above observations generalize to fine-grained classification, we repeated the experiments on the CUB2011 dataset, and show the results in Figure 2 bottom right . The curves reveal that when training data is limited, the 1.5k hashtag dataset is better, but once the number of training images surpasses 100","visual, distribution, bottom, right, cub2011, classification, fine-grained"
69,"a major difference between hashtag supervision and the labels provided in datasets such as ImageNet is that hashtag supervision is inherently noisy . To do SO, we pretrain ResNeXt-101 32x16d networks on a version of IG-1B-17k in which we randomly replaced p% of the hashtag","model, imagenet, hashtag, accuracy, supervision"
76,"we only train the final linear classifier on the target task, because full finetuning may mask the damage caused by pretraining noise . The results suggest that networks are resilient against label noise: a noise level of p = 10% leads to a loss of less than 1% in classification accuracy .","pretraining, noise, label"
77,Prior studies in language modeling found that resampling Zipfian distributions reduces the impact of the head of the word distribution on the overall training loss . We perform experiments in which we evaluate three different types of data sampling in the Instagram pretraining .,"imagenet, hashtag, instagram, sampling, square-root"
80,resampling of hashtag distribution is important in order to obtain good transfer to ImageNet image-classification tasks . using uniform or square-root sampling leads to an accuracy improvement of 5 to 6% irrespective of the number of ImageNet classes in the transfer task .,"image-classification, imagenet, hashtag, distribution, vocabularies"
81,"We use IG-940M-1.5k to pretrain ResNeXt-101 32x32d . We use these ""super-sized"" models to improve val-IN-1k results over the 32x16d model .","ig-940m-1.5k, bound, learning, model-capacity, transfer"
85,"#party may correspond to a large variety of visual scenes . We matched the 17k Instagram hashtags with a list of 40k ""concreteness"" values of nouns . Figure 6 displays these hashtag concreteness values and accuracy of predicting the hashtags correctly in a scatter plot .","classes, instagram, concreteness, visual"
95,"We compare performance on the 2017 test-dev set using several different pretrained networks . We use IN-1k, 5k pretraining and compare them to IG-940M-1k and IG-1B-17k for the largest model . For the Instagram pretrained models we found it necessary to perform grid search","imagenet, instagram, mask, pretraining, r-cnn"
96,"Figure 8 shows two interesting trends . First, we observe that when using large amounts of pretraining data, detection is model capacity bound . with the lowest capacity model, the gains from larger datasets are small or even negative . The second trend comes from comparing COCO's default AP metric .","model, large, precision, coco, pretraining, average, data, capacity"
100,"We observe that the improvement over IN1k, 5k pretraining from IG-1B-1k is much larger in terms of AP@50 . Thus, the gains from Instagram pretraining may be primarily due to improved object classification performance, rather than spatial localization performance .","hashtag, instagram, class, classification"
103,"Sun et al. train convolutional networks on the JFT-300M dataset of 300 million weakly supervised images . We obtain substantially higher accuracies on transfer tasks, compared to 79.2% reported in .","size, large, training, image, imagenet-1k, set, dataset"
104,Other prior studies trained convolutional networks to predict words or n-grams in comments on a collection of 100 million Flickr photos and corresponding comments . Other work also trained network to predict hashtags on the Flickr dataset but does not investigate transfer of the resulting networks to other tasks .,"flickr, convolutional, image, word, networks, dataset"
107,"We found that networks trained on a hashtag vocabulary outperformed those trained on twice as many images without such careful selection of hashtags . This observation paves the way for the design of ""label-space engineering"" approaches that aim to optimally select label sets for a particular target task .","task, engineering, label-space, imagenet-1k, target"
109,"our results suggest that accuracy improvements on target tasks may be obtained by further increases of the capacity of our networks . However, it is not unthinkable that some of the design choices that were made in current network architectures are too tailored to ImageNet-1k classification . Our results also underline the importance of increasing the visual variety that","imagenet, hashtag, visual, variety, large-scale"
114,"Rich feature hierarchies for accurate object detection and semantic segmentation . In: CVPR. 2. Donahue, J., Jia, Y., Vinyals, 0.","visualization, decaf, segmentation, semantic, generic, detection, object, accurate"
116,"In: CVPR. 10. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture . In: ICCV. 10. Carreira, J., Zisserman, A.: Towards accurate multi-person","convolutional, multi-scale, visual, image, recognition, common, architecture"
118,"ICCV. 29. Pathak, D., Girshick, R., Dollar, P., Darrell, T., Hariharan, B.: Learning features by watching objects move . In: CVPR. 31. Misra, I., Zitnick, C.L.,","imagenet, compositionality, content, visual"
120,"Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision . In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .","inception-resnet, vision, computer"
123,"All of our Instagram datasets are subsets of I, where each image in I is from a public Instagram post that has at least one hashtag in its caption . To construct a dataset, we select a set of hashtags that is a subset of H . We randomly samples images from I that are ","instagram, wordnet, dataset, synset"
124,"To determine if a match exists, we define a function s that returns the subset of S that matches hashtag h E H . We filter the hashtags in H by accepting only the ones that match to any synset in S .","hashtag-to-synset, matching, wordnet, synset"
129,"In the first stage, we search the set of 3.5 billion Instagram images2 using an approximate nearest neighbor search algorithm to identify 128 potential duplicates for each query image . In the second phase, we compute pairwise distances between the candidates and the query image, and apply a conservative distance threshold to generate potential pairs of duplicates","image, instagram, quality, features"
130,"In the first stage, we remove the last five3 convolutional layers from a ResNet-50 model . We compute regional maximum activations of convolutions features from the resulting feature maps . R-MAC features substantially improve the performance of the deduplication method .","transformations, features, convolutions, cropping, r-mac"
131,"To construct a searchable index of 3.5 billion images, we undo the scalar quantization, L2-normalize the resulting vector, and apply optimized product quantization to further reduce the feature representation to 256 dimensions . Each sub-quantizer uses k-means clustering to quantize the 1","image, index, representation"
135,"the 256 nearest sub-quantizers compute the squared distances between this residual and the residuals stored in the entries in the inverted list . This produces distance estimates, which we use to select the 128 nearest neighbors efficiently using a max-heap implementation .","distance, hamming, estimates, sub-quantizers"
136,"In the second stage, we compute R-MAC features for the query image and each of the 128 identified neighbors in the same way as in the first stage . For each image in the query set that has at least one neighbor in IG-3.5B, we manually annotated the 21 nearest neighbors to assess whether or not the","neighbor, features, image, r-mac, query, val-in-1k"
137,"In Table 2, we report a conservative lower bound on the accuracy of our best models that treats all images that have duplicates in the training set as being classified incorrectly4.","training, image, accuracy, duplicates, set"
144,"Weights of the final fully connected layer are sampled from a zero-mean Gaussian with standard deviation 0.01. Following , the final BN scale parameter 2 of each residual block is initialized to 0 .","bias, pressure, image, standard, 2, bn, parameters, parameter"
145,"Our feature transfer experiments require training an L2-regularized logistic regressor . For train-CUB-200, the optimal penalty was 0.001 .","l2-regularized, regressor, logistic"
147,"The learning rate is set to the initial LR at the start of training and decayed by the specified LR decay factor according to the steps . For Instagram datasets, the total training length is given in terms of the number of images processed .","(lr), training, imagenet, rate, learning, schedule, datasets"
148,"Grid search was performed on the COCO val2017 set, which results are reported on the test-dev2017 set . The optimal initial learning rates are shown in Table 5. For our object detection experiments, we found it necessary to perform grid search .","server, search, evaluation, learning, initial, rate, coco, grid"
151,"We used a proper validation set held out from the training set of the target task . In all cases, we fixed the length of the finetuning schedule based on preliminary experiments .","finetuning, transfer, learning"
155,"Hashtag frequencies follow a Zipfian distribution . For example, in the 17k hashtag vocabulary, the most frequent hashtag appears more than 1 million times as often as the least frequent hashtag .","distri, distribution, zipfian"
157,"Given an image I with hashtags hi, the image-level replication factor for I is computed as r = maxi r . The threshold t is selected so that the final list has a target length matching the desired training schedule length .","image-level, x, training, replication, factor, âŒ€(x), ="
160,"Table 6: Comparison with the state of the art on the ImageNet-1k validation set . We append the result of ResNeXt-101 32xCd with C E 16, 32, 48 pretrained on Instagram hashtag data and finetuned on train-IN-1k .","imagenet-1k, art, state, of, the"
