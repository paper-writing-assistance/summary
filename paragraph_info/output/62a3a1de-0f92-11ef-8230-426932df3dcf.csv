element_idx,summarized_text,keywords
4,"Vision Transformers became the state-of-the-art image classification model . It is the hierarchical Transformers that reintroduced several ConvNet priors . In this work, we reexamine the design spaces and test the limits of what a pure Convnet can achieve .","convnet, swin, transformer, vision"
6,"the primary driver was the renaissance of neural networks, particularly convolutional neural networks . Through the decade, the field of visual recognition shifted from engineering features to designing architectures .","visual, back-propagation, recognition, learning, deep, convnets"
10,"The introduction of AlexNet precipitated the ""ImageNet moment"" The field has since evolved at a rapid speed . Representative ConvNets like VGGNet , Inceptions , ResNet .","feature, visual, vision, learning, alexnet, computer"
11,"ConvNets have several built-in inductive biases that make them well-suited to a wide variety of computer vision applications . The most important one is translation equivariance, which is a desirable property for tasks like objection detection . For many decades, this has been the default use of Conv","sliding, vision, window, convnets, computer"
14,"ViT introduces no image-specific inductive bias and makes minimal changes to the original NLP Transformers . The biggest challenge is ViT's global attention design, which has a quadratic complexity .","natural, language, vision, architecture, backbone, processing"
15,"Hierarchical Transformers employ a hybrid approach to bridge this gap . For example, the ""sliding window"" strategy was reintroduced to Transformers . Swin Transformer is a milestone work in this direction .","transformers, vision, bridge, convnets, backbone, hierarchical"
16,"Transformers for computer vision have been aimed at bringing back convolutions . These attempts come at a cost: a naive implementation of sliding window selfattention can be expensive . On the other hand, itis almost ironic that a ConvNet already satisfies many of those desired","cyclic, convnet, vision, shifting, computer"
18,"In recent literature, system-level comparisons are usually adopted when comparing the two . ConvNets and hierarchical vision Transformers become different and similar at same time .","hierar-, chical, vision, convnets, transformer"
19,"ConvNeXts, constructed entirely from standard ConvNet modules, compete favorably with Transformers in terms of accuracy, scalability and robustness across all major benchmarks . Our exploration is directed by a key question: How do design decisions in Transformers impact Convnets' performance?","convnet, task, vision, swin-t"
22,"In this section, we provide a trajectory going from a ResNet to a ConvNet that bears a resemblance to Transformers . For simplicity, we will present the results with the ResNet-50 / Swin-T complexity models . The conclusions for higher capacity models are consistent and can","regime, resnet, resnet-200, /, resnet-, swin-b, swin-t"
26,Our starting point is a ResNet-50 model . We first train it with similar training techniques used to train vision Transformers . This will be our baseline .,"network, vision, modernization, resnet-50"
29,"In our study, we use a training recipe that is close to DeiTs and Swin Transformer's . This enhanced training recipe increased the performance of the ResNet-50 model . The training is extended to 300 epochs from the original 90 .","optimizer, resnet-50, vision, adamw, transformer"
32,"The original design of the computation distribution across stages in ResNet was largely empirical . The heavy ""res4"" stage was meant to be compatible with downstream tasks like object detection . Swin-T followed the same principle but with slightly different stage compute ratio of 1:1:3:1 .","computation, distribution, compute, resnet, ratio, stage"
35,"the stem cell in standard ResNet contains a 7 x7 convolution layer with stride 2 . Vision Transformers uses a similar ""patchify"" layer, but with a smaller patch size of 4 to accommodate the architecture's multi-stage design . The accuracy has changed from 79.4% to ","resnet, transformer, vision"
38,"ResNeXt has a better FLOPs/accuracy trade-off than a vanilla ResNet . The core component is grouped convolution, where the convolutional filters are separated into different groups .","convolutional, resnext, bottleneck, filters, width, block"
39,"In our case we use depthwise convolution, a special case of grouped convolution where the number of groups equals the numbers of channels . The combination of deepwise conv and 1 x 1 convs leads to a separation of spatial and channel mixing . We increase the network width to the same number of","performance, depthwise, convolution, self-attention, network"
46,"Despite the increased FLOPs for the depthwise convolution layer, this change reduces the whole network FLOP to 4.6G . In the ResNet-200 / Swin-B regime, this step brings even more gain .","bottleneck, inverted, blocks, residual"
49,"vision Transformers reintroduced the local window to the self-attention block . The window size is at least 7x7, significantly larger than the ResNet kernel size of 3x3 .","convolutional, large, convolutions, transformers, vision, kernels"
50,"Transformers: the MSA block is placed prior to the MLP layers . This is a natural design choice the complex/inefficient modules will have fewer channels . The efficient, dense 1 x 1 layers will do the heavy lifting .","large, depthwise, conv, layer, large-kernel, kernels"
52,"We experimented with several kernel sizes, including 3, 5, 7, 9, and 11. The network's performance increases from 79.9% to 80.6% . Additionally, the benefit of larger kernel sizes reaches a saturation point at 7x7 .","model, large, capacity"
57,Replacing ReLU with GELU One discrepancy between NLP and vision architectures is the specifics of which activation functions to use . The Gaussian Error Linear Unit (GELU) is used in the most advanced Transformers .,"gelu, architecture, vision"
58,"Transformers have fewer activation functions . Consider a Transformer block with key/query/value linear embedding layers, the projection layer, and two linear layers in an MLP block . In comparison, it is common practice to append an activation function to each convolutional layer, including the 1 x","fewer, function, activation, transformer, block"
65,"BatchNorm is an essential component in ConvNets as it improves the convergence and reduces overfitting . However, BN has many intricacies that can have a detrimental effect on the model's performance .","ln, transformers, normalization, layer, convergence"
69,"In Swin Transformers, a separate downsampling layer is added between stages . This modification leads to diverged training . Further investigation shows that, adding normalization layers wherever spatial resolution is changed can help stablize training.","swin, spatial, downsampling, transformers, layer, resolution, swin-t"
72,ConvNeXt is a pure ConvNet that can outperform the Swin Transformer for ImageNet-1K classification in this compute regime . All design choices discussed so far are adapted from vision Transformers .,"convnet, transformer, swin"
73,"Vision Transformers' scaling behavior is what truly distinguishes them . In the next section, we will scale up our ConvNeXt models .","swin, visual, convnet, transformers, recognition"
75,"ConvNeXtT/S/B/L is the end product of the ""modernizing"" procedure on ResNet-50/200 regime . In addition, we build a larger ConvNoXt-XL . The variants only differ in the number of channels C . Following both ResNets","swin, convnext, swin-t/s/b/l, transformers, resnet-xl, resnet"
78,ImageNet-1K dataset consists of 1000 object classes with 1.2M training images . We also conduct pre-training on ImageNet-22K . More details can be found in Appendix A.,"imagenet-1k, setups, validation"
79,We train ConvNeXts for 300 epochs using AdamW with a learning rate of 4e-3 . We use a batch size of 4096 and a weight decay of 0.05 .,"convnexts, convnet-1k, training, larger, models"
81,"We fine-tune ImageNet22K pre-trained models on ImageNet-1K for 30 epochs . We use AdamW, a learning rate of 5e-5, cosine learning rate schedule, layer-wise learning rate decay, no warmup, batch size of 512, and weight decay","imagenet-1k, modeling, fine-tuning, resolution"
84,"Table 1 shows the result comparison with two recent Transformer variants, DeiT and Swin Transformers . ConvNeXt competes favorably with two strong ConvNet baselines in terms of accuracy-computation tradeoff .","swin, convnet-1k, transformer, transformerv2"
88,"ConvNeXt can have a much higher throughput than Swin Transformer . Inference throughput is measured on a V100 GPU, following .","swin, convnext, transformer"
92,We present results with models fine-tuned from ImageNet-22K pre-training at Table 1 . Our results demonstrate that properly designed ConvNets are not inferior to vision Transformers when pre-trained with large dataset . ConvNeXts still perform on par or better than similarly-sized Swin,"swin, transformers, imagenet-22k, vision"
96,ConvNeXt-S/B/L uses the same feature dimensions as ViT . The block structure remains the same . We use the supervised training results from DeiT and MAE for ViT-L .,"vit, convnext, vit-style"
103,Table 3 shows object detection and instance segmentation results . ConvNeXt achieves on-par or better performance than Swin Transformers .,"swin, convnext, segmentation, instance, transformer"
104,"All model variants are trained for 160K iterations with a batch size of 16 . In Table 4, we report validation mloU with multi-scale testing .","upernet, convnext, segmentation, semantic, ade20k"
108,ConvNeXt and Swin Transformer exhibit a more favorable accuracy-FLOPs trade-off due to the local computations . This is true for both classification and other tasks requiring higher-resolution inputs. ConvNet inductive bias is not directly related to the self-attention mechanism in vision Transformers,"swin, convnext, transformer"
110,"In both the pre- and post-ViT eras, the hybrid model combining convolutions and self-attentions has been actively studied . Prior to ViT, the focus was on augmenting a ConvNet to capture long-range dependencies .","hybrid, convolution, models, model"
112,"ConvMixer uses a smaller patch size to achieve the best results . FFT is also a form of convolution, but with a global kernel size and circular padding .","swin, token, mixing, depthwise, convolution, fft"
114,"In the 2020s, vision Transformers began to overtake ConvNets as the favored choice for generic vision backbones . We propose ConvNeXts, a pure KonvNet model that can compete favorably with state-of-the-art hierarchical Vision Transformers across multiple computer vision","convnet, transformers, swin, vision"
126,"ConvNeXt-L pre-trained on ImageNet-1K, where the model accuracy is significantly lower than the EMA accuracy due to overfitting . We select its best EMA model during pre-training as the starting point for fine-tuning .","model, fine-tuning, final, imagenet-1k, weights"
137,"Additional robustness evaluation results for ConvNeXt models are presented in Table 8. We directly test our ImageNet-1K trained/fine-tuned classification models on several robustness benchmark datasets . We report mean corruption error for ImageNet-C, corruption error in ImageNetC, and top-1 Accuracy for","convnext, error, benchmark, corruption, imagenet-1k, robustness"
138,ConvNeXt demonstrates robustness behaviors . We note that these robustness evaluation results were acquired without using any specialized modules or additional fine-tuning procedures .,"convnext, model, robust, state-of-the-art, transformer"
143,"ResNet-200, the initial number of blocks at each stage is . We change it to Swin-B's at the step of changing stage ratio . This drastically reduces the FLOPs, SO at the same time .","resnet-200, swin-"
155,ImageNet models' inference throughputs in Table 1 are benchmarked using a V100 GPU . ConvNeXt is slightly faster in inference than Swin Transformer with similar parameters . We now benchmark them on the more advanced A100 GPUs .,"swin, transformer, imagenet"
160,"ConvNeXt can perform as good as a hierarchical vision Transformer on image classification, object detection, instance and semantic segmentation tasks . Our goal is to offer a broad range of evaluation tasks, but recognize computer vision applications are even more diverse . A case in point is multi-modal learning, in which","multi-modal, convnext, learning"
163,"In the 2020s, research on visual representation learning began to place enormous demands on computing resources . ViT, Swin, and ConvNeXt all perform best with their huge model variants . Investigating those model designs results in an increase in carbon emissions .","convnext, large, visual, dataset, representation"
165,"Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, Aravind Srinivas, Tsung- Yi Lin, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional networks.","swin, github, transformer"
166,"In CVPR, 2017. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning . MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark . ICML, 2021. Jia Deng, Wei Dong, Richard Socher,","chang, mmdetection, wei, qi-hang, qi, dai"
167,"Masked autoencoders are scalable vision learners . ArXiv:2111.06377, 2021. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017. Kaims He, Xi","mobile, visual, backpropagation, recognition, networks"
168,"In NeurIPS, 2019. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan- Yen Lo, and Piotr Dollar. On network design spaces for visual recognition. In CVPR, 2020. Prajit Ramachandran, Niki Parmar, Ashish","network, design, visual, recognition"
169,"In CVPR, 2013. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition . In ICLR, 2015. Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel,","mobile, visual, vision, recognition, action"
170,"In ECCV, 2018. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun . Unified perceptual parsing for scene understanding . In CVPR, 2017. Weijian Xu, Yifan X","a, ade20k, dataset"
