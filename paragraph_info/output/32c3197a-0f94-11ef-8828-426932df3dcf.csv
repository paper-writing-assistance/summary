element_idx,keywords,summarized_text
3,"learning, sequence, multi-dimensional, networks, neural, recurrent, rnns",Recurrent neural networks have proved effective at one dimensional sequence learning tasks . Some properties that make RNNs suitable for such tasks are also desirable in multidimensional domains .
5,"rnns, neural, recurrent, networks","Recurrent neural networks were originally developed as a way of extending neural networks to sequential data . Because of their recurrent connections, RNNs can adapt to stretched or compressed input patterns by varying the rate of change of their internal state ."
8,"convolution, multi-dimensional, networks, rnn, data, architecture","most successful use of neural networks for multi-dimensional data has been the application of convolution networks to image processing tasks . For example, sequences of handwritten digits must be pre-segmented into individual characters before they can be recognised by convolution nets ."
15,"hmms, multi-dimensional","Multi-dimensional HMMs suffer from two severe drawbacks: the time required to run the Viterbi algorithm, and thus calculate the optimal state sequences, grows exponentially with the number of data points . Numerous approximate methods have been proposed to alleviate one or both of these problems ."
19,"network, recurrent, mdrnn, connection",the basic idea of MDRNNs is to replace the single recurrent connection found in standard RNNs . The hidden layer of the network receives both an external input and its own activations from one step back along all dimensions .
21,"ordering, network, suitable","Data must be processed in such a way that when the network reaches a point in an n-dimensional sequence, it has already passed through all the points from which it will receive its previous activations . This can be ensured by following a suitable ordering on the points  . Note that this is not the"
27,"gradient, through, bptt, backpropagation, (bptt), time, error","the error gradient of an MDRNN can be calculated with an n-dimensional extension of the backpropagation through time algorithm . At each timestep, the hidden layer receives both the output error derivatives and its own n 'future' derivatives ."
28,"layer, hidden, mdrnn, summation, units","At a point x = in an n-dimensional sequence, define ix and hxk respectively as the activations of the jth input unit and the kth hidden unit . The forward pass for a sequence with dimensions can be summarised as follows ."
45,"recognition, network","At a point in the input sequence, the network has access to all points such that x'r  XiV i E . This defines an n-dimensional 'context region' of the full sequence . For some tasks, such as object recognition, this would in principal be sufficient ."
47,"networks, recurrent, multi-directional, bidirectional, brnns",BRNNs contain two separate hidden layers that process input sequence . The two hidden layers are connected to a single output layer . the network has access to both past and future context .
48,"layer, brnns, hidden","BRNNs can be extended to n-dimensional data by using 2n separate hidden layers . The hidden layers are chosen so that their origins lie on the 2n vertices of the sequence . As previously, the hidden layers have access to all surrounding context ."
49,"layer, hidden, multi-directional, data, mdrnn, processing","if the size of the hidden layers is held constant, multi-directional MDRNNs scales as O for n-dimensional data . In practice using 2n small layers gives better results than 1 large layer with the same overall number of weights ."
60,"network, rnn, output, architecture",So far we have implicitly assumed that the network can make use of all context to which it has access . The influence of a given input on the hidden layer decays or blows up exponentially as it cycles around the network's recurrent connections .
61,"lstm, memory, multiplicative, long-term, units","Long-Term Memory is an RNN architecture specifically designed to address the vanishing gradient problem . Each block contains a set of internal units, known as cells . The effect of the gates is to allow the cells to store and access information over long periods of time ."
70,"segmentation, task, 3d, image","We used the sequence to define a 2D image segmentation task . We divided the data at random into a 250 frame train set, 150 frame test set and 55 frame validation set . This would have left us with only one exemplar ."
71,"softmax, layer, hidden, activation, multi-directional, mdrnn, function","Each layer consisted of 25 memory blocks, each containing 1 cell, 2 forget gates, 1 input gate, 1 output gate and 5 peephole weights . This gave a total of 600 hidden units . The network contained 43,257 trainable weights in total ."
79,"mnist, digit, handwritten","The MNIST database of isolated handwritten digits is a subset of a larger database available from NIST . It consists of size-normalized, centered images, each of which is 28 pixels high and 28 pixels wide . The data comes divided into a training set with 60,000 images and "
93,"air, activations, database, network, freight, segments","Figure 9 shows the network activations during a frames from the Air Freight database . The network segments this image almost perfectly, in spite of difficult, reflective surfaces such as the glass and metal tube running from left to right ."
94,"jacobian, sensitivity, matrix, image",a precise measure of the network's sensitivity to context can be found by analysing the derivatives of the networks outputs at a particular point x in the sequence . The matrix of these derivatives dinx is referred to as the Jacobian matrix . It can be seen that the network responds to
98,"mdrnn, recurrent, multi-dimensional, networks",We have introduced multi-dimensional recurrent neural networks . We have added multidirectional hidden layers that provide the network with access to all contextual information .
100,"long-, memory, long-term, network, lstm","A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Bidirectional LSTM networks for improved phoneme classification and recognition . In Proceedings of the 2005 International Conference on Artificial Neural Networks, Warsaw, Poland, 2005 . S. Hochreiter"
102,"computer, society, icdar, gradi","J. Li, A. Najmi, and R. M. Gray. A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks . In Proceedings of the 9th International Conference on Document Analysis and Recognition, ICDAR 2007, Curitiba, Brazil, 2007. M."
