element_idx,keywords,summarized_text
5,"model-free, 2600, learning, conventional, reinforcement, state-of-the-art, architecture, atari","In recent years there have been many successes of using deep representations in reinforcement learning . Many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders . The main benefit of this factoring is to generalize learning across actions without imposing any change to the"
6,"deep, visuomotor, learning, reinforcement, policies","Over the past years, deep learning has contributed to dramatic advances in scalability and performance of machine learning . Notable examples include deep Q-learning , deep visuomotor policies , attention with recurrent networks and model predictive control with embeddings ."
7,"rl, algorithms, methods, network, architecture","The focus in these recent advances has been on designing improved control and RL algorithms . Here, we take an alternative but complementary approach . This approach has the benefit that the new network can be easily combined with existing and future algorithms for RL ."
13,"deep, learning, state, feature, q-networks, value, module, convolutional, function",the two streams are combined via a special aggregating layer to produce an estimate of the state-action value function Q . This dueling network should be understood as a single Q network with two streams that replace the popular single-stream Q network in existing algorithms such as Deep Q-Networks .
14,"stream, advantage, value, network, saliency","Intuitively, the dueling architecture can learn which states are valuable, without having to learn the effect of each action for each state . This is particularly useful in states where its actions do not affect the environment in any relevant way . These maps were generated by computing the Jacobians of the trained value and advantage streams with respect"
16,"pixels, 2600, scores, architecture, testbed, dueling, atari, image","We also evaluate the gains brought in by the dueling architecture on the challenging Atari 2600 testbed . Here, an RL agent with the same structure must be able to play 57 different games by observing image pixels and game scores only . The combination of prioritized replay with the proposed dueling network results in the"
22,"updating, learning, state, advantage, algorithm, value, function","Advantage updating algorithm was shown to converge faster than Q-learning in simple continuous time domains in . Its successor, the advantage learning algorithm, represents only a single advantage function ."
29,"st, step, time, atari","In the Atari domain, the agent perceives a video St consisting of M image frames: St = E S at time step t . The agent then chooses an action from a discrete set at E A = 1,  , A ."
39,"target, q-, learning, q(s, network, rate, sepa-",0- represents the parameters of a fixed and separate target network . We could try to use standard Qlearning to learn the parameters . This estimator performs poorly in practice .
41,"replay, dqn, experience","During learning, the agent accumulates a dataset Dt = e1, e2,  , et of experiences . The network is trained by sampling mini-batches of experiences uniformly at random ."
49,"replay, ddqn, prioritized, experience",A recent innovation in prioritized experience replay built on top of DDQN . Their key idea was to increase the replay probability of experience tuples that have a high expected learning progress . This led to both faster learning and better final policy quality across most games of the Atari benchmark suite .
52,"of, game, enduro, bootstrapping, action, value, setting, architecture","the key insight behind our new architecture is that for many states, it is unnecessary to estimate the value of each action choice . For example, knowing whether to move left or right matters when a collision is eminent . In some states it is of paramount importance to know which action to take ."
53,"q-network, layers, single, fully, dqn, connected, architecture","to bring this insight to fruition, we design a single Qnetwork architecture . The lower layers of the dueling network are convolutional as in the original DQNs . Instead, we use two sequences of fully connected layers . These streams are constructed so that they have the capability of providing separate estimates"
67,"optimization, module","On the one hand this loses the original semantics of V and A because they are now off-target by a constant . on the other hand it increases the stability of the optimization: with the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action's advantage in "
69,"q-network, algorithmic, modifi-, cations, back-pro, back-propagation",training of the dueling architectures requires only back-propagation . The estimates V and A are computed automatically without any extra supervision .
78,"environment, q, learned, values, corridor","The agent starts from the bottom left corner of the environment and must move to the top right to get the largest reward . In our setup, the two vertical sections both have 10 states while the horizontal section has 50 states ."
80,"single-stream, duel-, ing, environment, architecture, corridor",We compare a single-stream Q architecture with the dueling architecture . The 10 and 20 action variants are formed by adding no-ops to the original environment . We measure performance by Squared Error against true state values .
81,"q-network, speed, architecture, dueling, convergence","results show that with 5 actions, both architectures converge at about the same speed . However, when we increase the number of actions, the dueling architecture performs better than the traditional Q-network ."
87,"game, learning, arcade, re-, environment, wards",We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment . The challenge is to deploy a single algorithm and architecture with a fixed set of hyper-parameters to learn to play all the games given only raw pixel observations and game rewards .
89,"fully-connected, stream, convolution, layer, value, dqn, low-level, structure",The first convolutional layer has 32 8 x 8 filters with stride 4 . The second 64 4 x 4 filters with 2 . Our network architecture has the same low-level convolutionality structure of DQN .
91,"gradient, learning, stream, value, rate, dynamics","We rescale the combined gradient entering the last convolutional layer by 1/V2 . This simple heuristic mildly increases stability . In addition, we clip the gradients to have their norm less than or equal to 10."
92,"single, gradient, clipping, stream, ddqn, network","To isolate the contributions of the dueling architecture, we re-train DDQN with a single stream network using exactly the same procedure as described above . We apply gradient clipping, and use 1024 hidden units for the first fully-connected layer of the network SO that both architectures have roughly the same number of parameters"
100,"performance, baseline, difference, agent",an agent that achieves 2% human performance should not be interpreted as two times better . We chose not to measure performance in terms of percentage of human performance alone because a tiny difference relative to the baseline can translate into hundreds of percent in human performance difference.
107,"clip, level, percentage, duel, human, performance","Duel Clip does better than Single Clip on 75.4% of the games . It also achieves higher scores compared to Single baseline on 80.7% . Overall, our agent achieves human level performance on 42 out of 57 games."
111,"of, experience, architecture, replay, dueling, games, prioritization, atari",The dueling architecture can be easily combined with other algorithmic improvements . Prioritization of the experience replay has been shown to significantly improve performance of Atari games .
116,"gradient, clipping, interactions, td-errors, prioritization","To avoid adverse interactions, we roughly re-tuned the learning rate and the gradient clipping norm on a subset of 9 games . As a result of rough tuning, we settled on 6.25 x 10-5 for learning rate ."
117,"prioritized, baseline, dueling, games, atari, agent",prioritized dueling agent performs significantly better than the prioritized baseline agent and dueling agents alone . The total mean and median performance against the human performance percentage is shown in Table 1 .
119,"jacobian, of, v, maps, saliency","Saliency maps compute the absolute value of the Jacobian of V . To visualize the salient part of the image as seen by the advantage stream, we compute | s A; 0)1 ."
120,"rgb, frames, stream, scale, advantage, value, gray, input, maps, saliency, image",Figure 2 depicts the value and advantage saliency maps on the Enduro game for two different time steps. The value stream pays attention to the horizon where the appearance of a car could affect future performance. The advantage stream cares more about cars that are on an immediate collision course.
122,"q-learning, value, stream","The advantage of the dueling architecture lies partly in its ability to learn the state-value function efficiently . With every update of the Q values, the value stream V is updated . This contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions"
123,"state, average, action, ddqn, value, gap","differences between Q-values for a given state are often very small relative to the magnitude of Q . For example, after training with DDQN on the game of Seaquest, the average action gap across visited states is roughly 0.04 . This difference in scales can lead to small amounts of noise in the updates"
127,"deep, rl, state-of-the-art, network, atari, architecture",We introduced a new neural network architecture that decouples value and advantage in deep Q-networks . The new dueling architecture leads to dramatic improvements over existing approaches for deep RL in this popular domain .
