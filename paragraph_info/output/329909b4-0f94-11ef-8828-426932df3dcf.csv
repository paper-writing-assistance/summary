element_idx,keywords,summarized_text
4,"deep, lightweight, gradient, learning, asynchronous, reinforcement, descent, framework",We present asynchronous variants of four standard reinforcement learning algorithms . We show that parallel actor-learners have a stabilizing effect on training . The best performing method surpasses current state-of-the-art on the Atari domain .
6,"(rl), deep, algorithms, learning, reinforcement, networks, neural","Deep neural networks provide rich representations that can enable reinforcement learning algorithms to perform effectively . However, it was previously thought that the combination of online RL algorithms with deep neural networks was fundamentally unstable . Instead, a variety of solutions have been proposed to stabilize the algorithm ."
9,"rl, learning, line, reinforcement, off-policy, updates",line RL updates are strongly correlated . Aggregating over memory in this way reduces non-stationarity and decorrelates updates .
11,"deep, learning, reinforcement, agents, parallel","In this paper we provide a very different paradigm for deep reinforcement learning . Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment . This parallelism also decorrelates agents' data into a more stationary process ."
12,"atari, cpu, multi, multi-core, 2600",Parallel reinforcement learning paradigm also offers practical benefits . Our experiments run on a single machine with a standard multi-core CPU . asynchronous reinforcement learning achieves better results in far less time .
15,"advantage, actor-critic, asynchronous","the best of the proposed methods, asynchronous advantage actorcritic , also learned general strategies for exploring 3D mazes purely from visual inputs . We believe that the success of A3C on both 2D and 3D games, discrete and continuous action spaces, as well as its ability to train feedforward"
17,"asynchronous, learning, dqn, re-inforcement, agents, training","The General Reinforcement Learning Architecture of performs asynchronous training of reinforcement learning agents in a distributed setting . Each process contains an actor that acts in its own copy of the environment, a separate replay memory, and a learner that samples data from the replay memory . The updated policy parameters are sent to the actor"
18,"actor-learner, appro, linear, approximation, function",Parallelism was used to speed up large matrix operations but not to parallelize the collection of experience . Sarsa algorithm uses multiple actor-learners to accelerate training . Each actor learns separately and periodically sends updates to weights .
19,"dynamic, optimization, asynchronous, properties, technical, assumptions, setting, programming, convergence",studied convergence properties of Qlearning in the asynchronous optimization setting . These results show that Q-learning is still guaranteed to converge when some of the information is outdated as long as outdated information is always eventually eventually discarded .
23,"reinforcement, agent, learning, setting","the agent interacts with an environment 3 over discrete time steps . At each time step t, the agent receives a state St and selects an action at from some set of possible actions A according to its policy . The process continues until the agent reaches a terminal state after which the process restarts ."
24,"under, state, action, value, policy, s","the action value Q = E is the expected return for selecting action a in state s and following policy . The optimal value function Q* = max Q gives the maximum action value . Similarly, the value of states s under policy is defined as V = E ."
25,"model-free, learning, reinforcement, action, value, function",Q is an approximate action-value function with parameters 0. The updates to 0 can be derived from a variety of reinforcement learning algorithms . Q-learning aims to directly approximate the optimal action value function: Q* 21 Q .
27,"q-learning, state, pair, action",one-step Q-learning updates action value Q toward the onestep return r + 2 max ' Q . The values of other state action pairs are affected only indirectly through the updated value Q - a that led to the reward . This can make the learning process slow since many updates are required to propagate a
29,"state, propagating, action, pair, rewards, state-action","In n-step Q-learning, Q is updated toward the nstep return defined as rt + Yrt+1 +   + yn-1 + maxa YnQ . This results in a single reward r directly affecting the values of n preceding state action"
30,"ascent, gradient, model-, value-based, free, reinforce, methods","Standard REINFORCE updates the policy parameters 0 in the direction  log Rt, which is an unbiased estimate of V E . It is possible to reduce the variance of this estimate while keeping it unbiased by subtracting a learned function of the state bt from the return ."
31,"bt, value, baseline, actor-critic, architecture, function","the quantity Rt - bt used to scale the policy gradient can be seen as an estimate of the advantage of action at in state St, or A = Q - V . This approach can be viewed as an actor-critic architecture where the policy is the actor ."
33,"rl, policies, net-work, actor-critic",We present multi-threaded asynchronous variants of one-step Sarsa . The aim was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements .
34,"actor-learners, cpu, asynchronous, ple, threads, framework, gorila, multi-",Keeping learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates .
45,"actors, algorithm, dqn, training, parallel","learners running in parallel are likely to be exploring different parts of the environment . Moreover, one can explicitly use different exploration policies in each actor-learner to maximize this diversity ."
46,"stabilization, actor-critic, stabilizing, learning","using multiple parallel actor-learners has multiple practical benefits . First, we obtain a reduction in training time roughly linear in the number of parallel actors . Second, since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods such as Sarsa and"
47,"q-learning, one-step, loss, asynchronous",Asynchronous one-step Q-learning: Pseudocode is shown in Algorithm 1 . Each thread interacts with its own copy of the environment . We accumulate gradients over multiple timesteps before they are applied .
50,"performance, e-greedy, exploration, policy",giving each thread a different exploration policy helps improve robustness . Adding diversity to exploration in this manner also improves performance through better exploration .
51,"one-step, sarsa, timestep, asynchronous",Asynchronous one-step Sarsa uses a different target value for Q . The target value used is r + YQ where a' is the action taken in state s'
52,"q-learning, backpropagation, n-step","In order to compute a single update, the algorithm first selects actions using its exploration policy for up to tmax steps or until a terminal state is reached . The algorithm then computes gradients for n-step Q-learning updates for each of the state-action pairs encountered since the last update . Each "
53,"value, advantage, actor-critic, function",The algorithm maintains a policy and an estimate of the value function V . The policy and value function are updated after every tmax actions or when a terminal state is reached .
55,"value-based, neural, methods, training, network, convolutional, stability","the parameters 0 of the policy and 0v of the value function are shown as separate for generality, but we always share some of the parameters in practice . We typically use a convolutional neural network that has one softmax output ."
56,"term, regularization, entropy","we found that adding the entropy of the policy to the objective function improved exploration by discouraging premature convergence to suboptimal deterministic policies . This technique was originally proposed by , who found that it was particularly helpful on tasks requiring hierarchical behavior ."
60,"analysis, algorithms, rl, 2600, simulator, environment, benchmark, games, atari",We perform most of our experiments using the Arcade Learning Environment . This is one of the most commonly used benchmark environments for RL algorithms . We use the Atari domain to compare against state of the art results .
64,"a3c, labyrinth, algorithm, lab",Mujoco and Labyrinth are a physics simulator for evaluating agents on continuous motor control tasks with contact dynamics . The exact details of our experimental setup can be found in Supplementary Section 8 .
66,"asynchronous, controllers, neural, meth-, ods, network, speed, training","Figure 1 compares learning speed of the DQN algorithm trained on an Nvidia K40 GPU with the asynchronous methods trained using 16 CPU cores on five Atari 2600 games . The n-step method learn faster than one-step methods on some games, while the policy-based advantage actor-critic"
67,"norm, gradient, clipping, asynchronous, advantage, actor-critic, games, atari","In order to compare with the state of the art in Atari game playing, we largely followed the training and evaluation protocol of 57 Atari games . Specifically, we tuned hyperparameters using a search on six Atari Games and then fixed all 57 games for each ."
70,"advantage, actor-critic, asynchronous",Table 1 shows the average and median human-normalized scores obtained by our agents trained by asynchronous advantage actor-critic . A3C significantly improves on state-of-the-art the average score over 57 games in half the training time of the other methods while using only 16 CPU cores and no GPU 
74,"agent's, a3c, controlling, velocity, agent","A3C was the best performing agent, reaching between 75% and 90% of the score obtained by a human tester on all four game configurations in about 12 hours of training . We used the same neural network architecture as the one used in the Atari experiments specified in Supplementary Section 8 ."
76,"body, asynchronous, rigid, engine, physics, advantage-critic",Asynchronous Advantage-Critic found good solutions in less than 24 hours of training and typically in under a few hours . We also looked at a set of rigid body physics domains with contact dynamics where the tasks include many examples of manipulation and locomotion .
78,"a3c, random, apple, repair, maze",The specific task we considered involved the agent learning to find rewards in randomly generated mazes . Each maze contained two types of objects that the agent was rewarded for finding . Entering a portal led to a reward of 10 . An episode terminated after 60 seconds after which a new episode would begin .
83,"actor-learners, speed, parallel, up, games, atari","Table 2 shows the training speed-up achieved by using increasing numbers of parallel actor-learners averaged over seven Atari games . All four methods achieve substantial speedups from using multiple worker threads, with 16 threads leading to at least an order of magnitude speedup . This confirms that our proposed framework scale"
84,"asynchronous, superlinear, one-step, q-learning, speedups",asynchronous one-step Q-learning and Sarsa algorithms exhibit superlinear speedups that cannot be explained by purely computational gains . We believe this is due to positive effect of multiple threads to reduce bias in one step methods . These effects are shown more clearly in Figure 3 .
90,"algorithms, asynchronous, learning, random, initialization, rate","For each of the four algorithms we trained models on five games using 50 different learning rates and random initializations . Figure 2 shows scatter plots of the resulting scores for A3C, while Supplementary Figure S11 shows plots for the other three methods . The fact that there are virtually no points with scores of 0"
92,"asynchronous, reinforcement, algorithms, learning","4 standard reinforcement learning algorithms train neural network controllers on a variety of domains in a stable manner . Our results show that in our proposed framework stable training of neural networks through reinforcement learning is possible with both value-based and policy-based methods, off-policy as well as onpolicy methods ."
93,"stable, actor-learning, replay, online, dqn, experience, q-learning, parallel",using parallel actorlearners to update a shared model had a stabilizing effect on the learning process of the three value-based methods we considered . Incorporating experience replay into the asynchronous reinforcement learning framework could not be useful .
95,"deep, asynchronous, learning, value-based, reinforcement, framework, methods",Combining other existing reinforcement learning methods or recent advances in deep reinforcement learning with our asynchronous framework presents many possibilities for immediate improvements to the methods we presented . All of the value-based methods we investigated could benefit from different ways of reducing overestimation bias of Q-values .
96,"value-based, methods, network, architecture",the dueling architecture of has been shown to produce more accurate estimates of Q-values . The spatial softmax proposed by could improve both value-based and policy-based methods .
117,"deep, reinforcement, learning","Human-level control through deep reinforcement learning . Nature, 518:529-533, 02 2015. doi  org/ 1 0  1038 ."
143,"asynchronous, sgd, momentum, vector, setting",Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and well studied . Let 0 be the parameter vector that is shared across all threads and let 0i be the accumulated gradients of the loss with respect to parameters 0 computed by thread number i .
145,"gradients, rmsprop, elementwise, shared","We experimented with two versions of the algorithm . In one version, RMSProp, each thread maintains its own g shown in Equation S2 . Sharing statistics among threads also reduces memory requirements ."
146,"optimization, learning, asynchronous, random, rates, initialization, network",Figure S5 shows a comparison of the methods for two different reinforcement learning methods on four different games . Each curve shows the scores for 50 experiments that correspond to 50 different random learning rates and initializations . The x-axis shows the rank of the model after sorting in descending order by final average score .
149,"threads, actor-learner, network",Each experiment used 16 actor-learner threads running on a single machine and no GPUs . All methods performed updates after every 5 actions and shared RMSProp was used for optimization . The Atari experiments used the same input preprocessing as and an action repeat of 4.
150,"logu, orm, nif","Value based methods sampled the exploration rate E from a distribution taking three values €1, E2, E3 with probabilities 0.4, 0.3 and 0.3 respectively over the first four million frames . Advantage actor-critic used entropy regularization with a weight B = 0.01 for all Atari and "
152,"contact, asynchronous, advantage, algorithm, model, actor-critic, physics","the necessary setup is nearly identical to that used in the discrete action domains . However, the rewards and thus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco ."
153,"physical, state, value, network, function","For all the domains we attempted to learn the task using the physical state as input . The physical state consisted of the joint positions and velocities as well as the target position if the task required a target . In the low dimensional physical state case, the inputs are mapped to a hidden state"
156,"cpu, learning, curves, distribution, normal, cost","the asynchronous advantage actor-critic algorithm finds solutions for all the domains . Figure S8 shows learning curves against wall-clock time, and demonstrates that most domains from states can be solved within 24 hours ."
208,"a3c, ff, lstm",1 day A3C FF LSTM Alien 570.2 813.5 1033.4 1486.5 900.5 182.1 518 .4 945.3 Amidar 133.4 189.2 169.1 172.7 218.4 283.9 263.9 173.0 Assault 3332.3 11
