element_idx,keywords,summarized_text
3,"imagenet, computer, stance, vision, pre-training, in-, segmentation",We report competitive results on object detection and instance segmentation on the COCO dataset . The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system that were optimized for fine-tuning pretrained models .
5,"target, computer, segmentation, vision, task, image","In recent years, a well-established paradigm has been to pre-train models using large-scale data and then to fine-tune the models on target tasks that often have less training data . Pre-training has enabled state-of-the-art results on many tasks ."
10,"detection, pre-training, object, competitive","we report competitive object detection and instance segmentation accuracy is achievable when training on COCO from random initialization . We find that there is no fundamental obstacle preventing us from training from scratch, if: we use normalization techniques appropriately for optimization ."
13,"imagenet, model, random, initialization, large, comparability",We show that training from random initialization on COCO can be on par with ImageNet pre-training counterparts for a variety of baselines that cover Average Precision from 40 to over 50 . We also find that such comparability holds even if we train with as little as 10% COCO training data .
14,"imagenet, low-/mid-level, features, pre-training, up, speeds, convergence","ImageNet pre-training speeds up convergence, especially early on in training . but training from random initialization can catch up after training for a duration that is roughly comparable to the total ImageNet Pre-training plus fine-tuning computation-it has to learn the low-/mid-level features that are otherwise given by pre"
15,"imagenet, fine-tuning, pre-training","ImageNet pre-training does not automatically give better regularization . We find that new hyperparameters must be selected for fine-tuning to avoid overfitting . When training from random initialization using the same hyper-parameter, the model can match the accuracy without any extra regularization, even with only 10% CO"
16,"imagenet, pre-training, spatial, localization","ImageNet pre-training shows no benefit when target tasks/metrics are more sensitive to spatially welllocalized predictions . We observe a noticeable AP improvement for high box overlap thresholds when training from scratch . Intuitively, the task gap between the classification-based, ImageNet-like Pre-training and"
17,"imagenet, vision, computer, pre-training","ImageNet pre-training is a historical workaround for when the community does not have enough target data or computational resources to make training on the target task doable . In addition, ImageNet has been largely thought of as a 'free' resource thanks to the readily conducted annotation efforts and wide availability of pretrained models "
19,"imagenet, fine-tuning, net-work, net-works","the initial breakthrough of applying deep learning to object detection was achieved by fine-tuning networks that were pre-trained for ImageNet classification . Recent work pushes this paradigm further by pre-training on datasets that are 6x , 300x, and even 3000x larger than ImageNet ."
22,"detection-specific, training, architecture",Shen et al. argued for new design principles to obtain a detector that is optimized for the accuracy when trained from scratch . They designed a specialized detector driven by deep supervised networks and dense connections .
23,"imagenet, pre-training, architecture",Our focus is on understanding the role of ImageNet pre-training on unspecialized architectures . Our study demonstrates that it is often possible to match fine-tuning accuracy when training from scratch .
26,"imagenet, architectures, improvements, architecture, pre-training",Our goal is to ablate the role of ImageNet pretraining via controlled experiments . We describe the only two modifications that we find to be necessary .
28,"normalization, classifier, activation, training, image",Image classifier training requires normalization to help optimize optimization . Successful forms of normalization include normalized parameter initialization and activation normalization layers . Overlooking normalization can give misperception that detectors are hard to train from scratch .
29,"normalization, batch, sizes, training, bn","Batch Normalization , the popular method used to train modern networks, partially makes training detectors from scratch difficult . Object detectors are typically trained with high resolution inputs, unlike image classifiers . This reduces batch sizes as constrained by memory ."
31,"gn, statistics, batch",GN performs computation that is independent of the batch dimension . Synchronized Batch Normalization is an implementation of BN with batch statistics computed across multiple devices .
39,"semantics, imagenet, pre-training, image","ImageNet pre-training involves over one million images iterated for one hundred epochs . In addition to any semantic information learned from this large-scale data, the model has to learn low- and high-level features that do not need to be re-learned ."
40,"schedules, image-level, samples, training, fine-tuning, image","We consider three rough definitions of 'samples' - the number of images, instances, and pixels that have been seen during all training iterations . We plot the comparisons on the numbers of samples in Figure 2."
41,"image-level, samples, counterpart, fine-tuning, counting",Figure 2 shows a from-scratch case trained for 3 times more iterations than its fine-tuning counterpart on COCO . The sample numbers only get closer if we count pixellevel samples . This suggests that a sufficiently large number of total samples are required for the models trained from random initial
46,"r-cnn, mask, rpn, fpn, architecture, network",We investigate Mask R-CNN with ResNet or ResNeXt plus Feature Pyramid Network backbones . GN/SyncBN is used to replace all 'frozen BN' layers .
47,"longer, learning, detectron, training, rate","Original Mask R-CNN models in Detectron were fine-tuned with 90k iterations . We use similar terminology, e.g., a so-called '6x schedule' has 540k . Following the strategy in the 2x schedule, we always reduce the learning rate"
75,"augmentation, scale, training-time, data",Detectron: the shorter side ofimages is randomly sampled from pixels . Stronger data augmentation requires more iterations to converge . We increase the schedule to 9x when training from scratch .
79,"r-cnn, system, faster, cascade, two-stage","tra stages to the standard two-stage Faster R-CNN system by simply adding a mask head to the last stage . To save running time for the from-scratch models, we train Mask R-cNN from scratch without cascade ."
84,"backbone, coco, modeling, competition",This backbone has 4x more FLOPs than R101 . It achieves good results of 50.9 bbox AP and 43.2 mask AP . We submitted this model to COCO 2018 competition .
86,"imagenet, multiscale, dsod, augmentation, pre-training",DSOD reported 29.3 bbox AP using an architecture specially tailored for results of training from scratch . A recent work of CornerNet reported 42.1 using no ImageNet pre-training . Our results are higher than previous ones .
90,"r50-fpn, coco","Keypoint detection on COCO using Mask R-CNN with R50-FPN and GN . ImageNet pre-training has little benefit, and training from random initialization can quickly catch up without increasing training iterations ."
91,"detection, imagenet, spatial, keypoint, localization, pre-training",Keypoint detection is a task more sensitive to fine spatial localization . We train Mask R-CNN for the COCO human keypoint detection task .
92,"normalization, activation, modeling, scratch",Models without BN/GN - VGG nets can be trained from scratch without activation normalization . So far all of our experiments involve ResNet-based models . Our next experiment tests the generality of our observations by exploring the behavior of training Faster R-CNN from scratch .
93,"fpn, modeling, training, model","We adopt standard hyper-parameters with a learning rate of 0.02, learning rate decay factor of 0.1, and weight decay of 0.0001 . We use scale augmentation during training ."
98,"full, computation, computer, training, fine-tuning, convergence",VGG-16 reaches a similar level of performance with a maximum bbox AP of 35.2 after an 11 x schedule . This results indicate that our methodology of 'making minimal/no changes' but adopting good optimization strategies and training for longer are sufficient for training comparably performant detectors .
106,"imagenet, models, training, pre-training, smaller","We repeat the same set of experiments on a smaller training set of 10k COCO images . Again, we perform grid search for hyper-parameters on the models that use ImageNet pre-training ."
109,"imagenet, coco, training, pre-training, loss","In Figure 8 we repeat the same set of experiments using only 1k COCO training images and show the training loss . In terms of optimization, training from scratch is still no worse but only converges more slowly, as seen previously . For one experiment only we performed a grid search to optimize the from-scratch"
115,"detection, pascal, object, voc","Using ImageNet pre-training, our Faster RCNN baseline has 82.7 mAP at 18k iterations . Its counterpart trained from scratch in VOC has 77.6 ."
116,"image, coco, training, voc",There are 15k VOC images used for training . But these images have on average 2.3 instances per image and 20 categories . We suspect that the fewer instances has a similar negative impact .
126,"target, imagenet, computation, data, task","ImageNet can help speed up convergence, but does not necessarily improve accuracy unless the target dataset scale is too small . This suggests that collecting annotations of target data can be more useful for improving the target task performance ."
127,"imagenet, vision, computer, pre-training","ImageNet pre-training has been a critical auxiliary task for the computer vision community to progress . It enabled people to see significant improvements before larger-scale data was available . ImageNet reduced research cycles, leading to easier access to encouraging results ."
128,"imagenet, classification-level, clean-ing, data, pre-training",a generic largescale classification-level pre-training set is not ideal if we take into account the extra effort of collecting and cleaning data-the cost of collecting ImageNet has been ignored . but the 'pre-training' step is in fact not free when we scale out this paradigm .
129,"self-, learning, representations, community, supervised, universal",Our results do not mean deviating from this goal . Our study suggests that the community should be more careful when evaluating pre-trained features .
133,"deep, imagenet, networks, recognition, classification, visual, convolutional","ECCV, 2014. 2 J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization . In CVPR, 2018. 5 J. Carreira and A. Zisserman . Quo vadis, action recognition?"
134,"backbone, recognition, network, visual, backpropagation","In CVPR, 2018. 1, 2, 8 0. Matan, C. J. Burges, Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation"
136,"deep, learning, networks, neural, large-scale, recognition, visual, image","In ICCV, 2017. 2, 6 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition . In ICLR, 2015. 3, 6 C. Sun, A. Shrivastava, S. Singh, and D. Erhan. Deep neural networks for object detection"
