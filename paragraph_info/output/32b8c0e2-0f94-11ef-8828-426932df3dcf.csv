element_idx,keywords,summarized_text
6,"computational, word, similarity, quality, cost",We propose two novel model architectures for computing continuous vector representations of words from very large data sets . The quality of these representations is measured in a word similarity task . We observe large improvements in accuracy at much lower computational cost .
8,"nlp, atomic, word, model, simple, units",many current NLP systems and techniques treat words as atomic units . There is no notion of similarity between words . Simple models trained on huge amounts of data outperform complex systems trained on less data .
9,"techniques, automatic, recognition, speech, simple","simple techniques are at their limits in many tasks . For example, the amount of relevant in-domain data for automatic speech recognition is limited . In machine translation, the existing corpora for many languages contain only a few billions of words ."
16,"of, representative, vector, similarity, tions, quality","This has been observed earlier in the context of inflectional languages . for example, nouns can have multiple word endings . if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings."
18,"accuracy, word, vector, linear, regularities","In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words . We design a new comprehensive test set for measuring both syntactic and semantic regularities1 ."
20,"nnlm, model, neural, statistical, network, language",A very popular model architecture for estimating neural network language model was proposed in . a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn the word vector representation . This work has been followed by many others.
21,"nnlm, word, neural, vector, network","NNLM was presented in , where the word vectors are first learned using neural network with a single hidden layer . The words vectors were then used to train the NNML . Thus, the word Vectors are learned even without constructing the full NNL ."
22,"nlp, word, model, vector, log-bilinear","Estimation of the word vectors was performed using different model architectures and trained on various corpora . However, these architectures were significantly more computationally expensive for training than the one proposed in ."
24,"dirichlet, se, analysis, (lda), well-known, latent, semantic, (lsa), allocation","In this paper, we focus on distributed representations of words learned by neural networks . LDA becomes computationally very expensive on large data sets ."
31,"probabilistic, nnlm, layer, feedforward, model, neural, network, language, projection","NNLM architecture becomes complex for computation between the projection and the hidden layer . The hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V ."
32,"softmax, dominating, term","the dominating term is H x V . However, several practical solutions were proposed for avoiding it . either using hierarchical versions of the softmax , or avoiding normalized models completely ."
33,"normalization, softmax, huffman, neural, binary, network, hierarchical",Hierarchical softmax uses the vocabulary as a Huffman binary tree . This follows previous observations that the frequency of words works well for obtaining classes in neural networks .
35,"nnlm, connections, model, neural, time-delayed, recurrent, network, language, based","Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM . The RNN model does not have a projection layer; only input, hidden and output layer . This allows the recurrent model to form some kind of short term memory ."
40,"sets, replica, model, distbelief, data",We have implemented several models on top of a large-scale distributed framework called DistBelief . The framework allows us to run multiple replicas of the same model in parallel . Each replica synchronizes its gradient updates through a centralized server .
42,"neural, complexity, representation, networks","In this section, we propose two new models for learning distributed representations of words that try to minimize computational complexity . The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model ."
43,"language, nnlm, network, model","neural network language model can be successfully trained in two steps . first, continuous word vectors are learned using simple model . then the N-gram NNLM is trained on top of these distributed representations of words ."
45,"bag-of-words, bag-of-wor, model","the first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words . we also use words from the future; we have obtained the best performance on the task introduced in the next section ."
48,"word, classifier, current, log-linear, cbow","We use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word . We found that increasing the range improves quality of the resulting vectors, but it also increases the computational complexity ."
56,"quality, similarity, word","previous papers typically use a table showing example words and their most similar words, and understand them intuitively . We follow previous observation that there can be many different types of similarities between words . Example of another type of relationship can be word pairs big - biggest and small - smallest ."
57,"questions, vector, representation","To find a word that is similar to small in the same sense as big is similar . We can simply compute vector X = vector vector + vector . Then, we search in the vector space for the word closest to X measured by cosine distance ."
58,"nlp, retrieval, information, word, vector, systems, question, answering","Word vectors with such semantic relationships could be used to improve existing NLP applications, such as machine translation, information retrieval and question answering systems ."
63,"of, word, questions, syntactic, semantic, quality, vectors","To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions . The questions in each category were created in two steps: first, a list of similar word pairs was created manually, then a large list of questions is formed by"
64,"synonyms, accuracy, metric","Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question . This means that reaching 100% accuracy is likely to be impossible, as the current models do not have input information about word morphology . Further progress can be achieved by"
66,"co, google, corpus, news",We have used a Google News corpus for training the word vectors . This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words . The results using the CBOW architecture with different choice of word vector dimensionality are shown in Table 2.
76,"semantic-syntactic, test, word, similarity, semantic, set, relationship","In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary . We also include results on a test set introduced in that focuses on syntactical similarity between"
77,"corpora, nnlm, model, distbelief, training, network, language, parallel, ldc",The training data consists of several LDC corpora . We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU .
78,"cbow, skip-gram, nnlm, vector, architecture, shp-gram, shp","In Table 3, it can be seen that the word vectors perform well mostly on the syntactic questions . The NNLM vectors are directly connected to a hidden layer . This is not surprising, as the words in the RNNLM perform significantly better ."
97,"log-biline, model, microsoft, challenge, sentence, log-bilinear, completion","Microsoft Sentence Completion Challenge has been introduced as a task for advancing language modeling and other NLP techniques . This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence . Performance of several techniques has"
98,"sentence, skip-gram, architecture","We train the 640dimensional model on 50M words provided in . First, we compute score of each sentence in the test set . The final sentence score is then the sum of these individual predictions ."
99,"lsa, skip-gram, similarity, model",Skip-gram model does not perform on this task better than LSA similarity . a weighted combination leads to a new state of the art result 58.9% accuracy .
105,"dimensionality, sets, table, larger, data, 8","We believe word vectors trained on even larger data sets with larger dimensionality will perform significantly better . By using ten examples instead of one to form the relationship vector, we have observed improvement of accuracy by about 10% absolutely on the semantic-syntactic test ."
106,"value, vector, average, information","It is also possible to apply the vector operations to solve different tasks . For example, we have observed good accuracy for selecting out-of-the-list words ."
108,"word, representations, model, distbelief, quality","In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks . We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models ."
109,"nlp, detection, word, neural, paraphrase, rnn, network, based, vectors",An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 .
110,"analysis, relational, latent, ana",results from machine translation experiments look promising . We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors .
113,"code, nips, 2013, c++, paper","We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words per hour . Some of our follow-up work will be published in an NIPS 2013 paper ."
115,"analysis, representations, model, modeling, network, language, based","Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model . In: Large-Scale Kernel Machines, MIT Press, 2003 . T. Brants, A. C. Popat and P. Xu, F. J. Och"
118,"propagation, errors, propagating, back-","T. Mikolov, W.T. Yih, G. Zweig. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012. A. Mnih . G. Hinton. A Scalable Hierarchical Distributed Language Model . AIST"
