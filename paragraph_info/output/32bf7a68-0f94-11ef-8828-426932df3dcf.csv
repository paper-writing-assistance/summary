element_idx,keywords,summarized_text
3,"ppo, complexity, optimization, gradient, region, trust, policy",policy gradient methods alternate between sampling data through interaction with the environment . We propose a novel objective function that enables multiple epochs of minibatch updates . The new methods have some of the benefits of trust region policy optimization .
5,"optimization, region, trust, policy, approximation, function",Q-learning fails on many simple problems1 and is poorly understood . vanilla policy gradient methods have poor data effiency and robustness . trust region policy optimization is relatively complicated .
6,"of, optimization, state, communication, trpo, affair","This paper seeks to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO . To optimize policies, we alternate between sampling data from the policy and performing several epochs of optimization on the sampled data ."
7,"clipped, ppo, ratios, probability, surrogate, objectivity","Our experiments compare the performance of various different versions of the surrogate objective . We also compare PPO to several previous algorithms from the literature . On continuous control tasks, it performs better than A2C ."
14,"automatic, differentiation, policy, stochastic","expectation Et indicates the empirical average over a finite batch of samples, in an algorithm that alternates between sampling and optimization . Implementations that use automatic differentiation software work by constructing an objective function whose gradient is the policy gradient estimator; the estimator g is obtained by differentiating the objective ."
20,"equation, objective, penalty, b, coefficient, penalized","TRPO uses a hard constraint rather than a penalty because it is hard to choose a single value of B that performs well across different problems . To achieve our goal of a first-order algorithm that emulates the monotonic improvement of TRPO, experiments show that it is not sufficient to simply select a fixed"
26,"lcpi, co, combustion",Figure 1 plots a single term in LCLIP; note that the probability ratio r is clipped at 1 - 6 or 1 + E depending on whether the advantage is positive or negative .
34,"clipped, objective, surrogate, penalty","In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we've included it here because it's an important baseline."
38,"kl, divergence, update, algorithm, policy","the updated is used for the next policy update . The KL divergence is significantly different from dtarg . However, these are rare and quickly adjusts ."
43,"value, function, error, estimators, term, finite-horizon","if using a neural network architecture that shares parameters between the policy and value function, we must use a loss function that combines the policy surrogate and a value function error term . This objective can further be augmented by adding an entropy bonus ."
60,"algorithms, gym, openai, robotics, hyperparameters",We used 7 simulated robotics tasks2 implemented in OpenAI Gym . We do one million timesteps of training on each one .
75,"3d, robots, humanoid, robot","To showcase the performance of PPO on high-dimensional continuous control problems, we train on a set of problems involving a 3D humanoid . The robot must run, steer, and get up off the ground, possibly while being pelted by cubes . In concurrent work, Heess"
82,"wan+16, wan+, acer",We also ran PPO on the Arcade Learning Environment benchmark and compared against well-tuned implementations of A2C and ACER . The hyperparameters for PPO are provided in Table 5.
83,"appendix, metrics, learning, b, curves, average, reward, scoring","Table 2 shows the number of games ""won"" by each algorithm . We compute the victor by averaging the scoring metric across three trials ."
87,"code, optimization, region, trust, policy, change",proximal policy optimization uses multiple epochs of stochastic gradient ascent to perform each policy update . These methods have the stability and reliability of trust-region methods but are much simpler to implement .
