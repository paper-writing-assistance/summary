element_idx,summarized_text,keywords
5,Deformable DETR can achieve better performance than DETR with 10x less training epochs . Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach .,"spatial, feature, image, detr, maps, resolution"
7,"Modern object detectors employ many hand-crafted components . They are not fully end-to-end . DETR uses a simple architecture, by combining convolutional neural networks and Transformer encoders .","detr, hand-crafted, components, rules"
8,"DETR requires much longer training epochs to converge than the existing object detectors . For example, on the COCO benchmark, DETR needs 500 converge . This is around 10 to 20 times slower than Faster R-CNN .","image, detr, maps, feature"
16,"Deformable DETR combines the best of the sparse spatial sampling of deformable convolution, and the relation modeling capability of Transformers . We use the module to replace the Transformer attention modules processing feature maps, as shown in Fig. 1.","attention, deformable, module, transformer"
17,Deformable DETR opens up possibilities for us to exploit variants of end-to-end object detectors . We explore a simple and effective iterative bounding box refinement mechanism .,"iterative, bounding, refinement, detr, box, deformable"
18,Deformable DETR can achieve better performance with 10x less training epochs . Code is released at https :/ / github. com/ fundamentalvision /Deformable-DETR.,"detr, coco, deformable, two-stage"
20,"Transformers involve both self-attention and cross-attention mechanisms . One of the most well-known concern of Transformers is the high time and memory complexity at vast key element numbers, which hinders model scalability in many cases .","mechanism, transformers, attention, efficient"
21,"The first category is to use pre-defined sparse attention patterns on keys . The most straightforward paradigm is restricting the attention pattern to be fixed local windows . To compensate, Child et al.","pattern, attention, neighborhood, sparse, local"
27,Ramachandran et al. admits such approaches are much slower in implementation than traditional convolution with the same FLOPs .,"image, mechanism, design, attention, domain"
28,"variants of convolution can be viewed as self-attention mechanisms . Deformable convolution operates much more effectively and efficiently on image recognition than Transformer . In addition, it lacks the element relation modeling mechanism .","self-attention, convolution, deformable, dynamic"
30,FPN proposes a top-down path to combine multi-scale features . Kong et al. combines features from all scales by a global attention operation .,"multi-scale, features, attention, mecha-nism, representation"
32,"Multi-Head Attention in Transformers is of a network architecture based on attention mechanisms for machine translation . Given a query element and a set of key elements, the multi-head attention module adaptively aggregates the key contents according to the attention weights that measure the compatibility of query-key pairs .","module, multi-head, attention"
35,"attention weights Amqk exp  are normalized as VCU EKESUK . Um, Vm E RCvxC are also learnable weights .","head, attention, learnable, weights"
36,"Transformers need long training schedules before convergence . Suppose the number of query and key elements are of Nq and Nk respectively . It will lead 0 and variance of 1, which makes attention weights Amqk 22 Nk .","training, transformers, long, convergence, schedules"
37,"computational complexity of Eq. 1 is of O . In the image domain, where the query and key elements are both of pixels, Nq = Nk  C, the complexity is dominated by the third term, as O. Thus, the multi-head attention module suffers from a quadratic complexity growth with","multi-head, module, attention"
39,DETR exploits a standard Transformer encoder-decoder architecture to transform the input feature maps to be features of a set of object queries . A 3-layer feed-forward neural network and a linear projection are added on top of the object query features as the detection head . The linear projection acts as the classification branch,"input, feature, coordinates, regression, branch, bounding, maps, backbone, box, cnn"
41,"The input includes both feature maps from the encoder and N object queries represented by learnable positional embeddings . In the cross-attention modules, object queries extract features from the feature maps . The query elements are of the object queries, and key elements are .","feature, detr, self-attention, attention, maps, module"
42,DETR has relatively low performance in detecting small objects . Modern object detectors use high-resolution feature maps to better detect small objects. This is mainly because the attention modules processing image features are difficult to train .,"features, image, detr, attention, transformer"
50,The core issue of applying Transformer attention on image feature maps is that it would look over all possible spatial locations . The deformable attention module only attends to a small set of key sampling points around a reference point .,"feature, image, module, attention, maps, transformer, deformable"
52,pmqk and Amqk denote the sampling offset and attention weight of the kth sampling point in the mth attention head respectively . The scalar attention weight amqk lies in the range .,"softmax, hardness, attention, head, operator"
53,"the deformable attention module is designed for processing convolutional feature maps as key elements . Nq = HW, the complexity becomes O, which is of linear complexity with the spatial size .","cross-attention, convolutional, feature, attention, maps, module, deformable"
59,Function 1 in Equation 3 re-scales the normalized coordinates Pq to the input feature map of the l-th level . The multi-scale deformable attention is very similar to the previous single-scale version .,"head, attention, deformable, multi-scale"
60,"Deformable convolution is designed for single-scale inputs . The proposed attention module can be perceived as an efficient variant of Transformer attention, where a pre-filtering mechanism is introduced .","inputs, multi-scale, convolution, attention, multiscale, module, deformable"
61,"In encoder, we extract multi-scale feature maps x1 from the output feature maps of stages C3 through C5 in ResNet . The lowest resolution feature map xL is obtained via a 3 x 3 stride 2 convolution on the final stage, denoted as C6 .","multi-scale, deformable, encoder, detr, attention, module, transformer"
62,"the key and query elements are of pixels from the multi-scale feature maps . For each query pixel, the reference point is itself . The scale-level embedding eli=1 are randomly initialized .","multi-scale, feature, attention, maps, module, deformable"
63,cross-attention and self-attention modules are in the decoder . The query elements for both types of attention modules are of object queries .,"self-attention, transformer, module, deformable, decoder"
67,"detection head predicts the bounding box as relative offsets w.r.t. the reference point . In this way, learned decoder attention will have strong correlation with predicted bounding boxes .","multi-scale, attention, multiscale, module, deformable"
73,"To avoid this problem, we remove the decoder and form an encoder-only Deformable DETR for region proposal generation . In it, each pixel is assigned as an object query, which directly predicts a bounding box .","detr, deformable, proposals, region"
76,ImageNet pre-trained ResNet-50 is used as the backbone for ablations . Multi-scale feature maps are extracted without FPN . Other hyper-parameter setting and training strategy mainly follow DETR .,"detr, deformable, transformer, encoder"
82,Deformable DETR has on par FLOPs with Faster R-CNN + FPN and DETR-DC5 . But the runtime speed is much faster than DETR . The speed issue is mainly due to the large amount of memory access .,"detr, attention, flop, transformer, deformable"
88,"Using multi-scale inputs can effectively improve detection accuracy with 1.7% AP . Increasing the number of sampling points K can further improve 0.9% AP. Because the cross-level feature exchange is already adopted, adding FPNs will not improve the performance .","attention, deformable, module, multi-scale"
90,Table 3 compares the proposed method with other state-of-the-art methods . Iterative bounding box refinement and two-stage mechanism are both used by our models in Table 3 .,"resnet-101, table, 3, resnext-101"
98,"Deformable DETR is an end-to-end object detector, which is fast-converging . It enables us to explore more interesting and practical variants of object detectors .","feature, image, modules, maps, attention, deformable"
154,"complexity for calculating sampling coordinate offsets pmqk and attention weights Amqk is of O . The complexity of computing Equation 2 is O, where the factor of 5 in 5NqKC is because of bilinear interpolation . On the other hand, we can also calculate Wm x","deformable, attention, quality, query, weights, module, of"
156,the input multi-scale feature maps are extracted from the output feature maps of stages C3 through C5 in ResNet . The lowest resolution feature map xL is obtained via a 3 x 3 stride 2 convolution on the final stage .,"multi-scale, feature, attention, multiscale, fpn, deformable, map"
160,"the detection head predicts the bounding box as relative offsets w.r.t. the reference point pq = , i.e., bq= o), and  . The detection head uses 0 and 0 to ensure 6 is of normalized coordinates .","multi-scale, point, attention, reference, module, deformable"
166,"d E 1,2, , D, bgx,y,w,h E R are predicted at the d-th decoder layer . The initial box is set as bqx = Pqx, bay= Pqy, 60 3 = 0.1,","bg, layer, b, decoder"
168,"In iterative bounding box refinement, we sample key elements respective to the box bd-1 predicted from the -th decoder layer . Equation 3 in the cross-attention module serves as the new reference point . The sampling offset Pmlqk is also modulated by the box size","bo, bounding"
169,"In the first stage, given the output feature maps of the encoder, a detection head is applied to each pixel . Let 2 index a pixel from feature level li E 1, 2,  , L with 2-d normalized coordinates Pi = E 2 .","regression, bounding, detr, head, box, deformable"
172,"In our experiments, the number of attention heads is set as M = 8 . Weight parameters of the linear projection for predicting Amlqk are initialized to zero . In multi-scale deformable Attention modules, Wm E RCvxC is randomly initialized .","attention, deformable, module, multi-scale"
175,Deformable DETR looks at to give final detection result . We draw the gradient norm of each item in final prediction . The gradient norm can reflect how much the output would be changed .,"final, detr, prediction, detection, deformable, norm, gradient"
176,Deformable DETR looks at extreme points of the object to determine its bounding box . This is similar to the observation in DETR .,"bound, bounding, detr, box, deformable"
183,"Similar to DETR, the instances are already separated in the encoder of Deformable DETR . In the decoder, our model is focused on the whole foreground instance instead of only extreme points . The visualization also demonstrates that the proposed multi-scale deformable attention module can adapt its sampling points and attention","detr, attention, multiscale, module, deformable"
190,Figure 6: Visualization of multi-scale deformable attention . Each sampling point is marked as a filled circle whose color indicates its correspoinding attention weight . The reference point is shown as green cross marker .,"multi-scale, weight, attention, multiscale, deformable"
