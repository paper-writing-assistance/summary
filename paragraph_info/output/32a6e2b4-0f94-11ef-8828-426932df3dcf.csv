element_idx,keywords,summarized_text
5,"target, gradient, stochastic, policy, pol-, deterministic, icy","In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions . It is the expected gradient of the action-value function . To ensure adequate exploration, we introduce an off-policy actor-critic algorithm ."
7,"gradient, learning, reinforcement, policy, deterministic",Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces . The basic idea is to represent the policy by a parametric probability distribution = IP that stochastically selects action a in state s according to parameter vector 0. It is natural to wonder whether the same approach can be followed as for stochastic
10,"target, gradient, stochastic, policy, deterministic","In the stochastic case, the policy gradient integrates over both state and action spaces, whereas in the deterministic case it only integrates in the state space . In order to explore the full state or action space, an off-policy policy is often necessary . The basic idea is to choose actions according to "
15,"decision, space, state, cumulative, markov, reward, discounted, process, state-action","A policy is used to select actions in the MDP . In general the policy is stochastic and denoted by : S  P, where P is the set of probability measures on A and 0 E Rn is a vector of n parameters . The agent uses its policy to interact with the M"
24,"gradient, action-value, algorithm, q, reinforce, policy, (st, function",The policy gradient theorem has been used to derive a variety of policy gradient algorithms . One issue that these algorithms must address is how to estimate the action-value function Q .
26,"theorem, gradient, action-value, stochastic, policy, actor-critic, function",The actor-critic consists of two eponymous components . An actor adjusts the parameters 0 of the stochastic policy by stocastic gradient ascent of Equation 2. An action-value function Qw is used .
28,"value, compatible, approximators, function","condition ii) says that compatible function approximators are linear in ""features"" of the stochastic policy,  log . condition requires that the parameters are the solution to the linear regression problem that estimates Q from these features ."
34,"gradient, action-value, actor-critic, actor, function","An actor updates the policy parameters 0, also off-policy from these trajectories, by stochastic gradient ascent of Equation 5 . An actor estimates a state-value function, Vv 21 V , by gradient temporal-difference learning . Both the actor and the critic use an im"
36,"theorem, gradient, stochastic, policy, deterministic",Our main result is a deterministic policy gradient theorem . First we provide an informal intuition behind the form of the policy gradient .
39,"learning, action-value, improvement, temporal-difference, methods, policy, function","The majority of model-free reinforcement learning algorithms are based on generalised policy iteration . The most common approach is a greedy maximisation of the action-value function, k+1 = argmax Q m k ."
41,"improvement, global, policy, maximisation","In continuous action spaces, greedy policy improvement becomes problematic . Instead, a simple and computationally attractive alternative is to move the policy in the direction of the gradient of Q, rather than globally maximising Q . Each state suggests a different direction of policy improvement; these may be averaged together by taking an expectation with respect to"
43,"jacobian, gradient, state, 0, matrix, distribution","ome is a Jacobian matrix such that each column is the gradient  0 d of the dth action dimension of the policy with respect to the policy parameters 0. However, by changing the policy, different states are visited and the state distribution pu will change . As a result, it is not immediately obvious"
50,"policy, gradient, stochastic, deterministic",We parametrise stochastic policies by a deterministic policy th : S  A and a variance parameter . Then we show that as 0 = 0 the policy gradient converges .
55,"theorem, gradient, learning, off-policy, temporal-difference, policy, deterministic","We start with the simplest case - on-policy updates, using a simple Sarsa critic - SO as to illustrate the ideas as clearly as possible . We then consider the off-political case, this time using an easy Q-learning critic . These simple algorithms may have convergence issues in practice "
57,"stochastic, policy, actor-critic, deterministic","algorithm learns and follows a deterministic policy . Its primary purpose is didactic; however, it may be useful for environments in which there is sufficient noise in the environment to ensure adequate exploration . The critic estimates the action-value function while the actor ascends the gradient ."
62,"gradient, action-value, algorithm, off-policy, policy, actor-critic, deterministic, function","A critic estimates the action-value function Qw 21 Qu, off-policy from trajectories generated by B, using an appropriate policy evaluation algorithm ."
65,"policy, true, gradient, deterministic","We now find a class of compatible function approximators Qw such that the true gradient is preserved . The following theorem applies to both on-policy, E = Esp , and off-police, E= Essp . In general, substituting an approximate Qw into the"
68,"state, state-def-mo(s), action, features, function, approximator","the advantage function can be viewed as a linear function approximator, Aw = T w with statedef - mo) and paaction features  is an n x m Jacobian matrix, SO the feature vector is nx 1 and the parameter vector w is also"
69,"linear, action-value, function, approximator","a linear function approximator is not very useful for predicting action-values globally . The actionvalue diverges to 00 for large actions . However, it can still be highly effective as a local critic ."
70,qw(s,"To satisfy condition 2 we need to find the parameters w that minimise the mean-squared error between the true gradient and Qw . This can be seen as a linear regression problem with ""features""  and ""targets,""  a Qu|a=  words, features of the In other policy"
72,"q-learning, copdac, critic","the critic is a linear function approximator that estimates T  0M0 . This the action-value from features  = a may be learnt off-policy from samples of a behaviour policy B, for example using Q-learning or gradient Qlearning . The actor then updates its parameters"
73,"gradient, mean-squared, pro, projected, copdac, q-learning, error, bellman",Off-policy Q-learning may diverge when using linear function approximation . The basic idea of these methods is to minimise the mean-squared projected Bellman error by stochastic gradient descent . We use gradient temporal-difference learning in the critic .
74,"algorithms, gradient, metric, information, fisher, stochastic, natural, actor-critic",the natural policy gradient can be extended to deterministic policies . The steepest ascent direction with respect to any metric M is given by M-1 J .
75,"theorem, gradient, metric, m, policy, deterministic","For deterministic policies, we use the metric M = Esp which can be seen as the limiting case of the Fisher information metric as policy variance is reduced to zero . This algorithm can be implemented by simplifying Equations 20 or 24 to 0t+1=0t + aowt."
78,"policy, gradient, stochastic, deterministic","The problem is a continuous bandit problem with a high-dimensional quadratic cost function, -r = T C . The matrix C is positive definite with eigenvalues chosen from 0.1, 1, and a* = T We consider action dimensions of m = 10, 25, 50 "
86,"mountain, car, stochastic, algorithm, actor-critic","The stochastic actor-critic algorithm performed best out of several incremental actor critic methods in a comparison on mountain car . The critic uses a linear value function approximator V = v T  with the same features, updated by temporal-difference learning ."
87,"parameters, step-size, algorithms, copdac-q",The discount was 2 = 0.99 for mountain car and pendulum . Actions outside the legal range were capped . Figure 2 shows the performance of the best performing parameters for each algorithm .
89,"space, stochastic, action, gradients, policy, octopus, arm",The aim is to control a simulated octopus arm to hit a target . The arm consists of 6 segments and is attached to a rotating base . There are 50 continuous state variables and 20 action variables . An episode ends when the target is hit or after 300 steps .
93,"with, copdac-q, 6, octopus, dimensional, arm, segments","dimensional octopus arm with 4 segments . We applied the COPDAC-Q algorithm, using a sigmoidal multi-layer perceptron to represent the policy . The advantage function Aw was represented by compatible function approximation ."
95,"policy, gradient, stochastic, deterministic","Using a stochastic policy gradient algorithm, the policy becomes more deterministic as the algorithm homes in on a good strategy . However, the variance of the policy gradient for a Gaussian policy N is proportional to 1/02 . This problem is compounded in high dimensions, as illustrated by the"
99,"gradient, stochastic, policy, updates, copdac-q, deterministic, incremental","In our experiments COPDAC-Q was used to learn a deterministic policy, off-policy, while executing a noisy version of that policy . However, it is analogous to asking whether Q-learning or Sarsa is more efficient by measuring the greedy policy learnt by each algorithm ."
100,"gradient, learning, action-value, reinforcement, critic, neural, fitted-q","the NFQCA algorithm uses two neural networks to represent the actor and critic respectively . The actor adjusts the policy, represented by the first neural network, in the direction of the action-value gradient using an update similar to Equation 7 ."
102,"policy, algorithms, gradient, deterministic",We have presented a framework for deterministic policy gradient algorithms . These gradients can be estimated more efficiently than their stochastic counterparts .
