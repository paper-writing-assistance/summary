element_idx,keywords,summarized_text
4,gf-rnn,"the proposed RNN, gated-feedback RNN , extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing to lower layers using a global gating unit for each pair of layers . We evaluated the proposed GF-RNN on the tasks of character-level language"
6,"gating, networks, neural, recurrent, rnns, units",Recurrent neural networks have been widely studied and used for various machine learning tasks which involve sequence modeling . Recent studies have revealed that RNNs using gating units can achieve promising results .
9,gru,a gated activation function is designed to have more persistent memory SO that it can capture long-term dependencies more easily . affine transformation and point-wise nonlinearity can be achieved by modifying the RNN architecture .
10,"feedback, information, changing, longer-term, slow, rnn, dependencies",Sequences modeled by an RNN can contain both fast changing and slow changing components . These underlying components are often structured in a hierarchical manner . Koutnik et al. proposed a more explicit approach to partition the hidden units into groups .
11,"adaptive, learning, gf-rnn, multiple, timescales","the proposed RNN has multiple levels of recurrent layers like stacked RNNs do . However, it uses gated-feedback connections from upper to lower layers . This makes the hidden states across a pair of consecutive timesteps fully connected ."
24,"long-term, rnn, time-unfolded, dependencies",the difficulty of training an RNN to capture long-term dependencies has been known for long . A previously successful approach to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-lasting memory .
25,"short-term, long, memory, lstm",Lang-term memory was proposed by Hochreiter & Schmidhuber to address this issue of learning long-term dependencies . The LSTM maintains a separate memory cell inside it that updates and exposes its content only whendeemed necessary .
28,"content, ct-1, ct, memory, cell, unit, lstm","LSTM unit consists of a memory cell Ct, an input gate it, aforget gate ft, and an output gate Ot . The memory cell ct of the j-th unit at timestep t is updated similar to a gated leaky neuron ."
37,"content, memory, forget, cell, gate, lstm","an LSTM unit can adaptively forget, memorize and expose the memory content . If the detected feature is deemed important, the forget gate will be closed . This is equivalent to capturing a long-term dependency ."
39,"content, memory, gru, integration, leaky","The GRU was recently proposed by Cho et al. Like the LSTM, it was designed to adaptively reset or update its memory content . Each GRU thus has a reset gate r.t and an update gate 23t which are reminiscent of the forget and input gates ."
45,"capacity, gru, mechanism, reset","update mechanism helps the GRU to capture long term dependencies . Whenever a previously detected feature, or the memory content is considered to be important for later use, the update gate will be closed to carry the current memory content across multiple timesteps ."
48,"timescale, rnn, cw-rnn, clockwork","an RNN can capture dependencies of different timescales more easily and efficiently . The clockwork RNN implemented this by allowing the i-th module to operate at the rate of 2i-1, where i is a positive integer, meaning that the module is updated only when t mod 2.i-1 = 0."
50,"cw-rnn, stacked, rnn, gate, global, reset",This is contrary to the design of CW-RNN and the conventional stacked RNN . The recurrent connection between two modules is gated by a logistic unit which is computed based on the current input and the previous states of the hidden layers .
57,"gf-rnn, global, gates, reset","Fig. 1 illustrates the difference between the conventional stacked RNN and our proposed GF-RNN . In both models, information flows from lower recurrent layers to upper . The GF RNN further allows information from the upper layer, corresponding to finer timescales, flows back into the lower"
73,"contest, compression, hutter, knowledge, human, dataset","Hutter dataset was built from English Wikipedia . It contains 100 MBytes of characters including Latin alphabets, non-Latin alphabets and XML markups ."
76,"python, multiplication, addition, print, statement","Scripts used include addition, multiplication, subtraction, for-loop, variable assignment, logical comparison and if-else statement . The output script and the output are sequences of characters, where the input and output vocabularies respectively consist of 41 and 13 symbols."
86,"rate, rmsprop, learning","We used RMSProp and momentum to tune the model parameters . Whenever the norm of the gradient explodes, we halve the learning rate ."
95,"python, scripts, map-ping","Python program evaluation uses an RNN encoder-decoder based approach . Python scripts are fed into the encoder RNN, and the hidden state of the RNN is unfolded for 50 timesteps . The first hidden state is always initialized to zero vector ."
103,gru,"the proposed gated-feedback architecture outperforms the other baseline architectures that we have tried when used together with widely used gated units such as LSTM and GRU . However, the proposed architecture failed to improve the performance of a vanillaRNN with tanh units . In addition to the final"
109,"gf-lstm, gf-rnn, gates, global, reset",the test set BPC of GF-LSTM without global reset gates was 1.854 . This is in between the results of conventional stacked LSTM and GFLSTM with Global reset gates .
117,"rmsprop, multiplicative, gf-rnn, rnn, units, lstm","In Table 4, we present the test set BPC by a multiplicative RNN . The performance of the proposed GF-RNN is comparable to, or better than, the previously reported best results ."
123,"target, python, of, increases, number, pro-gram, levels, nesting, sequences","Fig. 3 represents the gaps between the test accuracies of stacked RNNs and GF-RNNs which are computed by subtracting from . We observed that in most of the test sets, GFNs are outperforming stacked ."
125,"python, deep, long-term, stacked, rnn, dependencies",We proposed a novel architecture for deep stacked RNNs . The architecture uses gated-feedback connections between different layers . Our experiments focused on challenging sequence modeling tasks .
129,"theano, pylearn2, theano-pylearn2",The authors would like to thank the developers of Theano and Pylearn2 . The authors thank Yann N. Dauphin and Laurent Dinh for comments and discussion .
