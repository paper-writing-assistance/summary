element_idx,summarized_text,keywords
5,"Ramachandran et al. showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks . This work provides evidence that attention layers can perform convolution . Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as","attention, vision, convolution, tasks"
7,"Recent advances in Natural Language Processing are largely attributed to the rise of the transformer . Pre-trained to solve an unsupervised task on large corpora of text, transformer-based architectures seem to possess the capacity to learn the underlying structure of text . The key difference between transformers and previous methods, such as","convolutional, neural, mechanism, attention, transformer, networks"
8,"Self-attention was first added to CNN by either channel-based attention or non-local relationships across the image . More recently, Bello et al. augmented CNNs by replacing some convolutional layers with self-attention layers, leading to improvements on image classification and object detection tasks .","image, cnn, self-attention, classification"
9,"These findings raise the question, do self-attention layers process images in a similar manner to convolutional layers? From a theoretical perspective, one could argue that transfomers have the capacity to simulate any function . Indeed, Perez et al. showed that a multilayer attention-based architecture with additive position","convolutional, layer, transfomers, cnn"
18,our results seem to suggest that localized convolution is the right inductive bias for the first few layers of an image classifying network . We provide an interactive website2 to explore how self-attention exploits localized position-based attention .,"encoding, self-attention, attention, quadratic, content-based"
25,"the self-attention model described above is equivariant to reordering . To alleviate the limitation, a positional encoding is learned for each token in the sequence .","input, model, self-attention, tokens, t"
51,Theorem is proven constructively by selecting the parameters of the multi-head self-attention layer SO that the latter acts like a convolutional layer . The exact condition can be found in the statement of Lemma 1 .,"so, convolutional, multi-head, self-attention, layer"
54,"the above encoding is not the only one for which the conditions of Lemma 1 are satisfied . In our experiments, only Dp = 3 dimensions suffice to encode relative position of pixels, while also reaching similar or better empirical performance .","1, encoding, above, quadratic, lemma"
56,"a multi-head self-attention layer uses by default the "" SAME "" padding . The correct way to alleviate these boundary effects is to pad the input image with zeros on each side . In this case, the cropped output of a MHSA and a convolutional layer are the same .","mhsa, multi-head, self-attention, layer, stride"
57,Theorem 1 can be straightforwardly extended to show that multi-head self-attention with Nh heads can also simulate a 1D convolutional layer with a kernel of size K = Nh with min output channels using a positional encoding of dimension Dp  2.,"convolutional, nh, heads, multi-head, with, self-attention, 1d, layer"
70,"Remark about Dh and Dout It is frequent in transformer-based architectures to set Dh = Dout/Nh . In that which does not suffice to express every convolutional layer with Dout channels . To cover both cases, we assert that the output channels should be min .","dh, dout, o, output, channels"
82,"The exact representation of one pixel requires a to be arbitrary large, despite the fact that the attention probabilities of all other pixels converge exponentially to 0 as a grows .","probabilities, magnitude, a, attention, of"
84,"the aim of this section is to validate the applicability of our theoretical results-which state that self-attention can perform convolution . In particular, we study the relationship between Self-attention and convolution with quadratic and learned relative positional encodings .","self-attention, convolution, lemma, 1"
86,"In all experiments, we use a 2 x 2 invertible down-sampling on the input to reduce the size of the image . The fixed size representation of the input image is computed as the average pooling of the last layer representations .","full, multi-head, self-attention, layer, attention, cifar-10"
92,"CIFAR-10 is faster to converge, but we cannot ascertain whether this corresponds to an inherent property of the architecture or an artifact of the adopted optimization procedures . We observed that learned embeddings with content-based attention were harder to train probably due to their increased number of parameters .","probabilities, accuracy, self-attention, attention, models"
96,Figure 4 displays all attention head at each layer of the model at the end of the training . We also include in the Appendix a plot of the attention positions for a higher number of heads .,"range, heads, attention, long, dependencies, head"
104,the relative positional encoding of a key pixel at position k is the concatenation of the row shift embedding 81 . We chose Dp = Dout = 400 in the experiment .,"position, encoding, 2d, relative, downsampling, row, shift, embedding"
105,Attention probabilities of each head at each layer are displayed on Figure 5 . Figure 5 confirms our hypothesis for the first two layers .,"heads, probabilities, attention"
107,We average the attention probabilities over a batch of 100 test images to outline the focus of each head at each layer . Our hypothesis is confirmed for some heads of layer 2 and 3 . Other heads use more content-based attention leveraging the advantage of Self-Attention over CNN .,"probabilities, heads, attention, head, cnn"
108,The similarity between convolution and multi-head self-attention is striking when the query pixel is slid over the image . Attention patterns in layers 2 and 3 are not only localized but stand at a constant shift . This phenomenon is made evident on our interactive website7 This tool is designed to explore different components of,"self-attention, attention, self, multi-head"
120,The use of CNN networks for text-at word level or character level -is more seldom than transformers . It was observed that transformers have a competitive advantage over convolutional model applied to text .,"text-at, convolutional, model, word, level, cnn"
121,the study of expressiveness of these architectures has focused on their ability to capture long-term dependencies . Turingcomplete is an important theoretical result but is not informative for practitioners .,"computational-cost, transformers, comparisons, cnn"
122,"The closest work in bridging the gap between attention and convolution is due to Andreoli . In this framework, the receptive field of a classical K x K convolutional  kernel would be encoded by A,q,k = 1k - q =  ","index, kernel, convolution, attention"
124,"We showed that self-attention layers applied to images can express any convolutional layer and that fully-attentional models learn to combine local behavior and global attention based on input content . More generally, fully- attentional models seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as the","fully-attentional, global, attention, models, cnn"
130,"TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org .","machine, systems, learning, heterogeneous, large-scale"
191,"Vconv = Wq,k Vq, k. We vectorize the weight matrices into dimension Din Dout x HW . We decompose VSA = WSA Aq with :,h = vect and h,i = a q,w","eq, q"
199,We trained our model using this generalized quadratic relative position encoding . Each head was parametrized by  E R2 and -1/2 R2x2 to ensure that the covariance matrix remained positive semi-definite .,"position, encoding, relative, attention, quadratic"
202,"Specifically in our model with 6-layer and 9-heads each, we pruned heads from the first to the last layer . This means that these layers cannot express a 3 x 3 kernel anymore .","patterns, degenerated, heads, unused, attention"
