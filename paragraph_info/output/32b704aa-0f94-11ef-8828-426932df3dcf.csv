element_idx,keywords,summarized_text
6,"learning, difficult, long, dnn, sentences, sequences",Deep Neural Networks are powerful models that have achieved excellent performance on difficult learning tasks . We present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure . Our method uses a multilayered Long Short-Term Memory to map the input sequence to a vector of a fixed 
8,"object, dnns, recognition, network, visual, backpropagation",Deep Neural Networks are extremely powerful machine learning models . They can sort N N-bit numbers using only 2 hidden layers of quadratic size . Large DNNs can be trained with supervised backpropagation .
9,"power, dimensionality, dnn, fixed, question, answering","DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality . It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori . For example, speech recognition and machine"
13,"time, memory, dimensionality, long-term, lag, timestep, lstm","Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and fixed . The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixeddimensional vector representation, and then to use another "
14,"learning, sequence, connectionist, input, sentence",The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks . It assumes a monotonic alignment between the inputs and the outputs .
17,"small-vocabulary, smt, bleu, score, neural, large, network",BLEU score was achieved by an LSTM with a vocabulary of 80k words . This result shows that a relatively unoptimized small-vocabulary neural network architecture which has much room for improvement outperforms a phrase-based SMT system .
19,"source, long, sentences, sentence, lstm","the LSTM did not suffer on very long sentences, despite the recent experience of other researchers with related architectures . We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set . By doing so, we introduced many short"
26,"dependencies, range, long, temporal, term, lstm","The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map it to the target sequence with another RNN . The Long Short-Term Memory is known to learn problems with long range temporal dependencies, SO an LSTM may succeed in this"
27,"sequence, y, lstm-lm, conditional, input, formulation, probability","The goal of the LSTM is to estimate the conditional probability p where is an input sequence and y1,  , YT' is its corresponding output sequence whose length T' may differ from T . The SLTM computes this conditional probability by first obtaining the fixeddimensional"
28,"softmax, distribution, lstm","In this equation, each p distribution is represented with a softmax over all the words in the vocabulary . We use the LSTM formulation from Graves . The overall scheme is outlined in figure 1 ."
29,"parameters, actual, sequence, number, model, input, lstm",We used two different LSTMs: one for the input sequence and another for the output sequence . we found that deep SLTMs significantly outperformed shallow LSÂ®s . We found it extremely valuable to reverse order of the words of the input sentence .
34,"english, to, french, wmt', dataset, 14",We trained our models on a subset of 12M sentences . We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT .
39,"search, log, translation, beam, probability",We search for the most likely translation using a left-to-right beam search decoder . At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary . This greatly increases the number of the hypotheses SO discard all but the B most likely hypothese .
43,"target, backpropagation, source, sentence","Normally, when we concatenate a source sentence with a target sentence, each word is far from its corresponding word in the target sentence . By reversing the words in the source sentence, the average distance between corresponding words is unchanged . Thus, backpropagation has an easier time ""establishing"
48,"lstm, deep, training","We used deep LSTMs with 4 layers, with 1000 cells at each layer and 1000 dimensional word embeddings . So the deep SLTM uses 8000 real numbers to represent a sentence . Each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden state ."
49,"gradient, descent, stochastic, distribution, lstm","We used batches of 128 sequences for the gradient and divided it the size of the batch . For each training batch, we compute 5g. s = ||9||2, where g is the gradient divided by 128 . If s > 5, we set g = s  Different sentences have"
51,"c++, softmax, deep, lstm","A C++ implementation of deep LSTM processes a speed of 1,700 words per second . This was too slow for our purposes, SO we parallelized our models using an 8-GPU machine . The remaining 4 GPUs were used to parallelize the softmax, SO each GPU was responsible for multiplying by a"
53,"of, bleu, score, quality, ground, truth, translations","We computed our BLEU scores using mul ti-bleu  pl 1 the tokenized predictions and ground truth . This way on of evaluating the BELU score, we get 37.0, which is greater than the 35.8 reported by statmt ."
78,"language, nnlm, alignment, source","researchers have begun to look into ways of incorporating information about the source language into the NNLM . Examples of this work include Auli et al. who combine an NNML with a topic model of the input sentence, which improves rescoring performance."
79,"lstm-like, rnn, architecture, translations, smoot",Cho et al. used an LSTM-like RNN architecture to map sentences into vectors and back . Bahdanau and Pouget-Abadie attempted direct translations with a neural network to overcome the poor performance on long sentences .
82,"deep, mt, large-scale, large, vocabulary, lstm","a large deep LSTM, that has a limited vocabulary, can outperform a standard SMT-based system whose vocabulary is unlimited on large-scale MT task . The success of our simple SLTM-based approach on MT suggests that it should do well on many other sequence learning problems, provided"
83,"short, problem, non-reversed, translation, rnn, term, dependencies","We were surprised by the extent of the improvement obtained by reversing the words in the source sentences . We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler ."
90,"long-term, backpropagation, dependencies, translation","Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model . IEEE Transactions on Neural Networks, 5:157-166, 1994 . K. Cho, B. Merrienboer, C. Gulcehre,"
