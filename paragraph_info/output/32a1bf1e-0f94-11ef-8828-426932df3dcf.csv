element_idx,keywords,summarized_text
6,"language, natural, understanding, glue",GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks . We include tasks with limited training data and a diagnostic test suite that enables detailed linguistic analysis .
8,"language, understanding, nlu","The human ability to understand language is general, flexible, and robust . NLU models above the word level are designed for a specific task and struggle with out-of-domain data ."
9,"answering, glue, question","GLUE does not place any constraints on model architecture beyond the ability to process single-sentence and sentence-pair inputs and to make corresponding predictions . For some tasks, training data is plentiful, but for others it is limited or fails to match the genre of the test set . None of the datasets in "
17,"multi-task, baseline, glue, model","unified multi-task trained models slightly outperform comparable models trained on each task separately . Our best multitask model makes use of ELMo , a recently proposed pre-training technique ."
18,"evaluation, sentence, representation, diagnostic","an online evaluation platform and leaderboard, based primarily on privately-held test data . The platform is model-agnostic, and can evaluate any method capable of producing results on all nine tasks ."
20,"nlp, multi-task, sharing, learning","Collobert et al. used a multi-task model with a shared sentence understanding component to jointly learn POS tagging, chunking, named entity recognition, and semantic role labeling . More recent work has explored using labels from core NLP tasks to supervise training of lower levels of deep neural networks and automatically"
21,"multi-task, glue, senteval, learning","Beyond multitask learning, much work in developing general NLU systems has focused on sentence-to-vector encoders . In this line of work, a standard evaluation practice has emerged, recently codified as SentEval ."
26,"answering, glue, question",decaNLP scores NLP systems based on their performance on multiple datasets . That benchmark lacks the leaderboard and error analysis toolkit of GLUE . It rewards methods that yield good performance on a circumscribed set of tasks .
28,"system, tasks, understanding, generalizable, glue, nlu","GLUE is centered on nine English sentence understanding tasks, which cover a broad range of domains, data quantities, and difficulties . We design the benchmark so that good performance should require a model to share substantial knowledge across all tasks . Although it is possible to train a single model for each task with no pretraining"
30,"linguistic, acceptability, coefficient, theory, correlation",CoLA The Corpus of Linguistic Acceptability consists of English acceptability judgments drawn from books and journal articles on linguistic theory . Each example is annotated with whether it is a grammatical English sentence . We use Matthews correlation coefficient as the evaluation metric .
34,"quora, pairs, question-answering, question, website","QQP The Quora Question Pairs2 dataset is a collection of question pairs . The task is to determine whether a pair of questions are semantically equivalent . We use the standard test set, for which we obtained private labels ."
43,"mnli, inference, corpus, naturalne, multi-genre, entence, te, language, annotations, entailment","MNLI The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations . Given a ten different sources, we use the standard test set, for which we obtained private labels from the authors, and evaluate on both the matched and mismatched"
44,"question-answering, dataset, question-paragraph, pair","QNLI The Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs . The task is to determine whether the context sentence contains the answer to the question . This modified version of the original task removes the requirement that the model select the exact answer, but also removes"
45,"rte5, rte","RTE The Recognizing Textual Entailment datasets come from a series of annual textual entailment challenges . We combine the data from RTE1 , RTE2 , and RTE5 ."
46,"sentence, pair, comprehension",WNLI The Winograd Schema Challenge is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent . Each one is contingent on contextual information provided by a single word or phrase in the sentence . The task is to predict if the sentence
53,"qnli, development, pronoun, training, substituted, set","We use a small evaluation set consisting of new examples derived from fiction books5 that was shared privately by the authors of the original corpus . While the included training set is balanced between two classes, the test set is imbalanced between them ."
55,"test, diagnostic, glue, benchmark, dataset",GLUE benchmark follows the same evaluation model as SemEval and Kaggle . The benchmark site shows per-task scores and a macro-average of those scores to determine a system's position on the leaderboard .
57,"fracas, phenomena, diagnostic, suite, dataset","Drawing inspiration from the FraCaS suite and the recent Build-It competition, we include a small, manually-curated test set for the analysis of system performance . The main benchmark mostly reflects an application-driven distribution of examples ."
58,"linguistic, phenomena, testing, nli",The NLI task is well-suited to this kind of analysis . We ensure the data is reasonably diverse by producing examples for a variety of linguistic phenomena and basing our examples on naturally occurring sentences .
62,"relationships, of, fracas, pairs, suite, sentence, quality, entailment","Annotation Process We construct each example by locating a sentence that can be easily made to demonstrate a target phenomenon . We make minimal modifications SO as to maintain high lexical and structural overlap within each sentence pair and limit superficial cues . Where possible, we produce several pairs with different labels for a single source"
64,"mnli, crowdsourced, data, label, enta, entailment","we audit the data for such artifacts . We reproduce the methodology of Gururangan et al., training two fastText classifiers to predict entailment labels on SNLI and MNLI using only the hypothesis as input."
66,"qualitative, comparison, diagnostic, model, set","Intended Use We do not expect performance on the diagnostic set to reflect overall performance or generalization in downstream applications . The set is provided not as a benchmark, but as an analysis tool for error analysis, qualitative model comparison, and development of adversarial examples ."
68,"learning, model, multi-task, baseline, glue",baselines evaluate a multi-task learning model trained on the GLUE tasks . Original code for baselines is available at https :/ / github  com/ nyu-mll .
69,"glue, sentence-to-vector, architecture","Architecture Our simplest baseline architecture is based on sentence-to-vector encoders . For single-sentence tasks, we encode the sentence and pass the resulting vector to a classifier . The classifier is an MLP with 512D hidden layer ."
79,"sentence, bilstm, encoder, training","Training We train our models with BiLSTMs shared across tasks . For each training update, we sample a task to train with a probability proportional to the number of training examples for each task ."
81,"embeddings, representation, glove, sentence, models","Sentence Representation Models Finally, we evaluate the following trained sentence-to-vector encoder models using our benchmark: average bag-of-words using GloVe embeddings . For these models, we only train task-specific classifiers on the representations they produce ."
86,"lablet, ls, 5","The coarse-grained categories are Lexical Semantics , Predicate-Argument Structure , Logic , and Knowledge and Common Sense ."
89,"computation, multi-sentence, multi-task, training, attention, task",Attention has negligible or negative aggregate effect in single task training . We see a consistent improvement in using ELMo embeddings . Using CoVe has mixed effects over using only GloVe.
91,"sentence, respentation, representation, glue","sentence representation models underperform on CoLA compared to the models directly trained on the task . On WNLI, no model exceeds most-frequent-class guessing . For STS-B, models trained on task lag significantly behind best sentence representation model ."
96,"attention, multi-task, training, model",The highest total score of 28 still denotes poor absolute performance . Performance tends to be higher on Predicate-Argument Structure and lower on Logic .
98,"hypo, hypothesis, hyp-trained, hyp, glue, models","GLUE-trained models only use GloVe embeddings . This is ameliorated by ELMo, and to some degree CoVe . Also, attention has mixed effects on overall results ."
99,"model, out-of-domain, attention, dataset, models","We expect that our platform and diagnostic dataset will be useful for similar analyses in the future, SO that model designers can better understand their models' generalization behavior and implicit knowledge ."
101,"learning, transfer, mechanisms, methods, attention, glue","We introduce GLUE, a platform and collection of resources for evaluating and analyzing natural language understanding systems . We find that, in aggregate, models trained jointly on our tasks see better performance than the combined performance of models trained for each task separately . When evaluating these models on our diagnostic dataset, we find that they fail"
103,"adeptmind, google, research, aw","We thank Ellie Pavlick, Tal Linzen, Kyunghyun Cho, and Nikita Nangia for their comments on this work at its early stages . We thank Ernie Davis, Alex Warstadt, and Quora's Nikhil Dandekar and Kornel Csern"
168,"nli, format, textual, entailment",Our similarity metric is based on CBoW representations with pre-trained GloVe embeddings . This approach to converting pre-existing datasets into NLI format is closely related to recent work by White et al. and to the original motivation for textual entailment
173,"attention, mechanism, states, hidden","We first compute matrix H where Hij = Ui  vj . We pass a second BiLSTM with max pooling over the sequence . Finally, we feed into a classifier ."
175,"sentence, bilstm, encoder, training","We train our models with the BiLSTMs shared across tasks . For each training update, we sample a task to train with a probability proportional to the number of training examples for each task . We use macro-average score over all tasks , as our validation metric ."
178,"tbc, average, bilstm, sentence, middle","Skip-Thought , a sequence-to-sequence model trained to generate the previous and next sentences given the middle sentence . We use the original pre-trained model7 trained on sequences of sentences from the Toronto Book Corpus ."
192,"language, natural, understanding","The dataset is designed to allow for analyzing many levels of natural language understanding, from word meaning and sentence structure to high-level reasoning and application of world knowledge . To make this kind of analysis feasible, we first identify four broad categories: Lexical Semantics, Predicate-Argument Structure, Logic, and Knowledge"
193,"qualitative, comparison, dataset, model","the dataset is provided not as a benchmark, but as an analysis tool to paint in broad strokes the kinds of phenomena a model may or may not capture . We recommend comparing performance that different models score on the same category ."
199,"in, lexical, language, monotonicity, entailment","Lexical Entailment can be applied not only on the sentence level, but the word level . For example, ""dog"" lexically entails ""animal"" because anything that is a dog is also an animal . This relationship applies to many types of words, e.g., in systems of"
203,"in, it, mentioned, entity","Constructions like ""I recognize that X"" are often called factive, and are sensitive to negation . There are also cases where a sentence entails the existence of an entity mentioned in it . ""I have found a unicorn"" doesn't necessarily mean ""A unicorn exists"""
205,"symmetry, symmetric, john, relations, likes, gary, met","Symmetry/Collectivity Some propositions denote symmetric relations, others do not . For example, ""John married Gary"" entails ""Gary married John"" but ""John likes Gary"" doesn't . We classify it under Lexical Semantics ."
209,"lexical, names, items","Named Entities Words often name entities that exist in the world . There are many different kinds of understanding we might wish to understand about these names, including their compositional structure or their real-world referents ."
210,"quantification, lexical, semantics","Quantifiers Logical quantification in natural language is often expressed through lexical triggers such as ""every"", ""most"", ""some"" and ""no"" Quantification and Monotonicity are a question of semantics ."
215,"symmetry, broke, arguments, vase, core","Core Arguments Verbs select for specific arguments, particularly subjects and objects . ""Jake broke the vase"" entails ""the vase broke"" . Other rearrangements of core arguments, such as those seen in Symmetry/Collectivity, also fall under the Core arguments label ."
217,"elephant, elephant's, foot","""I saw him"" is equivalent to ""He was seen by me"" ""the elephant's foot"" is the same thing as ""the foot of the elephant"" ""I caused him to submit his resignation"""
218,"system, russia, ruling","Often, the argument of a verb or other predicate is omitted in the text . We can construct entailment examples by explicitly filling in the gap . premise ""Putin is SO entrenched within Russias ruling system that many of its members can imagine no other leader than Putin"
222,"anaphora, ana",Anaphora/Coreference Coreference refers to when multiple expressions refer to the same entity or event . The meaning of an expression depends on another expression in context . In this category we only include cases where there is an explicit phrase that is co-referent with an antecedent .
226,"argument, factivity, intersectivity, structure","Intersectivity is related to Factivity . For example, ""fake"" may be regarded as a counter-implicative modifier ."
231,"fregean, logic, aristotelian, symbols, syllogisms, mathematical",the development of mathematical logic was initially guided by questions about natural language meaning . The notion of entailment is also borrowed from mathematical logic .
244,"of, statement, the, whole, entailment","""a"" is upward monotone in its restrictor . ""no"" is downward monotone . entailments in the restrictor do not yield a whole statement in the opposite direction ."
251,"interpretation, knowledge, world, sense, common","Strictly speaking, world knowledge and common sense are required on every level of language understanding for disambiguating word senses, syntactic structures, anaphora, and more . In these categories, we gather examples where the entailment rests on correct disambiguation of sentences ."
255,"humming, departure, tillerson's, birds, humming-birds","""The announcement of Tillerson's departure sent shock waves across the globe"" entails ""Marc Sims has been seeing his barber once a week, for several years"" ""The feeders are usually coloured SO as to attract hummingbirds"""
