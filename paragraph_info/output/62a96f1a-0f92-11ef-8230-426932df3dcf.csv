element_idx,summarized_text,keywords
4,Abstract-Humans can naturally and effectively find salient regions in complex scenes . Attention mechanisms were introduced into computer vision to imitate this aspect of the human visual system . Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image .,"system, adjustment, visual, mechanisms, weight, dynamic, attention"
8,"the human visual system uses one , , to assist in analyzing and understanding complex scenes efficiently and effectively . This in turn has inspired researchers to introduce attention mechanisms into computer vision systems to improve their performance . Attention mechanisms have provided benefits in very many visual tasks, e.g. image classification , object detection , semantic","task, visual, image, mechanisms, attention, processing, generation, classification"
9,"In the past decade, the attention mechanism has played an increasingly important role in computer vision . Progress can be coarsely divided into four phases . The first phase begins from RAM . It recurrently predicts the important region and updates the whole network in an end-to-end manner .","neural, mechanism, vision, learning, network, deep, attention, era, networks"
10,"J.J.Liu, P.T.Jiang and M.M. Cheng are with TKLNDST, College of Computer Science, Nankai University, Tianjin 300350, China . R.R.Martin was with Cardiff University, UK .","tsinghua, tklndst, tklnd, university"
13,"In this phase, recurrent neural networks were necessary tools for an attention mechanism . At the start of the second phase, Jaderberg et al. proposed the STN which introduces a subnetwork to predict an affine transformation used to select important regions in the input .","dc, dcns"
19,"Self-attention was first proposed in and rapidly provided great advances in the field of natural language processing . It was followed by a series of works such as EMANet , CCNet, HamNet and the Stand-Alone Network . Recently, various pure deep self-attention networks have appeared .","natural, language, self-attention, models, era, processing, attention-based"
24,"We divide existing attention methods into six categories which include four basic categories: channel attention, spatial attention , temporal attention and branch channel . These ideas are further briefly summarized together with related works in Tab. 2.","spatial, methods, temporal, attention, data, domain"
26,"a systematic review of visual attention methods, COVering the unified description of attention mechanisms . a categorisation grouping attention methods according to their data domain . suggestions for future research in visual attention .","methods, attention, visual"
29,"Chaudhari et al. provide a survey of attention models in deep neural networks which concentrates on their application to natural language processing, while our work focuses on computer vision . Three more specific surveys , , summarize the development of visual transformers while our paper reviews attention mechanisms in vision more generally .","visual, natural, language, methods, transformers, attention, processing"
32,Non-Local Network 2018-2020 2020-Now RAM Highway Network 2017.11.21 attention) attention method) computer vision) OCRNet and HamNet .,"highway, self-attention, network, ram, non-local"
34,"Phase 1 adopted RNNs to construct attention . Phase 2 explicitly predicted important regions, a representative method being STN . phase 4 implicitly completed the attention process .","representative, method, vision, self-attention, attention, computer"
39,"In this section, we first sum up a general form for the attention mechanism . Then we review various categories of attention models given in Fig. 1 . In each, we tabularize representative works for that category .","process, attention, recognition"
53,"Channel attention adaptively recalibrates the weight of each channel . In deep neural networks, different channels in different feature maps usually represent different objects .","channel, attention, maps, feature"
60,Global spatial information is collected in the squeeze module by global average pooling . The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers .,"vector, attention, excitation, squeeze, module"
61,"In the squeeze module, global average pooling is too simple to capture complex global information . In the excitation module, fully-connected layers increase the complexity of the model .","s, srm, excition"
65,a GSoP block also has a squeeze module and an excitation module . Each in the normalized covariance matrix explicitly relates channel i to channel j .,"excitation, squeeze, gsop, module, block"
71,Lee et al. proposed the lightweight style-based recalibration module . It uses mean and standard deviation of input features to improve its capability to capture global information .,"fully-connected, mechanism, channel-wise, lightweight, attention, layer, style, transfer"
72,"X E RCxHxW SRM first collects global information by using style pooling . Then a channel-wise fully connected layer, batch normalization BN and sigmoid function 0 are used to provide the attention vector . Overall, an SRM can be written as:","input, features, vector, srm, attention, pooling, style"
75,"Yang et al. proposes the gated channel transformation to model channel relationships . To overcome the above problems, using fully connected layers is an implicit procedure .","connected, fully, gct, layer, excitation, module"
76,"GCT first collects global information by computing the l2-norm of each channel . Next, a learnable vector a is applied to scale the feature . Then a competition mechanism is adopted by channel normalization to interact between channels .","bias, global, scale, learnable, in-formation, gct, 2, parameter"
83,"To avoid high model complexity, SENet reduces the number of channels . To overcome this drawback, Wang et al. proposed the efficient channel attention block .","correspondence, model, dimensionality, weight, vector, reduction, senet, complexity"
84,"An ECA block has similar formulation to an SE block . Instead of indirect correspondence, an ECA blocks only considers direct interaction between each channel and its k-nearest neighbors to control model complexity .","interaction, eca, cross-channel, efficient, excitation, module, block"
94,"input feature map X E RCxHxW CEM first . It sums the difference between the local descriptors in the input and the corresponding cluster centers using soft-assignment weights . Then, it applies aggregation to the K cluster centers instead of concatenation","map, input, feature, cem"
95,Qin et al. rethought global information captured from the viewpoint of compression . They analysed global average pooling in the frequency domain .,"multispectral, channel, global, average, attention, pooling"
111,bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel . The model pays more attention to higher-order statistical information compared with other attention-based models .,"interactions, bilinear, feature, statistical, pooling, pairwise, information, local"
113,"Spatial attention can be seen as an adaptive spatial region selection mechanism: where to pay attention . RAM , STN , GENet and Non-Local are representative of different kinds of spatial attention methods . STN represents those that use a sub-network implicitly to predict a soft mask to","spatial, selection, adaptive, attention, region"
118,Mnih et al. proposed the recurrent attention model that adopts RNNs and reinforcement learning to make the network learn where to pay attention . RAM pioneered the use of RNN for visual attention and was followed by many other RNN-based methods .,"rnn, reinforcement, ram, learning"
119,The glimpse sensor takes a coordinate lt-1 and an image Xt . The RNN model considers 9t and an internal state ht-1 .,"softmax, glimpse, sensor, ram"
122,Ba et al. proposed a deep recurrent network capable of processing a multi-resolution crop of the input image . The network updates its hidden state using a glimpse as input . It then predicts a new object as well as the next glimpse location .,"network, visual, recognition"
123,"context network, glimpse network, recurrent network, emission network, and classification network . first, the context network takes the down-sampled whole image as input . then, at the current time step t, the goal of the glimpse network is to extract useful information, expressed as the first glimpse .","model, visual, recurrent, network, attention, classification, emission"
127,"compared to a CNN operating on the entire image, the computational cost of the proposed model is much lower . Robustness is additionally improved by the recurrent attention mechanism, which also alleviates the problem of over-fitting .","rnn, overfitting, over-fitting, cnn"
130,"model aims to produce a caption by generating one word at each time step . This model adopts a long short-term memory network as a decoder . The weight at,i vector ai at the t-th of the feature time step is defined as the time step.","lstm, feature, memory, mechanism, a, attention, long, set, (lstm), short-term"
131,"the positive weight at,i can be interpreted either as the probability that location i is the right place to focus on . The hard attention mechanism assigns a multinoulli distribution parametrized by at,i and views Zt as a random variable .","multinoulli, distribution, perceptron, multilayer, attention, hard"
137,"Ranges means the ranges of attention map. S or H means soft or hard attention. choose region according to the prediction . element-wise product, aggregate information via attention map . focus the network on discriminative regions .","spatial, representative, recognition, attention, map"
141,input feature map X and the gating signal G E RC'xHxW are first linearly mapped dimensional space . the output to an RFxHXW is squeezed in the channel domain to produce a spatial . The overall process can attention weight map S E R1XHx,"gating, signal, weight, attention, map"
146,"Jaderberg et al. proposed spatial transformer networks that learn invariance to translation, scaling, rotation and other general warps . STN was the first attention mechanism to explicitly predict important regions .","image, translation, data, equivariance, cnn"
153,the network can sample relevant input regions using the correspondence . xt and yt are corresponding coordinates in the output feature map and the 0 matrix is the learnable affine matrix.,"coordinates, input, features, output"
163,"Self-attention was proposed and has had great success in the field of natural language processing . Recently, it has also shown the potential to become a dominant tool in computer vision .","spatial, vision, self-attention, attention, computer"
174,"GCNet analyses the attention map used in selfattention . The global contexts obtained by selfattention are similar for different query positions in the same image . This is like average pooling, but is a more general process for collecting global information .","self-, attention, map"
178,Yin et al deeply analyzed the self-attention mechanism . The pairwise term focuses on modeling relationships while the unary term is on salient boundaries . This decomposition prevents unwanted interactions between the two terms .,"term, unary, self-attention, pairwise, decoupling"
181,"EANet proposes that self-attention should only consider correlation in a single sample . it makes use of an external attention that adopts learnable, lightweight and shared key and value vectors . It further reveals that using softmax to normalize the attention map is not optimal .","eanet, self-attention, map, correlation"
188,"SASA suggests that using self-attention to collect global information is too computationally intensive and instead adopts local self attention . The authors show that doing so improves speed, number of parameters and quality of results .","spatial, self-attention, convolution, local"
200,"many transformer-based architectures have appeared following ViT . Query2Label , MoCoV3 , BEiT , SegFormer , FuseFormer and MAE have appeared .","architecture, tasks, visual"
201,Dosovitskiy et al proposed the vision transformer which is the first pure transformer architecture for image processing . It is capable of achieving comparable results to modern convolutional neural networks .,"natural, language, vision, processing, transformer"
202,"the main part of ViT is the multi-head attention module . It concatenates a class token with the input feature F E RNxC where N is the number of pixels . Next, Q, K and V are divided into H heads in the channel domain .","mha, multi-head, module, attention"
206,"GENet combines part gathering and excitation operations . In the second step, it generates an attention map of the same size as the input feature map, using interpolation .","genet, input, features"
222,Temporal attention can be seen as a dynamic time selection mechanism determining when to pay attention . Previous works often emphasise how to capture both short-term and long-term cross-frame feature dependencies .,"time, temporal, selection, attention"
225,Li et al. proposed a globallocal temporal representation to exploit multi-scale temporal cues in a video sequence . GLTR consists of a dilated temporal pyramid for local temporal context learning and a temporal self attention module .,"dilated, global-local, temporal, cues, pyramid, multiscale, gltr, representation"
229,GLTR combines the advantages of both modules . It can be incorporated into any stateof-the-art CNN backbone to learn a global descriptor .,"long-term, temporal, gltr, information, short-term"
251,"SK convolution is implemented using three operations: split, fuse and select . During split, transformations with different kernel sizes are applied to the feature map . Information from all branches is then fused together via element-wise summation to compute the gate vector .","gate, split, vector, fuse, select"
258,"Representative branch attention mechanisms sorted by date . Cls = classification, Det=Object Detection. g and f, x) are the attention process described by Eq.","representative, branch, mechanisms, attention, map"
263,"A basic assumption in CNNs is that all convolution kernels are the same . In order to more efficiently increase the representational power of a network, Yang et al. proposed a novel multi-branch operator called CondConv.","convolutional, neural, multi-branch, operator, networks, condconv"
270,"The extremely low computational cost of lightweight CNNs constrains the depth and width of the networks, further decreasing their representational power . To address the above problem, Chen et al. proposed dynamic convolution, a novel operator design that does not change the width or depth of the network .","cnns, lightweight, convolution, dynamic"
280,The residual attention network pioneered the field of channel & spatial attention . It adopted a bottom-up structure consisting of several convolutions to produce a 3D attention map .,"spatial, attention, channel"
289,Each attention module stacked in a residual attention network can be divided into a mask branch and a trunk branch . The trunk branch processes features and can be implemented by any state-of-the-art structure including a pre-activation residual unit and an inception block .,"branch, mask, network, attention, residual, module"
291,"hup is a bottom-up structure, using max-pooling several times after residual units to increase the receptive field . hdown is the top-down part using linear interpolation to keep the output size the same as the input feature map .","bottom-up, structure"
292,a bottom-up top-down feedforward structure models spatial and cross-channel dependencies . Residual attention can be incorporated into any deep network structure in an end-to-end training fashion .,"spatial, top-down, structure, bottom-up, attention, dependencies, map"
297,"Combining channel attention and spatial attention sequentially, CBAM can utilize both spatial and cross-channel relationships of features to tell the network what to focus on and where to focus . To be more specific, it emphasizes useful channels as well as enhance informative local regions .","spatial, channel, attention, architecture, cnn, map"
300,"BAM infers the channel in attention Sc E RC and spatial attention Ss E RHxW two parallel streams . The channel attention branch, like an SE block, applies global average pooling to the feature map to aggregate global information . In order to utilize contextual information, the MLP uses channel dimensionality reduction","spatial, rc, channel, feature, branch, dimensionality, e, reduction, rhxw, attention, map, ss, x"
304,"Ranges means the ranges of attention map. S or H means soft or hard attention. element-wise product. aggregate information via attention map . focus the network on the discriminative region, emphasize important channels, capture long-range information, capture cross-domain interaction .","spatial, representative, channel, attention, map"
319,"triplet attention uses three branches, each of which plays a role in capturing cross-domain interaction between any two domains from H, W and C. In each branch, rotation operations along different axes are applied to the input first, and then a Zpool layer is responsible for aggregating information in the zeroth","input, x, feature, triplet, attention, map"
321,Yang et al. also stress the importance of learning attention weights that vary across channel and spatial domains . The design of SimAM is based on well-known neuroscience theory .,"simam, attention, weights, learning"
330,"BAM and CBAM adopt convolutions to capture local relations . To solve these problems, Hou et al. proposed coordinate attention, a new attention mechanism .",bam
331,"coordinate attention mechanism has two consecutive steps . First, two spatial extents of pooling kernels encode each channel horizontally and vertically . In the second step, a shared 1 x 1 convolutional transformation function is applied to the concatenated outputs of the two pooling layers .","coordinate, attention, embedding, pooling, information, kernels"
336,"Using coordinate attention, the network can accurately obtain the position of a targeted object . This approach has a larger receptive field than BAM and CBAM .","coordinate, mobile, attention, networks"
338,Fu et al. proposed a new framework for natural scene image segmentation . It adopts a selfattention mechanism instead of stacking convolutions to compute the spatial attention map .,"spatial, danet, attention, map"
339,DANet uses in parallel a position attention module and a channel attention module to capture feature dependencies in spatial and channel domains . The position attention modules selectively aggregate features at each position using a weighted sum of features at all positions . Finally the outputs from the two branches are fused to obtain final,"position, channel, feature, attention, dependencies, module"
368,Chen et al. proposed a new spatial and channel-wise attention-based convolutional neural network . It uses an encoder-decoder framework where a CNN encodes an input image into a vector and then an LSTM decodes the vector into an sequence of words .,"spatial, convolutional, neural, network, attention, multi-layer, cnn"
372,"SCA-Net leverages the semantic vector to produce the spatial attention map as well as the channel-wise attention weight vector . Being more than a powerful attention model, the model also provides a better understanding of where and what the model should focus on during sentence generation.","spatial, sca-cnn, attention, sca-net, map"
374,Linsley et al. proposed the global-and-local attention module . It extends an SE block with a spatial attention mechanism .,"model, (gala), attention, global-and-local, module"
375,"GALA uses an attention mask that combines global and local attention to tell the network where and on what to focus . Global attention aggregates global information by global average pooling . In local attention, two consecutive 1 x 1 convolutions are conducted on the input to produce a positional weight map .","global, mask, perceptron, multilayer, multiplication, attention, gala"
381,"Spatial & temporal attention combines the advantages of spatial attention and spatial attention as it adaptively selects both important regions and key frames . Some works compute temporal & spatial attention separately, while others produce joint spatiotemporal attention maps . Further works focusing on capturing pairwise relations .","spatial, temporal, attention"
383,"Song et al. proposed a joint spatial and temporal attention network based on LSTM to adaptively find discriminative features and keyframes . Its main attention-related components are a spatial attention sub-network, to select important regions, and a temporal . attention . sub-","lstm, spatial, attention, sub-network, attention-related, components"
396,the spatiotemporal attention mechanism in RSTAN consists of a spatial attention module and a temporal attention module applied serially . The given feature map X is reshaped to RDxTx and we define X as the feature vector for the k-th location of the n-th,"spatiotemporal, rstan, recognition, action, attention"
404,"Action=action recognition, ReID = re-identification. Ranges means the ranges of attention map. S or H means soft or hard attention. g and f, x) are the attention process described by Eq. 1. element-wise product. aggregate information via attention map . emphasize key points in both","spatial, temporal, recognition, and, re-identification, attention, attentions, map"
411,"STGCN includes two parallel GCN branches, the temporal graph module and the structural graph module . It takes each patch as a graph node and constructs a patch graph for the video .","graph, temporal, module, gcn"
415,STGCN uses the GCN to capture the spatiotemporal relationships of patches across different frames . Pairwise attention is used to obtain the weighted adjacency matrix .,"spatial, temporal, and, attention, stgcn, dimensions, pairwise"
419,"GoogleNet conforms to the above formula, but does not belong to the attention mechanisms . The necessary and sufficient conditions for the attention mechanism are still worth exploring .","sufficient, googlenet, mechanism, necessary, attention, condition"
424,"Channel attention is well-suited to dense prediction tasks such as semantic segmentation and object detection . Based on this observation, we encourage consideration as to whether there could be a general attention block that takes advantage of all kinds of attention mechanisms . For example, a soft selection mechanism could choose between channel attention, spatial attention and temporal","spatial, attention"
426,"Typically, attention-based models are understood by rendering attention maps . This can only give an intuitive feel for what is happening, rather than precise understanding . Applications in which security or safety are important, such as medical diagnostics and automated driving systems, often have stricter requirements .","system, visual, attention, models, attention-based"
428,We visualize some attention map and obtains consistent conclusion with ViT . There phenomenon give us a inspiration that sparse activation can achieve a strong performance in deep neural networks . These motivate us to explore which kind of architecture can simulate human visual system.,"activation, attention, sparse, map"
430,"Attention-based pre-trained models have had great success in natural language processing . Due to their ability to adapt to varying inputs, attention-based models are naturally suited to transferring pretrained weights to a variety of tasks .","natural, language, pre-training, models, processing, attention-based"
432,"SGD and Adam are well-suited for optimizing convolutional neural networks . Recently, Chen et al. significantly improved visual transformers by using a novel optimizer .","convolutional, neural, methods, adamw, optimization, networks"
434,"Convolutional neural networks have a simple, uniform structure which makes them easy to deploy on various hardware devices . However, it is difficult to optimize complex and varied attention-based models on edge devices.","convolutional, neural, models, struc-ture, networks, attention-based"
436,"This survey has systematically reviewed and summarized attention mechanisms for deep neural networks in computer vision . We have grouped different attention methods according to their domain of operation, rather than by application task, and show that attention models can be regarded as an independent topic in their own right .","neural, mechanisms, learning, deep, attention, networks"
440,"A model of saliency-based visual attention for rapid scene analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 11, pp. 1254-1259, 1998 . M. Hayhoe and D. Ballard, ""Eye movements in natural behavior,"" Trends in cognitive","convolutional, cbam, visual, attention, cognition, block"
443,"J. Fu, J. Liu, H. Tian, Y. Bao, Z. Fang, and H. Lu, ""Dual attention network for scene segmentation,"" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . W. Li, X. Zhu, and","image, attention, visual, recognition"
444,"J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ""Bert: Pretraining of deep bidirectional transformers for language understanding, "" 2020. Z. Yang, X. Wang, Y. Wei, L. Huang, H. Shi, W","convolutional, transformers, vision, cvpr, networks"
447,"J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi, ""Gather-excite: Exploiting feature context in convolutional neural networks,"" in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 5589-","visual, image, a2-net, recognition, attention"
448,"H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, ""Psanet: Point-wise spatial attention network for scene parsing"" in Proceedings of the 27th ACM International Conference on Multimedia, 2019, ","attention, visual, recognition"
452,"Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, uSA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 37","spatial-temporal, vision, network, attention, pooling, computer"
454,"Y. Zhang, C. Lan, W. Zeng, and J. Liu, ""Sta: Spatial-temporal attention for large-scale video-based person re-identification,"" IEEE Transactions on Multimedia, 2017, pp. 4263-4270 . S. Wang, C-L","mobile, visual, attentive, network, attention"
457,"V. Likhosherstov, D. Dohan, X. Song, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Belanger, L-Colwell, and A. Weller, ""Rethinking attention with performers,""","convolutional, visual, vision, recognition, network, large-scale, deep, graph"
458,"X. Chen, C.-J. Hsieh, and B. Gong, ""When vision transformers outperform resnets without pretraining or strong data augmentation,"" 2021. N. Qian, ""On the momentum term in gradient descent learning algorithms""","transformers, vision"
