element_idx,keywords,summarized_text
4,"language, current, big-bench, models","BIG-Bench is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models . Language models have already made good progress on this benchmark, with the best model in the paper outperforming average reported human-rater results ."
5,"code-davinci-002, hard, codex, big-bench, code-da",We focus on a suite of 23 challenging BIG-Bench tasks . These are the task for which prior language model evaluations did not outperform the average human-rater . We find that applying chain-of-thought prompting to BBH tasks enables PaLM to surpass average humanrater performance on 10
14,"compute, scaling, palm, size, language, dataset, models","Scaling language models in terms of model parameters, training compute, and dataset size has led to improved capabilities on finetuned tasks . Scaling PaLM to 540B parameters achieves better few-shot performance than the average human-rater score . This raises the questions of whether the tasks on which PaLM perform"
15,"code-davinci-002, hard, openai, model, big-bench, code-",We select a subset of 23 particularly challenging tasks and group them into an evaluation suite referred to as BIG-Bench Hard . We manually write CoT exemplars for BBH and find that combining CoT prompting with the strongest OpenAI model surpasses the average human-rater on 17 of 23 evaluation
20,"imitation, game, hard, benchmark, question-answering, big-bench, language, models","BIG-Bench is a collaborative benchmark that aims to quantitatively measure the capabilities and limitations of language models . The benchmark has over 200 diverse text-based tasks in task categories including traditional NLP, mathematics, commonsense reasoning, and question-answering ."
21,"google, big-bench, models, organizers",BIG-Bench organizers evaluated performance on each task using several language model families . enlisted a team of raters to manually solve each task and score them against golden labels .
22,"tasks, metadata, data, big-bench, clean","We select a set of criteria to filter BIG-Bench to arrive at a clean, challenging, and tractable subset . For example, some tasks that were added later in the curation process lack human baselines . Other tasks only have the minimum number of examples needed to qualify for submission ."
24,"tasks, hard, baselines, human-rater, big-bench, multiple-choice",After filtering out tasks without human-rater baselines 78 Clean multiple-choice or exact match tasks 78 - After filters out tasks in which the best reported model beats average reported reported humanrater score 23 . Remaining tasks = BIG-Bench Hard .
26,"tasks, control, multipl, clean, multiple-choice, exact-match","These filtering criteria result in 78 clean multiple-choice or exact-match tasks . Of these 78 tasks, the best reported model outperforms the average human-rater score on 42 of them . No previous model surpasses the Average Human- Rater score for the remaining 36 ."
27,"deduction, hard, shuffled, price, openai, api, tracking, big-bench, objects, logical",evaluating BBH on text-davinci-002 would cost $195.33 USD at the current OpenAI API price of $0.02 USD per 1K tokens.
34,"space, prompting, input-shot, few-shot, input-output, inference-time, example",The baseline approach we consider is few-shot prompting . We include both a task instruction and answer options in the prompt . This follows the recent protocol used in and is consistent with empirical work suggesting language models benefit strongly from knowing the desired output space .
35,"prompting, few-shot, chain-of-thought","Recent work has suggested that augmenting few-shot exemplars with a chain-of-thought can substantially improve the performance of language models on a range of tasks . arithmetic reasoning, commonsense reasoning, symbolic manipulation, and program execution can be improved ."
36,"code-cushman-002, codex, code-, code-curie-002","Codex , InstructGPT , and PaLM consider three families of language models . For Codex, we focus on codedavinci-002 and code-cushman-001 . 3 For PaLM, we use the three available model sizes ."
44,"answer-onl, instructgpt, prompting, 540b, bbh, palm","Table 2 summarizes the performance of PaLM, InstructGPT, and Codex models on BBH for answer-only and CoT prompting approaches . In the setting reported in BIG-Bench paper, none of the evaluated models outperformed human-rater baselines on any of the tasks meeting the BBH criteria"
45,"prompting, cot, human-rater, performance, codex",Codex with CoT prompting outperforms the average human-rater score on 17 out of 23 tasks . Codex lags behind the best human rater performance by over 20% .
46,"rank, tasks, likelihood, classification, multiple-choice","Rank classification appends each multiple-choice option to the model one at a time, measures likelihood, selects the one with the highest model likelihood, and determines whether it matches the ground-truth label ."
49,"prompting, cot, scale, model, performance",CoT prompting has negative or zero performance gain for text-ada-001 to text-curie-002 but the performance delta between CoT and no CoT increases with model scale up to the largest model size . This result shows that CoT is an emergent prompting strategy that requires sufficiently large models 
54,"prompting, scaling, cot, curves, flat",CoT prompting does not unlock emergent task performance in all BBH tasks . Causal Judgement achieves 57.8% performance with answer-only prompting . We leave this task for future work as a challenge task that perhaps requires novel prompting techniques.
61,"practices, understanding, natural, common, language, codex","CoT appears to facilitate the decomposition of complex, multi-step problems into smaller, solvable problems in sufficiently large models . Codex, trained on both code and text data, shows better performance in following task instructions and exploiting algorithmic patterns based on prompt exemplars compared to InstructGPT and Pa"
64,"code-davinci-002, hard, big-bench, code-, codex","Codex outperforms the average human-rater baseline on 17 of 23 tasks . The models are InstructGPT , Codex , and PaLM 540B ."
65,"knowledge, control, reasoning, multilingual, quality, estimation","Previous studies have shown that large, general-purpose language models are capable of performing translation and multistep reasoning in multilingual setups . We considered one multilingual task, Salient Translation Error Detection, based on translation quality estimation and cross-lingual natural-language inference ."
66,"prompting, cot, snarks, sarcasm","CoT prompting achieves 10-20% accuracy gains over answer-only prompting across the three language models on average . Two of these tasks require use of world knowledge, for example common presuppositions, human perception and usage of humor . Figure 7 shows two instances in Snarks where CoT does not seem"
70,"performances, human-rater, big-bench, performance, task","Srivastava et al. report the average and maximum of human-rater performances on most BIG-Bench tasks . The human-evaluation component of the benchmark took almost a year . In some cases, the task description was hard to follow and could have been better-formulated ."
73,"nlp, gft-3, creative, prompting, downstream, generation","Brown et al. demonstrated strong few-shot prompting abilities of GPT-3 on a range of downstream NLP tasks . Since then, there has been a long line of work proposing new methods for prompting pre-trained language models ."
77,"prompting, natural, bbh, reasoning, language","Recent work has aimed to leverage the natural language abilities of language model for reasoning tasks . These natural language reasoning paths can be used both before or after the answer is given . For example, providing reasoning paths before the answer can improve performance ."
78,"scaling, law, emergence","Empirical studies on scaling laws for neural language models have shown that model performance improves with increasing model capacity and training data . In this work, we show that CoT prompting is a key to unlock emergent task capabilities in BBH at sufficiently large model scales."
80,"hard, model, baseline, human-rater, big-bench, performance, codex",Experiments show that answer-only prompting underestimates model capabilities . CoT prompting enables most capable Codex model to outperform average human-rater baseline on 17 out of 23 tasks in BBH .
94,"understanding, transformers, aclanthology, language, bidirectional","Association for Computational Linguistics: Human Language Technologies, Volume 1 . doi: 10.18653/v 1/N19-1423 ."
136,"collaboration, human-ai, naacl-main, aclanthology","Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl and Yejin Choi . Reframing human-AI collaboration for generating free-text explanations ."
367,a,"Question Chain-of-Thought A: Let's think step by step . Remember that expressions inside brackets are always evaluated first . The order of operations from highest priority to lowest priority is ""not"""
368,"a, b","True and False and not True and True is A: Let's think step by step . Remember that the order of operations from highest priority to lowest priority is ""not"", ""and"", ""or"""
369,a,"We first simplify this expression ""Z"" as follows: ""Z = not not ) = not False = True. So the answer is True."
375,"frank, questions, t., functional, function","Frank T. had no experience with guns, his hand slipped on the barrel of the gun . The bullet bounced off a boulder several feet away and hit the neighbor's body, causing significant injury ."
380,"went, frank, wild, gun, t., shot","""Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild,"" a typical person would assume . The answer is No. Answer ."
381,"billy, motion, suzy, detector","Suzy and Billy are working on a project that is very important for our nation's security . The boss tells them: ""Be sure that you are here at exactly 9 am. It is absolutely essential that you arrive at that time"""
384,"triggering, boss, detector, motion","A typical person would assume that the person probably meant to say the detector was set up to be triggered if ""both persons"" appeared in the room at the same time, not at least one person . Because the motion detector went off, a typical person could come to the conclusion that both Suzy and Billy triggered the motion"
385,"lena, region, george, low, point","George and his sister Lena reunite at their parents' house for Thanksgiving . They split the first two games, and the third is close until the end . Who will win comes down to George's last shot . George sets up his shot and the dart lands in the low point region ."
395,"yyyyy, of, mm/dd/yyyyy, mm/dd/yyyyyyyyyyy, eve, 1937, christmas, yyyyyy","If today is Christmas Eve of 1937, then today's date is December 24, 1937 . 10 days before today is December 14, 1937, that is 12/14/1937 . Q: Tomorrow is 11/12/2019. The date one year ago from today is 11/11/2019 . Infer the date from context."
401,"of, counselor, state, pronoun, the, antecedent, ambiguous","The chief told the counselor that they took the day off Ambiguous A: Let's think step by step . Here we need to determine who the pronoun ""they"" might be referring to . Let X be the chief and Y the counselor. The sentence is then of the form ""X told Y"
405,ansew,"Let's consider Y first: ""X sent a message to Y, but Y didn't reply yet"" This case doesn't make sense, because X was the initial sender of the message . So it is now Y's turn to write back to X ."
409,"x, meet, will, y’s, to, office, at, bailey, y, x's, plan","The verb phrase ""plan to meet"" might be able to help us determine which one is more likely . Let's consider Y first: ""X will plan to meet Y at Y's office"" This case makes sense, because X might want to meet up with Y ."
414,"configuration, stack, input","Q: Complete the rest of the sequence, make sure that the parentheses are closed properly. Input: "", """", ""]."
416,"answork, stack","stack:  2: > ; stack: empty 3: "", "") . So the answer is ]) ."
418,"answork, stack"," : stack:  2: ; stack: "", "">"". So the answer is  [, ]>."
423,"lesley, close, friend, leroy, leroy’, leroy’s",Lesley is a close friend of Fernando or a schoolmate of Lowell . The argument is deductively valid or invalid .
426,"lesley, leroy","Lesley is a close friend of Fernando or a schoolmate of Lowell . If X = friend OR SCHOOLMATE, then Lesley = great-grandfather? Let's see whether the Hypothesis can be deduced from the arguments ."
427,"of, ancestor, stepbrother, dana, brian","Being an ancestor of Dana is sufficient for not being a great-grandfather of Brian . Is the argument, given the explicitly stated premises, valid or invalid?"
430,"of, ancestor, stepbrother, dana, brian","Whoever is not a great-grandfather of Clyde is a stepbrother of Brian: IfX = NOT ). Hypothesis: Does it follow that everyone who is an ancestor of Dana is . So, the answer is valid."
431,"of, soap, premises, valid, consumer, paul, shampoo, mitchell, regular, lush","whoever is an infrequent user of Paul Mitchell shampoo is not a regular consumer of Lush soap . Is the argument, given the explicitly stated premises, valid or invalid?"
434,"of, soap, infrequent, consumer, paul, user, shampoo, mitchell, regular, lush","Every infrequent user of Paul Mitchell shampoo is either a rare consumer of Nioxin shampoo or a loyal buyer of Caress soap at the same time . If X = regular-consumer, then x = NOT AND loyal-buyer . Let's see whether the Hypothesis can be deduce"
454,"line, coordinates, svg, creates, path","This SVG path element contains ""M"" and ""L"" commands . M takes two parameters and moves the current point to the coordinates . L draws a line from the previous coordinate to the new coordinate ."
457,"by, step, triangle, svg","This SVG path element contains ""M"" and ""L"" commands . M takes two parameters and moves the current point to the coordinates . L 58.44,36.69: Create a line from 14.19,26.04 to 51.43,39.21 ."
474,"equal-length, long-, sides, long-term, line, adjacent, activity","L 37.00,43.00: Create a line from 41.00,33.00 to 41.00 . L 45.00,34.00: Make a Line from 415.00,33.0 to 410.00,43.0 . ""kite"" and ""rectangle"" are the only two four-sided polygons on"
482,"adjectives, terrible, rubber, ship","Option : ""rubber terrible ship"". Rubber falls into the material category . Option has the following adjective order: . Because 7  1 is not correct, does not have the correct ordering ."
484,"of, ship, state, small, exercise, brazilian","Repulsive small Brazilian exercise ship A: Let's think step by step . When there is more than one adjective before a noun, the adjectives need to respect the following order before the noun: "" noun"". Option : ""repulsive"" falls into the opinion category. ""small"" falls"
489,"square, noun, adjective, shoe, order","Option : ""blue gold wonderful square shoe"". ""gold"" falls into the material category . ""wonderful"" falls in the opinion category. The adjective order that Option has is . The answer is : ."
501,"green, leftmost, book, white","The green book is to the right of the white book: "" ? white ? green orange "" . The orange book is the rightmost . So the answer is "" white green orange"""
505,"leftmost, book, gray, white, red","The white book is to the left of the gray book: "" ? white ? gray ? "" . The red book is the second from the left: "" white red gray"""
515,"movie, american, princess, big, the, bride, lebowski",Star Wars Episode IV - A New Hope - Indiana Jones and the Last Crusade . The Empire Strikes Back - The Big Lebowski These are all famous American movies produced before 2000 .
531,"a, b, a*b","This equation can be written as ""A * B"", whereA= and B= . Let's calculate A = = - 0) = -5 - 36 = -3 . The final equation is A *B= -41 * -3= * = 123."
533,"b, a+b, b+b","The order of operations in mathematics is as follows: Parentheses, exponents, multiplication and division . This equation can be written as ""A + B"", whereA= and B = . Let's calculate A = * ) = 3969 . The answer is 3929 ."
535,"by, step, mathematics, analysis","Let's remember that the order of operations in mathematics is as follows: Parentheses, exponents, multiplication and division . This equation can be written as ""A - B"", whereA= and B= . The answerisA-B=-163 - 56 =-219."
544,y,"A: Let's think step by step. We start at the origin , facing the positive y-axis . Turn left: , face the negative x-axes. Take 7 steps: ., facing . the positive. y axis. take 8 steps ."
548,y,"A: Let's think step by step. We start at the origin , facing the positive y-axis . Take 1 step: , face the negative y axis. Take 9 steps: ., facing a positive Y-axie. The answer is No."
554,y,"A: Let's think step by step. We start at the origin , facing the positive y-axis . Take 2 steps right: , face the negative y axis. Take 7 steps right ."
599,"brown, stress, keychain, ball","The stress ball is the second object on the list, namely a brown keychain . The color of the keychain is brown ."
610,"keych, burgndy, keychain","The teal plate is the first item, namely . There is no item to the left of the teal item . The number of non-orange items to the right is zero ."
621,"snape, severus, rock, hard, band, harry, whitesnake, series, potter","""whitesnake"" is the name of an old English hard rock band . It is a compound word, formed by the words ""white"" and ""sneke"" Snape makes a reference to the fictional character Severus Snape in the Harry Potter series, so is indeed a meaningful and funny"
624,"dinosaur, missing","The original name is ""one of our dinosaurs is missing"" This is the name of an old British movie . Here the word ""of"" is changed to ""ofne"" but the term ""af"" is not an actual word . ""one af our dinosaur is missing,"" ""one"" is indeed a"
628,"crows, countingy, counting, coutnting","The original name is ""counting crows"" This is the name of an American rock band . The name comes from the British nursery rhyme ""One for Sorrow"""
633,"monuments, brandenburg, city, architectural",The following translations from German to English contain a particular error . Named Entities: An entity is changed to a different entity . Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms . Dropped Content: A significant clause in the translation is
638,"of, city, brandenburg, source, lenzen, architectural, monuments, sentence","The correct translation of the source sentence from German to English is ""The list of monuments in Lenzen includes all the monuments . On the other hand, the provided translation is ""In the list of architectural monuments"" The translation contains an error pertaining to Named Entities ."
639,"german, translations",The following translations from German to English contain a particular error . Named Entities: An entity is changed to a different entity . Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms . Dropped Content: A significant clause in the translation is
644,"lech, source, architectural, am, monuments, landsberg, sentence","Google Translate says the correct translation of the source sentence from German to English is ""The monuments of the Upper Bavarian district town of Landsberg am Lech are compiled on this page' "" The translation should have said ""Upper Bavaria district town"" Because a significant clause in the translation was removed, the"
645,"translation, antonyms, values, entity",The following translations from German to English contain a particular error . The error will be one of the following types: Named Entities: An entity is changed to a different entity . Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms . Drop
649,"sentence, google, source, translate","Google Translate says the correct translation of the source sentence from German to English is ""Leba is a small town and seaside resort in the Powiat Leborski county of the Polish Pomeranian Voivodeship of Poland' "" The provided translation is based on a negation or Antonyms"
658,"deal, sarcasm, actively, them, researching, huge","A: Let's think step by step. If we look at , it says that having interests and actively researching them is a huge waste, implying that it is an important and consequential effort . The presence of such a juxtaposition in suggests that it contains a taste of irony and sarcas"
661,"hominem, attacks, ad, sarcasm","avoiding ad hominem attacks really help your case, implying that they are adverse and injurious . If we look at , it says that avoiding them is often useful and helpful . The speaker in this sentence seems to mean the opposite of what they are saying ."
664,"sarcastic, conscience, option, morality, punishments, league’s","Consistency in the league's punishments? What do you think this is supposed to be, politics? A: Let's think step by step . If we look at , it likens the consistency in . the league . punishments with that in morality . This sentence appears to be sati"
675,"park, ethics, amusement",Elizabeth saw Emily reading at the library from 2pm to 4pm . Leslie saw Emily waiting at the airport from 6pm to 7pm - the museum was closed after 7pm. Between what times could Emily have gone to the museum? We know that: Elizabeth woke up at 7am . David saw Elizabeth fixing their computer
690,"alice, bob, lola, dancing, is, with","Alice, Bob, and Claire are dancers at a square dance . At the start of a song, they each have a partner: Alice is dancing with Lola . Bob and Bob switch partners ."
699,"henry, john",Jerry says Fidel tells the truth. Raymond says Millicent says Vina lies. Does Raymond tell the truth? A: Let's think step by step . Raymond tells The Truth. Raymond: Does Raymond Tell the Truth?
701,"christian, lies","Maybelle says Millie tells the truth. Fidel says Maybellen lies . Now, the question asks: Does Leda tell the truth? The answer is yes ."
703,"christian, lies","So, we know that Kristian tells the truth. Michaela says Kristian lies . Since we know from that Michaela lies, if Raymond says Michaela telling the truth, then Raymond lies. Osvaldo says Raymond tells The truth . So, the answer is No."
708,"costume, counterpart, oven","The first letter: ""oven"": """" . ""counterpart"". Now let's sort this subpart by looking at their third letters . We now have: ""costume""  ""conpart""."
709,"credulity, phone, ponderosa","Hypochlorite ponderosa phone credulity A: Let's think step by step . The first letter: ""hypochlorite"": ""h"" . ""pondenosa"": p ."
710,"arson, parthia, seismography","Sort the following words alphabetically: newt arson parthia seismography mugho aspect census A: Let's think step by step . The first letter: ""newt"": ""n"" . ""parthia""  ""seismography"" Now let's sort this"
714,"color, interpretation","color, hhh alignment, key value maps, unit interpretation, elementary math qa, intersect geometry, symbol interpretation, conceptual combinations, language games, modified arithmetic, chess state tracking, unnatural in context learning ."
716,"physical, intuition, questions, question, cycled, letters","English russian proverbs, analytic entailment, general knowledge, mathematical induction, evaluating information essentiality, persian idioms, code line description, sentence ambiguity, figure of speech detection, physics questions . logical args, novel concepts, repeat copy logic, "
718,"gender, questions, sensitivity, squad, shifts","unqover, squad shifts, convinceme, subject verb agreement, context definition alignment, coqa conversational question answering, sudoku, twenty questions, roots optimization and games, word problems on sets and graphs, self evaluation tutoring, text navigation game, gender sensitivity chinese, factuality of summary,"
720,"context, in, dynamic, social, sparc, parsing, support, counting, dynamics","parsinlu qa, which wiki edit, auto categorization, gender inclusive sentences german, paragraph segmentation, linguistics puzzles, abstraction and reasoning corpus, international phonetic alphabet transliterate, topical chat, polish sequence labeling ."
724,"resolution, barqa, anaphora, entailed, qa, enta, disfl, polarity","metaphor boolean, logic grid puzzle, entailed polarity hindi, strange stories, fantasy reasoning, winowhy, emojis emotion prediction, question selection, vitaminc fact verification, kannada, anachronisms, strategyqa, fact checker ."
