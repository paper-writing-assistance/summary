element_idx,summarized_text,keywords
3,"masked autoencoders are scalable self-supervised learners for computer vision . Our MAE approach is simple: we mask random patches of the input image . We develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches .","vision, mae, autoencoders, masked, computer"
6,self-supervised pretraining allows training of generalizable NLP models containing over one hundred billion parameters . based on autoregressive language modeling in GPT and masked autoencoding in BERT .,"natural, nlp, processing, language"
11,Vision Transformers should no longer present an obstacle to convolutional networks . Convolutions typically operate on regular grids and it is not straightforward to integrate 'indicators',"convolutional, transformers, vision, architecture, networks"
12,"Languages are human-generated signals that are highly semantic and information-dense . Images, on the contrary, are natural signals with heavy spatial redundancy .","information, density, vision"
19,a simple strategy works well in computer vision: masking a very high portion of random patches . This strategy largely reduces redundancy and creates a challenging selfsupervisory task that requires holistic understanding beyond low-level image statistics .,"computer, image, self-, vision, supervisory, reconstruction, statistics"
20,"autoencoder's decoder plays a different role between reconstructing text and images . In vision, the decodere reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks .","autoencoder, language, representation"
22,Shifting mask tokens to the small decoder results in a large reduction in computation . a very high masking ratio can achieve a win-win scenario . This can reduce overall pre-training time by 3x or more .,"mae, lightweight, reconstruction"
23,"MAE learns very high-capacity models that generalize well . With a vanilla ViT-Huge model, we achieve 87.8% accuracy when finetuned on ImageNet-1K . This outperforms all previous results that use only Imagenet-1K data .","model, learning, imagenet-1k, generalization, transfer"
28,Masked language modeling and its autoregressive counterparts are highly successful methods for pre-training in NLP . They hold out a portion of the input sequence and train models to predict the missing content . These methods have been shown to scale excellently and a large abundance of evidence indicates that these pre-trained representation,"modeling, nlp, language, pre-training, masked"
29,"Autoencoding has an encoder that maps an input to a latent representation and a decoder that reconstructs the input . A series of methods can be thought of as a generalized DAE under different corruptions, e.g., masking pixels or removing color channels .","representative, autoencoding, learning"
30,The pioneering work of presents masking as a noise type in DAE . Context Encoder inpaints large missing regions using convolutional networks .,"context, self-supervised, image, encoding, encoder, learning, dae"
31,"Recently, contrastive learning has been popular, e.g., which models image similarity and dissimilarity between two or more views . Contrastive and related methods strongly depend on data augmentation .","self-supervised, vision, learning, contrastive, computer"
33,"masked autoencoder is a simple autoencoding approach that reconstructs the original signal given its partial observation . We adopt an asymmetric design that allows the encoder to operate only on the partial, observed signal and a lightweight decoder . Figure 1 illustrates the idea, introduced next.","autoencoder, masked"
35,"Random sampling with a high masking ratio largely eliminates redundancy, thus creating a task that cannot be easily solved by extrapolation from visible neighboring patches .","image, largely, elasticity, removed, center, patches"
36,"MAE encoder is a ViT but applied only on visible, unmasked patches . The full set is handled by a lightweight decoder, described next .","mae, visible, encoder, patches"
40,"MAE decoder is only used during pre-training to perform the image reconstruction task . We experiment with very small decodings, narrower and shallower than the encoder .","image, recognition, mae, lightweight, decoder"
41,Each element in the decoder's output is a vector of pixel values representing a patch . Our loss function computes the mean squared error between the reconstructed and original images in the pixel space.,"error, mae, target, mean, reconstruction, squared"
43,"Our MAE pre-training can be implemented efficiently, and importantly, does not require any specialized sparse operations . First we generate a token for every input patch and remove the last portion of the list based on the masking ratio . After encoding, we append a list of mask tokens to","implementation, simple, mask, pre-training, mae, tokens"
67,"MAE ablation experiments with ViT-L/16 on ImageNet-1K . We report fine-tuning and linear probing accuracy . If not specified, the default is: the decoder has depth 8 and width 512 .","imagenet-1k, imagenet"
71,a sufficiently deep decoder can account for the reconstruction specialization . The last several layers in an autoencoder are more specialized for reconstruction . This design can yield up to 8% improvement in linear probing .,"linear, deep, probing, decoder"
79,"if the encoder uses mask tokens, it performs worse: its accuracy drops by 14% in linear probing . This gap may degrade accuracy in deployment .","probing, token, mask, linear, accuracy"
80,"In Table 1c, we reduce the overall training FLOPs by 3.3x . This leads to a 2.8x wall-clock speedup in our implementation . Note that the speedup can be >4x for a masking ratio of 75% .","mae, speedup, large-batch, training"
84,"Reconstruction targets are based on pixels without normalization . This per-patch normalization enhances the contrast locally . In another variant, we perform PCA in the patch space .","target, pca, coefficients, reconstruction"
85,"Specifically for this variant, we use the MAE pre-trained dVAE as the tokenizer . This tokenization improves fine-tuning accuracy by 0.4% vs. unnormalized pixels .","pre-trained, mae, tokenization, dalle, dvae, decoder"
98,Our ablations thus far are based on 800-epoch pre-training . Figure 7 shows the influence of the training schedule length. The accuracy improves steadily with longer training .,"training, learning, mae, schedule, contrastive"
101,"All self-supervised methods are evaluated by end-to-end fine-tuning . All results are on an image size of 224, except for ViT-H .","imagenet-1k, vit-h, imagenet-h"
106,"MAE can scale up easily and has shown steady improvement from bigger models . By fine-tuning with a 448 size, we achieve 87.8% accuracy using only IN1K data .","mae, state-of-the-art, model, bigger"
115,Table 1 shows that linear probing and fine-tuning results are largely uncorrelated . Linear probing has been a popular protocol in the past few years . It misses the opportunity of pursuing strong but non-linear features .,"linear, fine-tuning, probing"
116,Figure 9 shows the results . Fine-tuning only one Transformer block boosts the accuracy from 73.5% to 81.0% . This variant is basically fine-tuneing an MLP head .,"fine-tuning, mlp, head, transformer, block"
128,"On iNat, our method shows strong scaling behavior . Our results surpass the previous best results by large margins . On Places, our MAE outperforms the previous worst results .","inaturalists, transfer, learning"
137,"In NLP, simple self-supervised learning methods enable benefits from exponentially scaling models . In computer vision, practical pre-training paradigms are dominantly supervised . Selfsupervised learning may be embarking on similar trajectory as in NLP .","simple, imagenet, self-supervised, learning, deep"
138,"Images are recorded light without a semantic decomposition into the visual analogue of words . Instead of attempting to remove objects, we remove random patches that most likely do not form semantic segments . Similarly, our MAE reconstructs pixels, which are not semantic entities .","semantics, visual, image, mae, analogue"
142,"An empirical study of training self-supervised Vision Transformers . In CVPR, 2021. Xinlei Chen, Saining Xie, and Kaiming He . A simple framework for contrastive learning of visual representation .","visual, image, transformers, bert, representation"
144,"In AISTATS, 2010. Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Self-supervised pretraining of visual features","visual, backpropagation, representation"
146,"In CVPR, 2017. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. Ilya","image, representation, attention, visual"
147,"In CVPR, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Leon Bottou. Stacked denoising autoencoders: Learning useful representation","ade20k, dataset"
151,We follow the standard ViT architecture . Each block consists of a multi-head self-attention block and an MLP block . The encoder ends with LN .,"vit, layer, architecture, scaling"
155,"Linear probing requires a very different recipe than end-to-end fine-tuning . In particular, regularization is in general harmful for linear probing. We do not use mixup , cutmix , drop path , or color jittering.","probing, following, training, linear, classifier"
156,"It is a common practice to normalize the classifier input when training a classical linear classifier . Following , we adopt an extra BatchNorm layer without affine transformation . This layer is applied on pre-trained features produced by the encoder .","probing, affine, linear, property, classifier, transformation"
174,We adapt the vanilla ViT for the use of an FPN backbone in Mask R-CNN . ViT has a stack of Transformer blocks that all produce feature maps at a single scale .,"fpn, maps, multiscale, backbone"
190,In Table 13 we evaluate the robustness of our models on different variants of ImageNet validation sets . We use the same models fine-tuned on original ImageNet . Increasing the model sizes has significant gains .,"validation, imagenet, sets, table, 13"
