element_idx,keywords,summarized_text
7,"llm, optimization, prompt","Optimization by PROmpting is a simple and effective approach to leverage large language models as optimizers, where the optimization task is described in natural language . In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the"
10,"movie, recommendation, gsm8k, 2-l-it, palm, 2-l-",optimization on GSM8K has pre-trained PaLM 2-L as the scorer . BBH movie_recommendation has text-bison as scorer and PaLM 1-IT as the optimizer. Each dot is the average accuracy across all generated instructions .
17,"optimization, techniques, algorithm","optimization is critical for all areas . Optimization starts from an initial solution, then iteratively updates the solution to optimize the objective function . The optimization algorithm typically needs to be customized for an individual task ."
18,"llm, problem, optimization",Optimization with LLMs enables quick adaptation to different tasks by changing the problem description in the prompt . The optimization process can be customized by adding instructions to specify the desired properties of the solutions .
19,"science, salesman, problem, traveling, computer, regression, llm, linear","We first present case studies on linear regression and the traveling salesman problem . On small-scale optimization problems, we show that LLMs are able to find good-quality solutions simply through prompting ."
20,"optimization, accuracy, llm, task, prompt","LLMs are shown to be sensitive to the prompt format . In particular, semantically similar prompts may have drastically different performance . However, the large and discrete prompt space makes it challenging for optimization ."
21,"llm, optimization, prompt",The meta-prompt contains two core pieces of information . The first piece is previously generated prompts with their corresponding training accuracies . the second piece is the optimization problem description .
22,"gsm8k, gsm8, gpt, model, family","We conduct comprehensive evaluation on several LLMs, including text-bison and Palm 2-L in the PaLM-2 model family . We optimize prompts on GSM8K and Big-Bench Hard which are reasoning benchmarks where prompting techniques have achieved remarkable performance breakthrough ."
27,"zero-shot, llm, prompting, optimization",Optimizers consistently improve the performance of the generated prompts through iterative optimization until convergence . The best generated instructions match the few-shot chain-of-thought prompting performance when applied to PaLM 2-L .
29,"problem, optimization, opro, task","In each optimization step, the LLM generates candidate solutions to the optimization task based on the optimization problem description . Then the new solutions are evaluated and added to the meta-prompt for the subsequent optimization process . We first outline the desired features of LLMs for optimization ."
31,"language, natural, llm, optimization","LLMs allow people to describe their optimization tasks without formal specifications . In prompt optimization, the task can be described with a high-level text summary along with input-output examples ."
32,"trade-off, exploration, exploitation",the exploration-exploitation trade-off is a fundamental challenge in optimization . This means that LLMs should be able to exploit promising areas of the search space where good solutions are already found .
39,"llm, trajectory, optimization, optimiz",Optimization trajectory allows LLMs to recognize patterns from in-context demonstrations . Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores .
42,"llm, instability, optimization, stability","In the optimization process, not all solutions achieve high scores and monotonically improve over prior ones . Due to the sensitivity of in-context learning to the prompt, LLM output can be drastically affected by low-quality solutions in the input optimization trajectory . This sometimes results in optimization instability and large variance ."
45,"optimization, trajectory, regression, llm, linear","We present a case study on linear regression as an example of continuous optimization . On both tasks, we see LLMs properly capture the optimization directions on small-scale problems based on the past optimization trajectory provided in the meta-prompt."
47,"value, llm, optimization, stability","In linear regression problems, the goal is to find the linear coefficients that probabilistically best explain the response from the input variables . We study the setting in which the independent and dependent variables X and y are both one-dimensional and an intercept 6 is present . In a synthetic setting, we sample ground truth values for"
48,"llm, son, text-bi, text-","Table 2 summarizes the results with one of the following optimizer LLMs . We study three settings of Wtrue and btrue: within the starting region x, ""near outside"" , and ""far outside"""
51,"optimizer, llm, llms",Both w and 6 start from 5 random starting points in each setting . Bold numbers indicate the best among three LLMs in each set .
53,"of, optimization, steps, pairs, trajectory, number, unique, black-box",text-bi son and gpt-4 models outperform the optima with fewer steps . The gppt-4 model also outperses in finding the optimala . the problem becomes harder when the ground truth moves farther from the starting region .
55,"salesman, problem, algorithms, tsp, heuristic, numerous, training",Traveling Salesman Problem is a classical combinatorial optimization problem with numerous algorithms proposed in literature . The TSP task is to find the shortest route that traverses all nodes from the starting node .
56,"llm, process, optimization","Our optimization process with LLMs starts from 5 randomly generated solutions, and each optimization step produces at least 8 new solutions . We use the Gurobi solver to construct the oracle solutions and compute the optimality gap for all approaches . Besides evaluating OPRO, we compare OPRO to the following heuristic"
57,"partial, solution, neighbor, nn, nearest","Nearest Nei ghbor  Starting from an initial node, the solution is constructed with the nearest neighbor heuristic . At each step, among the remaining nodes that are not included in the current partial solution, NN selects the node with the shortest distance to the end node of the"
60,"runs, optimization, steps, opro, successful","""# steps"" calculates the mean  standard error of optimization steps for successful runs that find the optimal solution . When no optimal solution is found for any evaluated problem, the corresponding number of steps is N/A."
63,"llm, optimization, steps, gap","We randomly generate 5 problem instances for each number of nodes n . In addition to measuring the optimality gap, we also show the number of optimization steps taken to reach the global optimum . On larger-scale problems, gpt-4 still finds solutions with a comparable quality to heuristic algorithms ."
65,"optimization, opro, landscape","OPRO is designed for neither outperforming the stateof-the-art gradient-based optimization algorithms for continuous mathematical optimization . Instead, the goal is to demonstrate that LLMs are able to optimize different kinds of objective functions simply through prompting, and reach the global optimum for some smallscale problems ."
69,"optimization, accuracy, test, llm, set","We focus on prompt optimization for natural language tasks, where both the input and output are in the text format . The task is represented as a dataset with training and test splits . We compute the test accuracy on the test set after the optimization finishes ."
80,"optimization, 2-l, palm, llm, prompt",INS> denotes the position where the generated instruction will be added . The blue text contains solution-score pairs; the purple text describes the optimization task and output format .
89,"problem, optimization, llm, training, set","The problem description includes a few examples taken from the training set to demonstrate the task for the generated instructions . For example, from the input-output pair in Figure 3, we can infer this is a math word problem . In each optimization step, we add several training examples to the meta-prompt ."
99,"math, hard, commonsense, word, reasoning, big-bench, problems","GSM8K is a benchmark of grade school math word problems with 7,473 training samples and 1,319 test samples . BBH offers 23 challenging BIG-Bench tasks that covers a wide range of topics beyond arithmetic reasoning . Each task contains up to 250 examples in total."
101,"optimization, details, trajectory, studies, ablation","We set the default temperature to be 0 when evaluating the performance of generated instructions . At each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8 instructions, then we add these instructions with their training scores to the optimization trajectory . Appendix C.2 presents the full"
109,"optimization, set, training, gsm8k","the same subset is used throughout optimization, SO that the task accuracies computed at intermediate optimization steps are approximations of the training accuracy on all 7,473 training examples . After the optimization procedure finishes, we evaluate the found instructions on the entire GSM8K test set."
119,"operations, accuracy, appropriate, training, mathematical","""Solve the following problems using the given information,"" at Step 2 with training accuracy 59.8 . ""Let's read the problem carefully and identify the given info. Then, we can create an equation and solve for the unknown variable,"" says Step 4 ."
120,"improvement, qualitative, step-by-step, opro","Figure 1 experiment found ""Let's do the math!"" at Step 6 with training accuracy 78.2 . This is because a leap in our optimization curve does not always correspond to a much better instruction being discovered . The latter usually happens several steps after the latter ."
121,"2-l, palm, performance, prediction","Figure 4 shows that the pre-trained PaLM 2-L performs better when the prompt is formatted in a few-shot manner . The generated instructions follow the same style as ""The answer is"" most instructions are also phrases suitable as the prefix of a sentence ."
129,"appendix, e, optimization, initialization","optimization starts from an empty string as the initial instruction by default . The instructions are placed at A start when the scorer is PaLM 2-L . For each task, we utilize a subset of 20% examples for prompt optimization ."
130,"empty, accuracy, per-task, opro","Figure 5 shows the per-task accuracy difference on all 23 BBH tasks compared to the instruction ""Let's think step by step.' Our instructions outperform by over on 5% on 19/23 tasks with the PaLM 2-L scorer, and 15/23 with the text-bison scorer ."
133,"movie, accuracy, artist, humorous, training, names, edits","""Consider the following when editing artist or movie names humorously"" at Step 1 with training accuracy 72.0 . ""We can make humorous edits of artist/movie names by changing letters to create new words that sound similar"""
136,"the, when, period, person, time","""To solve this problem, we need to first identify the time period when the person was not seen doing anything else"" at Step 2 with training accuracy 42.0; ""To find the time periods when a person could have gone to a place"""
141,"accuracy, period, 72.0, time",rule out any time periods during which the person was not seen doing anything else . The remaining time periods are the possible times when the person could have gone to the place .
149,"optimization, step, gsm8k, test, by, stability, set","""Let's think step by step"" achieves accuracy 71.8 . The PaLM 2-L scorer is only 49.4 . This behavior increases the variance across single-step instructions ."
155,"accuracy, scores, optimizer, quality, llm, difference",Figures 7 and 7 show that the default setting achieves better final accuracies and converges faster . One hypothesis is that the optimizer LLM output is affected more by past instructions closer to the end of the meta-prompt .
164,"llm, trajectory, optimization","presenting exemplars in the meta-prompt is critical, as it provides information on what the task looks like and helps the optimizer model phrase new instructions better . a few exemplar parts are usually sufficient to describe the task ."
165,"optimization, gradient, descent, stochastic, llm","Calculating a mini-batch of gradients reduces the variance of a stochastic gradient descent procedure . generating multiple instructions in each step improves optimization stability with LLMs . To achieve better performance with a fixed budget, the number of per-step instructions should not be too large ."
166,"empty, string, text-bison, llm","Figure 9 shows the performance of text -bi son as the scorer LLM with 3 options of initial instructions . We observe that the accuracies do not differ much with  different starting points . Most of the generated instructions starting from and contain the phrase ""solve this problem"""
177,"optimization, string, empty, scorer, llm","Figure 9 presents the results of PaLM 2-L as the scorer LLM with the following options of initial instructions: ""Let's solve the problem"" starting from leads to better generated instructions than in the first 30 steps, while the instructions optimized from both and are worse than throughout . A similar observation holds when using P a"
182,"optimization, step, curve, diversity, per","We evaluate the following temperatures of the optimizer LLM: 0.0, 0.5, 1.0 , 1.5, 2.0 . Figure 10 shows the default temperature 1.0 achieves the best performance . Optimizers with smaller temperatures lack exploration and thus creativity ."
183,"optimization, step, 2-l-it, trajectory, optimizer, palm, multiple, instruction","a baseline generates all instructions in a single step without entering into the optimization procedure . For GSM8K the scorer LLM is pre-trained PaLM 2-L and the initial instruction is ""Let's solve the problem"" BBH sports_understanding compares these two approaches to the paLM 2-"
184,"gsm, gsm8k","Our results show that this one-step instruction generation performs much worse than our optimization approach . On GSM8K, the best instruction among all 50 is still ""Let's solve the problem"" with a 78.2 training accuracy and a 76.3 test accuracy ."
187,"validation, opro, set","Overfitting is less harmful when each candidate solution overfits to a similar extent . In this case, a higher training accuracy solution still achieves a better validation/test accuracy . OPRO can adopt solutions with the highest training accuracy as the final result ."
195,"ga, optimization, differential, meta-prompt, evolution, prompt","Some concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to perform mutation and crossovers of existing prompts . In our evaluation, we compare our approach to the Genetic Algorithm and Differential Evolution versions of EvoPrompt."
196,"understanding, optimization, sports, bbh, benchmarks",Figure 12 presents results on GSM8K and BBH sports_understanding benchmarks . We use gpt-3  5-turbo as the optimizer . EvoPrompt does not utilize exemplars for prompt optimization .
197,"understanding, optimization, sports, trajectory, bbh","EvoPrompt is able to find better prompts than the initial ones, but the optimization curve is less stable than OPRO . This indicates that leveraging the optimization trajectory helps the LLM to identify promising directions to improve existing prompts."
203,"optimization, gradient-free, prompt-tuning, llm, api, prompt",Prior works have developed soft prompt-tuning methods that optimize the prompt represented as task-specific continuous vectors . These approaches become inapplicable when there is only API access to the LLM . Other works designed edit-based approaches for gradient-free prompt optimization .
204,"language, natural, llm, feedback","Yuan et al. develops a human-in-the-loop framework for deriving system-level feedback from a collection of instances . This is then used to revise the model output, which has shown effectiveness in reducing LLM outputs ."
208,"code, optimization, soft, generation, model, tuning, diff, large, language, prompt","Meyerson et al. uses language models with few-shot exemplars to propose evolutionary cross-overs on tasks such as image and code generation . In Lehman and lehman, large language model trained on code diff generation is used as the mutation operator, and they further design a fine-tuning"
210,"salesman, optimization, traveling, llm, problems","LLMs generate new solutions to optimize an objective function . We motivate OPRO with linear regression and traveling salesman problems . For prompt optimization, optimized prompts outperform human-designed prompts on GSM8K ."
211,"llm, optimization, prompt","a number of unresolved questions are open for future research on LLMs for optimization . For prompt optimization, one limitation of our current implementation is that the optimizer LLM does not effectively utilize error cases in the training set to infer promising directions to improve the generated instructions . Another limitation is that prompt optimization requires"
213,"gsm8, big-bench, hard, gsm8k",This work uses synthetic math problems for linear regression and traveling salesman problems . These tasks have been commonly used in similar works . There is a peril that LLMs may generate harmful information .
300,"optimizer, value, llm","LLMs often output contents like ""the function value at is 15"" despite that the true value is not 15. The model will get it right if external tools that can reliably calculate the value are triggered ."
301,"llm, old, generation, pair","optimizer LLMs do not 100% reliably follow this instruction even if its own outputs often include sentences like ""I will provide a new pair that is different"" Thus triggering such behaviors may be a solution . How to implement this feature without harming the instruction following performance of other parts remains an interesting topic to study"
302,"math, optimization, ws, black-box, meta-prompt","In black-box math optimization, getting stuck at a point that is neither global nor local optimal: The in-context exemplars all share the same w or 6 that is different from Wtrue or btrue . This case is more likely to be avoided when a larger number of past solutions are included"
303,"function, black-box, loss, landscape","Optimizer LLM sees a decrease of objective value when it drastically decreases both x and y to 0. Then starting from , the optimizer LLM is hard to further navigate the loss landscape towards x . For example, when minimizing the Rosenbrock function f = 2 +b2 with "
334,"trace, below, coordinates","Below are some previous traces and their lengths . The traces are arranged in descending order based on their length, where lower values are better ."
389,"empty, string, prompt, optimization","Table 11, 12 and 13 show the instructions found by prompt optimization . Their accuracies are listed in Table 10 . The optimizations find instructions better than the empty starting point ."
390,"a, optimization, 2-l-it, palm, sentences, interrogative, begin","In Section E.3, we show the A start optimization results with the non-empty starting point ""Let's solve the problem"" Most results there are declarative sentences - more suitable for A start ."
409,"slot, alignment, accurate, color, time","Effect Our Instruction boolean_expressions Accurately use order of operations and parentheses to evaluate logical expressions and determine truth values efficiently . causal_judgement Consider all relevant factors, prioritize overall well-being and ethical considerations, make well-informed decisions while foreseeing potential consequences efficiently"
425,"accurately, information, accurate, creativity",dyck_ languages Let's accurately determine the correct date based on the given information and select the corresponding option in the standard MM/DD/YYY format with utmost precision and reliability . disambiguation_qa ensuring the most definitive and reliable solution possible for accurate representation in all scenarios
