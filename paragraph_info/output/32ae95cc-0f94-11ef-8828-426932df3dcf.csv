element_idx,keywords,summarized_text
5,"computation, state-of-art, language, models, method","CoT uses language models to produce text describing reasoning, and computation . In PoT, the computation can be delegated to a program interpreter . We evaluate PoT on five math word problem datasets and three financialQA datasets ."
7,"math, (mwp), word, problems",A surge of datasets has been proposed recently to benchmark deep-learning models' capabilities to perform numerical/ arithmetic reasoning . Some widely used benchmarks are based on Math word problems .
8,"language, rationales, natural, fine-tune, state-of-the-art, performance, models, llms","Prior work has studied how to train models from scratch or fine-tune models to generate intermediate steps to derive the final answer . Such methods are data-intensive, requiring a significant number of training examples with expert-annotated steps ."
19,"computation, expressions, model, reasoning, language, mathematical","CoT uses LLMs for both reasoning and computation, i.e. the language model needs to generate the mathematical expressions . We argue that language models are not ideal for actually solving these math expressions, because: 1) LLM is very prone to arithmetic calculation errors, especially when dealing with large numbers"
20,"python, program-of-thoughts, complex, equation","In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter . In the upper example, CoT cannot solve the cubic equation with language models and outputs a wrong answer . in the lower example, PoT can express the iteration process with "
21,"pot, cot, self-consistency, pot+sc, (sc)","We evaluate PoT prompting across five MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, MultiArith . These datasets cover various input formats including text, tables, and conversation . Under both fewshot and zero-shot settings, PoT outperforms Co"
30,"input-output, exemplars, llms","In-context learning only takes a few annotations/demonstrations as a prompt . LLMs receive input-output exemplars as the prefix, followed by an input problem . This approach has been shown to elicit LLM's strong reasoning capabilities on various kinds of tasks ."
36,"interest, natural, variable, language, rate, names","programs can also be used to express our thought processes . For example, in Figure 1, we first create an unknown variable named interest_rate . Then we bind 'summation in two years with ... interest rate' to the variable sum in_two_years_ with_XXX_ interest ."
38,"comparison, thoughts, generation, language, models","The 'program of thoughts' is different from generating equations directly . The generation target would be solve3 - 2000 - x * 20000 * 3 - 1000, x) PoT breaks down the equation into a multi-step 'thought' process ."
39,"zero-shot, llm, few-shot, setting","We show the proposed PoT prompting method in Figure 3 . Under the few-shot and zero-shot settings, a few exemplars of pairs will be prefixed as demonstrations ."
48,"prompting, questions, pot-only, aqua, multi-choice","In Figure 3, the program will be executed to return a float number 'ans=2.05' which means that after 2.05 hours the two trains will meet . However, directly adding 2.05 to 11 AM does not make sense . Please note that this prompting strategy is only needed for the AQuA because the other"
54,"conversational, linearized, text, conversation, string, table, question","For table inputs, we adopt the same strategy as Chen to linearize a table into a text string . The columns of the table are separated by 'P' for text+table hybrid inputs we separate tables and text with 'n' For conversational history, we also separate conversation turns by n"
57,"code-davinci-002, api2, code-turbo-3.5, code-","Implementation Details We mainly use OpenAI Codex API2 for our experiments . We also tested GPT-3 , ChatGPT , CodeGen, CodeT5+ and Xgen3 for ablation experiments. For the few-shot setting, we use 4-8 shots for all the datasets, based on"
58,"llm, reasoning, multi-step","To elicit the LLM's capability to perform multi-step reasoning, we found a prompt to encourage LLMs to generate reasonable programs without demonstration . However, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program . Therefore, we suppress the"
59,"gsm8k, sv, github, svamp","Metrics We adopt exact match scores as our evaluation metrics for GSM8K, SVAMP, and MultiArith datasets . We will round the predicted number to a specific precision and then compare it with the reference number . For the AQuA dataset, we use PoT to compute the intermediate answer and then"
60,"lamda, palm, api","Baselines We report results for three different models including Codex , GPT-3 , PaLM and LaMDA . We consider two types of prediction strategies including direct answer output and chain of thought ."
62,"cot+calc, fem","PoT improves on GSM8K / AQuA/' TabMWP by more than 8% . On SVAMP, the improvement is 4% mainly due to its simplicity . The larger improvements in FinQA and ConvFinQA are due to miscalculations on LLMs for large"
68,"gsm, gpt-4, gsm8k","Published SoTA includes the best-known results . On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding ."
72,"pot, prompting, few-shot, performance, zero-shot","Zero-shot Results We also evaluate the zero-shot performance of PoT and compare with Kojima et al. in Table 3 . On the evaluated datasets, PoT's outperforms CoT by an even larger margin ."
81,"code-davinci-002, source, backbone, open, models","Backend Ablation To understand PoT's performance on different backbone models, we compare the performance of text-davinci-002 . We choose three representative datasets GSM8K, SVAMP, and FinQA . As can be seen, gpt-3.5-turbo can"
82,"sensitivity, gsm8k","Sensitivity to Exemplars To better understand how sensitive PoT is w.r.t different exemplars, we conduct a sensitivity analysis . For k-shot learning, we randomly sample k = out of the 20 exemplaires three times as v1, v2 and v"
91,"reasoning, multi-step","Semantic Binding and Multi-Step Reasoning The two core properties of 'program of thoughts' are: multiple steps: breaking down the thought process into the step-by-step program, semantic binding: associating semantic meaning to the variable names . To better understand how these two properties contribute, we "
92,"symbolic, equation, polynomial, combinatorics, aqua, symboli","We manually classify the questions in AQuA into several categories . The major categories are linear equations, arithmetic, combinatorics, probability, and iterative . We show the accuracy for each subcategory in Figure 6 ."
99,"correct, grounding, generation, value, logic, values, error","Error Analysis We considered two types of errors: value grounding error, and logic generation error . The first type indicates that the model fails to assign correct values to the variables relevant to the question . In the lower example, the model fetches the value of the variables incorrectly while the computation logic is correct ."
102,"mathqa, mathqa-pypy, llm",LiLA proposes to assemble a large set of mathematical datasets into a unified dataset . LiLA also annotates Python programs as the generation target for solving mathematical problems .
106,"few-shot, lm, natural, predictions, language, gpt-3, smaller","GPT-3 demonstrated a strong capability to perform few-shot predictions . Model size, data, and computing are crucial to enable this learning ability . Recently, Rae et al. have proposed to train different types of LLMs with different training recipes ."
108,"code, numerical, llm, reasoning, robust","Recently, CoT was proposed to enable LLM's capability to perform reasoning tasks by demonstrating 'natural language rationales' Suzgun et al. have shown that CoT can already surpass human performance on challenging BIG-Bench tasks ."
110,"pot, self-eval, self-solve, planning, self-evaluation",Self-critic and self-eval adopt self-estimation to enhance the robustness of the generated program . plan-and-solve adopt more detailed planning instruction to help LLMs create a high-level reasoning plan .
111,"python, transformer, models","These work propose to adopt different tools to help the language models ground on external world . These work generalizes our Python program into more general API calls to include search engine, string extraction, etc."
113,"commonsense, reasoning, numerical","In this work, we have verified that our prompting methods can work efficiently on numerical reasoning tasks like math or finance problem solving . We also study how to combine PoT with CoT to combine the merits of both prompting approaches ."
115,"computation, numerical, llm, reasoning, problems","In this work, we investigate how to disentangle computation from reasoning in solving numerical problems . By 'program of thoughts' prompting, we are able to elicit LLMs' abilities to generate accurate programs to express complex reasoning procedure . This approach can boost the performance on several math datasets ."
120,"code, math, generated, llm, aqua, problems","PoT would require execution of 'generated code' from LLMs . This could contain certain dangerous or risky code snippets like 'import OS; os.rmdir', etc."
156,"nlp, goyal, navin, goy","Arkil Patel, Satwik Bhattamishra, and Navin Goyal . Are NLP models really able to solve simple math word problems?"
176,"zhu, content, textual, and, tabular, fengbin",Tat-qa: A benchmark on a hybrid of tabular and textual content in finance . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing .
195,"egg, eggs, breakfast, every, janet, ever",Janet's ducks lay 16 eggs per day . She sells the remainder at farmers' market daily for $2 per fresh duck egg .
197,"house, of, ans, cost, value, josh, original, return","Josh buys a house for $80,000 and then puts in $50,000 in repairs . This increased the value of the house by 150% ."
198,"chicken, fe, feed, wendi","Every day, Wendi feeds each chickens three cups of mixed chicken feed . In the morning, she gives her flock of chickens 15 cups of feed, in the afternoon, 25 cups . How many cups does she need to give her chickens in the final meal of the day? # Python code, return ans"
205,"marissa, mile, remaining, miles, average, speed","Marissa is hiking a 12-mile trail . She took 1 hour to walk the first 4 miles . If she wants her average speed to be 4 miles per hour, what speed does she need to walk?"
206,"lemon, tree, carlos, plant","Each year it will grow 7 lemons, which he can sell for $1.5 each . It costs $3 a year to water and feed the tree ."
208,"tomato, cooks, freda, sauce",Each 16 ounce can of tomatoes that Freda uses contains three tomatoes . Each tomato sauce made 32 ounces of sauce . How many tomatoes did Freda use? # Python code .
209,"bir, cake, birthday, jordan, homemade","Jordan wanted to surprise her mom with a homemade birthday cake . From reading the instructions, she knew it would take 20 minutes to make the cake batter and 30 minutes to bake . The cake would require 2 hours to cool and an additional 10 minutes to frost ."
211,"duration, aircraft","in a flight of 600 km, an aircraft was slowed down due to bad weather . Its average speed for the trip was reduced by 200 km/hr . Time of flight increased by 30 minutes ."
213,"money, deposit, interest",a sum of money at simple interest amounts to Rs. 815 in 3 years . The sum is: # Answer option: deposit = Symbol interest . Symbol money_in_3_years .
216,"speaking, french, employees",35% of employees of a company are men . 60% of the men in the company speak French and 40% of the employees speak French . num_women = 65 men_speaking_french = 0.6 .
217,"boat, stream, speed","In one hour, a boat goes 11 km/hr along the stream . The speed of the boat in still water is: # Answer option: boat_speed = Symbol stream_speed ."
218,"rate, c.i., simple, interest",the difference between simple interest and C.I. at the same rate for Rs.5000 for 2 years in Rs.72 . The rate of interest is? # Answer option: interest_rate = Symbol amount = 5000 amount_with_simple_interest = amount * amount_without_compound_interest= amount
219,"permimeter, area, rectangle",The area of a rectangle is 15 square centimeters and the perimeter is 16 centimetres . What are the dimensions of the rectangle? # Answer option: width = Symbol height = 15 permimeter = 16 solution = solve_it ans = .
