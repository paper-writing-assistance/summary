element_idx,summarized_text,keywords
4,"SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features . It does not need positional encoding, thereby avoiding the interpolation of positional codes . The proposed MLP decoder aggregates information from different layers .","perceptron, multilayer, lightweight, segformer"
6,Semantic segmentation is a fundamental task in computer vision . It produces per-pixel category prediction instead of image-level prediction . This relationship is pointed out and systematically studied in a seminal work .,"image, segmentation, fcn, semantic, classification"
7,"semantic segmentation frameworks are variants of popular architectures for image classification on ImageNet . The evolution of backbone architectures has remained an active area . Since early methods using VGGs , backbones have remained a focus .","classi-fication, imagenet, image, segmentation, semantic, classification"
15,"Dosovitskiy et al. proposed vision Transformer for image classification . Following the Transformer design in NLP, the authors split an image into multiple linearly embedded patches and feed them into a standard Transformer with positional embeddings .","imagenet, image, vision, transformer, classification"
16,SETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution . ViT outputs single-scale low-resolution features instead of multi-scale ones . It has high computation cost on large images .,"vision, pyramid, backbone, pvt, transformer, cnn, decoder"
19,"proposed encoder avoids interpolating positional codes when performing inference on images with resolutions different from the training one . In addition, our encoder can easily adapt to arbitrary test resolutions without impacting the performance . We propose a lightweight MLP decoder where the key idea is to take advantage of the","lightweight, mlp, decoder"
20,"On Citysapces, our lightweight model, SegFormer-B0, yields 71.9% mIoU at 48 FPS . This represents a relative improvement of 60% and 4.2% in latency and performance respectively .","cityscapes, ade20k, segformer"
22,Semantic segmentation can be seen as an extension of image classification from image level to pixel level . FCN is a fully convolution network that performs pixel-to-pixel classification .,"image, segmentation, pixel, fcn, semantic, level, classification"
26,More recent methods have proved the effectiveness of Transformer-based architectures for semantic segmentation . These methods are still computationally demanding and complicated .,"architectures, technologies, automl, transformer-based"
27,ViT is the first work to prove that a pure Transformer can achieve state-of-the-art performance in image classification . ViT treats each image as a sequence of tokens and then feeds them to multiple Transformer layers . More recent methods introduce tailored changes to ViT to further improve image classification performance .,"image, vit, backbones, transformer, classification"
28,"PVT is the first work to introduce a pyramid structure in Transformer . This is demonstrating the potential of a pure Transformer backbone compared to CNN counterparts in dense prediction tasks . After that, methods such as Swin , CvT, CoaT , LeViT and Twins enhance the","tasks, prediction, pvt, transformer, cnn, dense"
29,"DETR is the first work using Transformers to build an end-toend object detection framework without non-maximum suppression . Other works have also used Transformers in a variety of tasks such as tracking , super-resolution , Colorization , Retrieval and multi-modal learning.","multi-modal, nms, learning"
31,SegFormer consists of two main modules: a hierarchical Transformer encoder to generate high-resolution coarse features; and a lightweight All-MLP decoder to fuse these multi-level features to produce the final semantic segmentation mask .,"segmentation, framework, segformer"
32,Given an image of size H x W x 3 we first divide it into patches of size 4 x 4 . We then pass these multi-level features to the H W Ncls is the All-MLP decoder .,"image, all-mlp, segmentation, decoder"
37,"Unlike ViT that can only generate a single-resolution feature map, the goal of this module is, given an input image, to generate CNN-like multi-level features . These features provide high-resultaneous coarse features and low-reresolution fine-grained features that usually boost the","hierarchical, feature, features, multi-level, representation"
38,"Given an image patch, the patch merging process is used in ViT . This can easily be extended to unify a 2x 2 x Ci feature path into a 1x 1 x C vector to obtain hierarchical feature maps . To this end, we define K, S, and P, where K","overlapping, patch, merging"
41,"C  R refers to reshape K to the one with where K is the sequence to be reduced, Reshape and Linear refer to a linear layer taking a Cin-dimensional tensor as input . In our experiments, we set R to from stage-1 to stage-4 .","linear(c, c, linear(cin, Â·, r)"
42,"CPVT uses 3 x 3 Conv together with the PE to implement a data-driven PE . To alleviate this problem, we introduce Mix-FFN which considers the effect of zero padding to leak location information .","encoding, conv, positional, 3, x"
47,"The proposed All-MLP decoder consists of four main steps . First, multi-level features Fi from the MiT encoder go through an MLP layer . In a second step, features are up-sampled to 1/4th and concatenated together .","mlp, segmentation, mask, layer, m"
51,"In Figure 3, we visualize ERFs of the four encoder stages and the decoder heads for both DeepLabv3+ and SegFormer . We can make the following observations:","erf, mlp, segmentation, heads, decoder, erfs, semantic, of, the"
53,"The ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage . SegFormer's encoder produces local attentions which resemble convolutions at lower stages .","deeplabv3+, attention, local"
54,the limited receptive field in CNN requires one to resort to context modules such as ASPP . Our decoder design benefits from the non-local attention in Transformers . We will verify this later in Table 1d .,"cnn, limited, field, receptive"
55,Our MLP decoder design essentially takes advantage of a Transformer induced feature that produces both highly local and non-local attention at the same time . This is another key reason that motivated our design .,"induced, mlp, feature, transformer, decoder"
60,"SegFormer's encoder has a hierarchical architecture, which is smaller than ViT . In contrast, SETR's ViT encoder can only generate single low-resolution feature map . SETR uses fixed shape Positional Embedding which decreases accuracy .","features, positional, embedding, low-resolution, segformer"
63,Cityscapes is a driving dataset for semantic segmentation . COCO-Stuff covers 172 labels and consists of 164k images .,"city, cityscapes"
64,We used the mmsegmentation codebase and train on a server with 8 Tesla V100 . We pre-train the encoder on the Imagenet-1K dataset and randomly initialize the decoder . Following we set crop size to 640 x 640 on ADE20K for our largest,"mlou, segmentation, codebase, semantic, ade20k, cityscapes"
66,"Figure 1 shows the performance VS. model efficiency for ADE20K as a function of the encoder size . For MiT-B5 encoder, the decoder only takes up to 4% of the total number of parameters in the model . In terms of performance, we can observe that, overall, increasing","size, performance, segformer-b5, model, efficiency, ade20k"
67,"In Table 1b we show performance, flops, and parameters as function of this dimension . We can observe that setting C = 256 provides a very competitive performance and computational cost . This performance plateaus for channel dimensions wider than 768 .","flops, performance, channel, mlp, dimension, decoder"
78,"Mix-FFN VS. Positional Encoder . To this end, we train Transformer encoders with a positional encoding . We perform inference on Cityscapes with two different image resolutions .","mix-ffn, encoding, encoder, positional, transformer"
79,"Table 1c shows the results for this experiment . For a given resolution, our approach using Mix-FFN clearly outperforms using a positional encoding . In contrast, when we use the proposed Mix FFN the performance drop is reduced to 0.7% .","mix-ffn, positional, encoding"
82,"MLP-decoder with a CNN-based encoder yields a significantly lower accuracy compared to coupling it with the proposed Transformer encoder . Intuitively, coupling the encoder with the MLP decoder leads to the best performance .","mlp-decoder, transformer, cnn"
85,"Table 2 summarizes our results including parameters, FLOPS, latency, and accuracy for ADE20K and Cityscapes. In the bottom part, we focus on performance and report the results of our approach and related works using stronger encoders.","ade20k, cityscapes, lightweight, encoder"
86,"SegFormer-B0 yields 37.4% mIoU using only 3.8M parameters and 8.4G FLOPs, outperforming all other real-time counterparts in terms of parameters, flops, and latency . For instance, compared to DeeplabV3+, SegForm-","miou, application, state-of-the-art, ade20k"
87,SegFormer-B0 runs at 47.6 FPS and yields 71.9% mIoU . This is 17.3 FPS faster and 4.2% better than ICNet .,"cityscape, segformer-b5, segformer-b0, ta, table, cityscapes, 2"
88,"On Cityscapes test set, we follow the common setting and merge the validation images to the train set and report results using Imagenet-1K pre-training . Our method achieves 82.2% mloU outperforming all other methods including SETR . SegFormer provides better details than SETR and smoother","mapillary, validation, pre-training, images, imagenet-1k, vistas"
91,SegFormer-B5 reaches 46.7% mIoU with only 84.7M parameters . The flops on this dataset are similar to those reported for ADE20K .,"setr, coco-stuff, segformer, dataset"
105,"In this paper, we present SegFormer, a simple, clean yet powerful semantic segmentation method . It contains a positional-encoding-free, hierarchical Transformer encoder and a lightweight AllMLP decoder . We hope our method can serve as a solid baseline and motivate further research ","segmentation, semantic, common, segformer, datasets"
113,Pi: the padding size of the overlapping patch embedding in Stage i . Ci: the channel number of the output of Stage 2; Li: the number of encoder layers in Stage 2 .  Ri: the reduction ratio of the Efficient Self-Attention .,"patch, number, overlapping, embedding, head"
119,"In Figure 6, we select some representative images and effective receptive field . SegFormer's ERF learned the pattern of roads, cars, and buildings . Our Transformer encoder has a stronger feature extraction ability than ConvNets.","erf, segformer, of, deeplabv3+"
139,"In CVPR, 2015. 1, 2, 7 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected C","convolutional, image, segmentation, semantic, deep, cvpr, networks"
141,"Contextnet: Exploring context and detail for semantic segmentation . In CVPR, 2018. Junjun He, Zhongying Deng, Lei Zhou, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improving","segmentation, network, attention, semantic"
143,"In ICCVW, 2019. Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Squeezenas: Fast neural architecture search for faster real-time semantic segmentation . In CVPR","transformer, vision, dynamic, routing"
145,"Transformer is all you need: Multimodal multitask learning with a unified transformer . arXiv, 2021. 3 Md Amirul Islam, Sen Jia, and Neil DB Bruce . Understanding the effective receptive field in deep convolutional neural networks.","multitask, neural, multimodal, axial-attention, learning, networks"
