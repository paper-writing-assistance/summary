element_idx,keywords,summarized_text
5,"encoder, bidirectional, transformers, representations",BERT is designed to pretrain deep bidirectional representations from unlabeled text . The pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models .
6,"multinli, accuracy, acc, bert","BERT obtains state-of-the-art results on eleven natural language processing tasks . It pushs the GLUE score to 80.5% , MultiNLI accuracy to 86.7% ."
8,"tasks, inference, token-level, natural, language, processing",Language model pre-training has been shown to be effective for improving many natural language processing tasks . These include natural language inference and paraphrasing . models are required to produce fine-grained output at token level .
9,"applying, task-specific, architecture, fine-tuning, pre-training","feature-based approach, such as ELMo , uses task-specific architectures that include pre-trained representations as additional features . The fine-tuning approach introduces minimal tasks-specific parameters and is trained on downstream tasks by simply fine tuning all pretrained parameters ."
10,"tasks, openai, token-level, gpt, fine-tuning","the major limitation is that standard language models are unidirectional . This limits the choice of architectures that can be used during pre-training . For example, in OpenAI GPT, the authors use a left-toright architecture ."
11,"transformers, bert, from, masked, representations, model, encoder, language, bidirectional",BERT: Bidirectional Encoder Representations from Transformers . The objective is to predict the original vocabulary id of the masked language model .
13,"right, mlm, word, model, language, left-to-, transformer, bidirectional","the MLM objective enables the representation to fuse the left and the right context . We also use a ""next sentence prediction"" task that jointly pretrains text-pair representations ."
14,"of, bert, state, representations, the, art, bidirectional",BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks . The code and pre-trained models are available at https :/ / github  com/ googl
18,"nlp, guage, pre-trained, embedding, left-to-right, lan-, word, modeling",Pre-trained word embeddings are an integral part of modern NLP systems . Left-to-right language modeling objectives are used to discriminate correct from incorrect words in left and right context .
19,"embeddings, coarser, representations, granularities, sentence","These approaches have been generalized to coarser granularities . To train sentence representations, prior work has used objectives to rank candidate next sentences ."
20,"contextual, embedding, elmo, word, traditional","Melamud et al. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs . Similar to ELMo, their model is feature-based and not deeply bidirectional ."
23,"openai, gpt",OpenAI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark . The advantage of these approaches is that few parameters need to be learned from scratch .
28,"genet, translation, large, datasets, ima-, machine",Computer vision research has demonstrated the importance of transfer learning from large pre-trained models . An effective recipe is to fine-tune models with ImageNet . There has also been work showing effective transfer from supervised tasks .
30,"fine-tuning, bert, model","During pre-training, the model is trained on unlabeled data over different tasks . For finetuning, the BERT model is first initialized with the pre-trained parameters . Each downstream task has separate fine-tuned models ."
33,"transformer, model, bert, architecture",Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. We will omit an exhaustive background description of the model architecture .
39,"token, bert, sequence, representation, input","Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence . A ""sentence"" refers to the input token sequence to BERT, which may be"
40,"embeddings, token, cls, special, classification, wordpiece","We use WordPiece embeddings with a 30,000 token vocabulary . The first token of every sequence is always a special classification token ."
44,"conditioning, model, right-to-left, language, bidirectional","Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model . standard conditional language models can only be trained left to right or right-to left . the model could trivially predict the target word in a"
46,"cloze, task, wordpiece, tokens","In order to train a bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens . In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random ."
47,"bidirec-, fine-tuning, tional, pre-training","To mitigate this, we do not always replace ""masked"" words with the actual token . The training data generator chooses 15% of the token positions at random . Ti will be used to predict the original token with cross entropy loss ."
49,"next, nli, prediction, nsp, sentence, task","In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus . Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows"
54,"bookscorpus, booksc","For the pre-training corpus we extract only the text passages and ignore lists, tables, and headers . It is critical to use a document-level corpus rather than a shuffled sentence level corpus such as the Billion Word Benchmark to extract long contiguous sequences."
56,"self-attention, fine-tuning, mechanism, bert","BERT uses the self-attention mechanism in the Transformer to model many downstream tasks . For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention ."
64,"fine-tuning, token, glue, log(softmax)","To fine-tune on GLUE, we represent the input sequence as described in Section 3 . We use the final hidden vector C E RH corresponding to the first input token as the aggregate representation ."
69,"shuffling, fine-tuning, data, glue","We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks . For each task, we selected the best fine tuning learning rate on the Dev set ."
70,"openai, gpt","Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin . For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement ."
76,"task, answer-, ing","In the question answering task, we represent the input question and passage as a single packed sequence . We only introduce a start vector S E RH and an end vector E E R H during fine-tuning . The probability of word i being the start of the answer span is computed by a dot"
77,"squad, leaderboard, top",Table 2 shows top leaderboard entries as well as results from top published systems . 11 use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA .
87,"bert, span, answer, model, squad, v1.1",We use a simple approach to extend the SQuAD v1.1 BERT model . We treat questions that do not have an answer as having an answer span . The probability space for the start and end answer span positions is extended to include the position of the token .
105,"left-context-only, openai, gpt, model, ltr","LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right LM, rather than an MLM . The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-t"
109,"token, elmo, model, rtl, bidirectional","this is twice as expensive as a single bidirectional model . This is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question ."
112,"parameters, tasks, bertlarge, contains, 340m, glue","In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning . We can see that larger models lead to a strict accuracy improvement across all four datasets . For example, the largest Transformer explored in Vaswani et al. is with 100M parameters for the encode"
113,"held-out, perplexity, translation, lm, modeling, training, language, machine","LM perplexity of held-out training data shown in Table 6 . This is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained."
114,"pre-trained, downstream, bi-lm, size, task","Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increased further to 1,000 did not bring further improvements . Both of these prior works used a feature-based approach ."
116,"bert, approach, model, encoder, fine-tuning, transformer","All of the BERT results presented so far have used the fine-tuning approach . The feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages . First, not all tasks can be easily represented by a Transformer encoder architecture ."
126,"nlp, tasks, learning, transfer, low-resource","Recent improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems . Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of tasks."
197,"transformer, encoder",Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words . This does not seem to harm the model's language understanding capability .
206,"tokenization, lm, wordpiece","To generate each training input sequence, we sample two spans of text from the corpus . The first sentence receives the A embedding and the second receives . 50% of the time B is the actual next sentence that follows A ."
208,"acti-, learning, vation, openai, gpt, rate, warmup, gelu","Adam with learning rate of 1e-4, B1 = 0.9, B2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps . We use a dropout probability of 0.1 on all layers ."
219,"openai, gpt",OpenAI GPT trains a left-to-right Transformer LM on a large text corpus . Many of the design decisions in BERT were intentionally made to make it as close to GPT as possible SO that the two methods could be minimally compared .
220,"bert, rate, learning","GPT uses a sentence separator and classifier token which are only introduced at fine-tuning time . BERT learns , and sentence A/B embeddings during pre-training ."
223,"fine-tuning, token-level, bert, task","BERT is incorporated with one additional output layer, SO a minimal number of parameters need to be learned from scratch . In Figure 4, E represents the input embedding, Ti represents the contextual representation of token i, is the special symbol for classification output ."
228,"qnli, sentence, question, answering, dataset","QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary classification task . Positive examples are pairs which do contain the correct answer, and negative examples are from the same paragraph which do not contain the answer ."
237,"openai, glue, gpt",WNLI Winograd NLI is a small natural language inference dataset . Every trained system submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class . We therefore exclude this set to be fair to OpenAI GPT .
244,"mnli, pre-training, mlm, model","Answer: Does BERT really need such a large amount of pre-training to achieve high fine-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps ."
249,"masking, fine-tuning, strategies",Masking strategies are to reduce mismatch between pre-training and fine-tuning . We report the Dev results for both MNLI and NER . The model will not have the chance to adjust the representations .
