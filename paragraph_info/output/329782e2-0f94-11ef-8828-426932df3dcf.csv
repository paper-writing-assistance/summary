element_idx,keywords,summarized_text
8,"tagging, networks, bio, connectionist, neural, segmental, recurrent, classification, temporal","segmental recurrent neural networks define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments . These ""segment embeddings"" are used to define compatibility scores with output labels . Fully supervised training-in which segment boundaries and labels are"
11,"learning, representation, networks, neural, recursive, segmental, network, prediction, bidirectional, structured","SRNNs combine two powerful machine learning tools . First, bidirectional recurrent neural networks embed every feasible segment of the input in a continuous space . These embeddings are then used to calculate compatibility of each candidate segment with a label ."
12,"random, conditional, fields, semi-markov","At the same time, SRNNs are a variant of semi-Markov conditional random fields . This allows explicit modeling of statistical dependencies, such as those between adjacent labels, and also of segment lengths . Because the probability score decomposes into chain-structured clique potentials, polynom"
19,"srnn, neural, segmental, recurrent, network",a segmental recurrent neural network defines a joint distribution p over a sequence of labeled segments each of which is characterized by a duration and label . We write the starting time of segment i as Si = 1 + Eji zj .
25,"candidate, concatenation, field, random, rnn, vector, label, segment, conditional, duration, segmentation","RNN computes the forward segment em1 by ""encoding"" the zi-length subsequence of x starting at index Si . gy and gz are functions which map the label candidate y and segmentation duration z into a vector representation . The notation denotes"
29,"bidirectional, representation, lstms, learning",Bidirectional LSTMs are a popular variant of RNNs which have been seen successful in many representation learning problems . We chose bidirectional SLTMs as the implementation of the RNN in Eq. 4.
33,"dynamic, sequence, programming, function, segmentation","We are interested in three inference problems: finding the most probable segmentation/labeling for a model given a sequence x; evaluating the partition function Z; and computing the posterior marginal Z . For simplicity, we will assume zeroth order Markov dependencies between the yis . Each of these algorithms"
39,"runtime, values, algorithm","In practice, we often can put a upper bound for the length of a eligible segment thus reducing the complexity of runtime to O . This savings can be substantial for very long sequences ."
46,"posterior, dynamic, z(x, program, y, marginal","To compute the posterior marginal Z, it is necessary to sum over all segmentations that are compatible with a label sequence y given an input sequence x . To do SO requires only a minor modification of the previous dynamic program to track how much of the reference label sequence has been consumed ."
48,"avoiding, similar, dynamic, overflow, summation, programming",2An alternative strategy for avoiding overflow in similar dynamic programs is to rescale the forward summations at each time step . In a semi-Markov architecture each term in ai sums over different segments .
60,"tagging, bio, connectionist, recognition, classification, temporal, task, segmentation","We present two sets of experiments to compare segmental recurrent neural networks against models that do not include explicit representations of segmentation . For the handwriting recognition task, we consider connectionist temporal classification ."
64,"al., variant, (2004), et, well-know, dataset, taskar","Taskar et al. selected a ""clean"" subset of about 6,100 words and rasterized and normalized the images of each letter . Then, the uppercased letters are removed and only the lowercase letters are used ."
70,"gold, ctc, segmentation, model",Implementation We trained two versions of our model on this dataset . The fully supervised model takes advantage of the gold segmentations on training data . A CTC model reimplemented on the top of our Encoder BiRNNs layer is used as a baseline SO For the decoding of the that we can
71,"information, corresponding, vector, dataset, dimensional","we first represented each point in the dataset using a 4 dimensional vector, p = . Px and Py are the normalized coordinates of the point . Then we map the points inside one stroke into a fixed-length vector ."
73,"hidden, dimension, state",We used 5 as the hidden state dimension in the bidirectional RNNs . We set the hidden dimensions of c in our model and CTC model to 24 and segment embedding h as 18 . These dimensions were chosen based on intuitively reasonable values .
75,"handwriting, ctc, model, recognition, task",Results The results of the online handwriting recognition task are presented in Table 2. We see that both of our models outperform the baseline CTC model . The partially supervised model performs slightly worse in the development set .
77,"tagging, word, pos, chinese, segmentation",The first task is joint Chinese word segmentation and POS tagging . The y variables assign POS tags as labels to these words .
78,"interpretation, aa, rules, ctc","4The CTC interpretation rules specify that repeated symbols, e.g. aa will be interpreted as a token of a . The segments in the handwriting recognition problem are extremely short . Our experiments indicate this has little effect, and this change does not harm performance in general ."
85,"word, standard, benchmark, dataset, chinese, segmentation","Dataset We used standard benchmark datasets for these two tasks . For the joint Chinese word segmentation task, we used the Penn Chinese Treebank 5 . This dataset contains four portions covering both simplified and traditional Chinese ."
86,"srnn, log, tagger, bi-directional, probabilities, lstm",Implementation Only supervised version SRNNs is tested in these tasks . It takes the c at each time step and pushes it through an element-wise non-linear transformation followed by an affine transformation to map it to the same dimension as the number of labels .
88,"h, embedding, tagger, character, bi-directional, baseline, input, segment, lstm","the dimension for input character embedding is 64 . For the baseline bi-directional LSTM tagger, we set the hidden dimension size to 128 . Here we deliberately chose a larger size than in our model ."
98,"labeling, ctc, connectionist, segmental, classification, temporal, problems","CTC reduces the ""segmental"" sequence label problem to a classical sequence labeling problem . Every position in an input sequence x is explicitly labeled by interpreting repetitions of input labels-or input labels followed by a special ""blank"" output symbol-as being a single label"
99,"token, ctc, segmentation, recognition, phone, explicit","CTC has several potentially serious limitations . First, it is not possible to model interlabel dependencies explicitly-these must instead be captured indirectly by theunderlying RNNs . Second, CTC is no explicit segmentation model . To illustrate the value of explicit segments, consider the problem of phone recognition ."
100,"marginalization, ctc, mechanisms, attention, problems","Several alternatives to CTC have been approached, such as using various attention mechanisms in place of marginalization . These have been applied to endto-end discriminative speech recognition problem ."
101,"abdel-hamid, multi-frame, segments","Abdel-Hamid's work seeks to construct embeddings of multi-frame segments . First, they compute representations of variable-sized segments by uniformly sampling a fixed number of frames . second, they do not consider them problem of latent segmentation ."
106,"information, labeling, representations, segmental, segment",We have proposed a new model for segment labeling problems that learns representations of segments of an input sequence and then labels them . We have not trained the segmental representations beyond making good labeling decisions .
