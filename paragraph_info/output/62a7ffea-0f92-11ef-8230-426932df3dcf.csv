element_idx,summarized_text,keywords
4,"Vision Transformers show superior performance with a more compact model size than conventional convolutional neural networks . This leads to a dramatic increase of computation and memory cost with the increase of sequence length, thus introducing difficulties when applying Transformers to vision tasks that require dense predictions based on high-resolution feature maps .","long-range, transformers, vision, dependencies, transformer"
5,"In this paper, we propose a new vision Transformer, called Glance-and-Gaze Transformer . It is motivated by the Glance and Gaze behavior of human beings . The Glance branch is achieved by performing self-attention on the adaptively-dilated partitions of the input .","natural, vision, gg-transformer, scenes, transformer"
7,Convolution Neural Networks have been dominating the field of computer vision . CNNs model images from a local-to-global perspective . They lack the ability to globally represent long-range dependencies .,"tasks, long-range, vision, dependencies, cnn, computer"
8,Transformers naturally learn global features in a parameter-free manner . This makes them stronger alternatives and raises questions about the necessity of CNNs in vision systems .,"system, transformer, vision, cnn"
11,"Self-attention operates on the whole sequences, incurs much more memory and computation costs than convolution, especially when it comes to natural images . For example, Pyramid Vision Transformer introduced a progressive shrinking pyramid to reduce the sequence length of the Transformer with the increase of network depth, and adopted spatial-reduction attention .","depth, network, swin-tranformer, pvt, swin-transformer"
12,"Spatial-reduction attention can reduce memory and computation costs to learn high-resolution feature maps . Adopting self-attention within local windows is efficient with linear complexity, but it sacrifices the most significant advantage of Transformers in modeling long-range dependencies.","feature, transformers, attention, high-resolution, maps, spatial-reduction"
13,Glance-and-Gaze Transformer consists of two parallel branches . A Glance branch performs self-attention within adaptively-dilated partitions of input images or feature maps . a merging operation finally re-arranges the points in each partition to their original locations .,"locality, convolutions, self-attention, gg-transformer, of"
15,"Convolution has been the basic unit in deep neural networks for computer vision problems . In addition to studying how to organize convolutional blocks into a network, several variants of the convolution layer have also been proposed . Representative works such as ASPP and PPM enhance CNNs with multi-scale context .","convolution, vision, self-attention, cnn, computer"
16,"ViT was proposed to adapt the Transformer for image recognition by tokenizing and flattening 2D images into sequence of tokens . Since then, many works have been done to improve Transformers, making them more suitable for vision tasks . Type I made efforts to improve ViT design itself .","vit, transformer, vision"
20,"using self-attention on high-resolution features is not affordable in terms of memory and computation cost . Later, Liu et al. proposed a new hierarchical Transformer architecture, named Swin-Transformer . To handle the expensive computation burden, they divided feature maps into several non-overlapped windows","spatial, reduction, attention, spatial-reduction, swin-transformer"
21,"Type I, II methods usually use a large patch size and thus incompatible to work with high-resolution feature map . Type III methods proposed new attention mechanism to handle the extreme memory and computation burden with long sequences . In contrast, GGTransformer proposes a more efficient Transformer block .","feature, mechanism, attention, high-resolution, gg-, transformer, map"
23,"The design of GG-Transformer draws inspiration from how human beings observe the world, which follows the Glance and Gaze mechanism . We note that these behaviors surprisingly match the property of self-attention and convolution, which models long-range dependencies and local context .","self-attention, gg-transformer, self, convolution"
31,"a 2D image is often first tokenized based on non-overlapping image patch grids . In MSA, the relationships between a token and all tokens are computed . Such designs incur a computation complexity quadratic to N:","token, image, patch, msa, grid"
32,"For ViTs that only work on 16x down-sampled feature maps, the computation cost is affordable, since in this scenario N = 14 x 14 = 196 . However, a more general vision scenario with the need of dense prediction based on high-resolution feature maps can easily lead to out-of","feature, imagenet, vision, high-resolution, maps, transformer"
34,"existing solutions often adapt Transformers to high-resolution feature maps by down-sampling the key and value during the attention process . For local-region based methods, though they still have a quadratic complexity and thus may not scale up to a larger input size .","feature, self-attention, attention, high-resolution, maps, limit, process"
37,"We propose Glance attention, which performs self-attention efficiently with a global receptive field . It shares same time complexity as , but directly models long-range dependencies . We name this operation Adaptively-dilated Splitting . For example, a partition contains M x M token","self-attention, dilation, adaptive, rate"
54,"Wir build a hierarchical GG-Transformer with the proposed Glance-and-Gaze branches as shown in Fig. 2. For fair comparison, we follow the same settings as Swin . We set M to be same as the window size in .","size, depth, window, network, gg-transformer, swin-transformer"
55,"All GG-Transformers consists of 4 hierarchical stages, which correspond to feature maps with down-sampling ratio 4, 8, 16, 32 . The first patch embedding layer projects input to a feature map with channel C = 96.","spatial, size, feature, gg-transformer, map"
60,AdamW optimizer for 300 epochs with cosine learning rate decay . Initial learning rate starts at 0.001 and weight decay is 0.05 .,"optimizer, adamw, rand-augment, rand-augmentation"
61,"Table 1 shows that GG-Transformer achieve better accuracy-cost trade-off compared to other models . Furthermore, our model consistently brings an improvement to baseline, with a consistent improvement of 0.8% and 0.2% .","model, trade-off, gg-transformer, light-weight, swin-transformer"
67,"We follow and use MMSegmentation to implement all related experiments . We use AdamW with a learning rate starting at 6 x 10-5 weight decay of , 0.01, batch size of 16, crop size of 512 x 512. The learning rate schedule contains a warmup of 1500 iterations","learning, rate, schedule, mmsegmentation, decay"
68,"GG-Transformer achieves 46.4% mIoU with single-scale testing . This surpasses ResNet50, PVT-Small, SwinT's multi-scale tests results .","gg-transformer, single-scale, multi-scale, testing"
77,"GG-T achieves 44.1 box AP and 39.9 mask AP, which surpasses both CNNs and other ViTs with a similar model size and computation costs . compared with the state-of-the-art Swin-Transformer, GG Transformer achieves better performance","gg-transformer, performance, swin-transformer, superior"
80,Kernel Choice of Gaze Branch is based on fixed or adaptive mechanism . Using a larger kernel leads to a non-significant improvement .,"kernel, branch, sizes, choice, gaze"
82,"Swin-T serves as the baseline for all variants, which achieves 78.50% top-1 accuracy on ImageNet validation set . We replace the W&SW-MSA with MSA for all blocks in stage 3 and 4 . This leads to a 1.29% performance improvement .","imagenet, window, attention, shifted, swin-t"
92,"Besides using depthwise convolution, another natural choice is to adopt self-attention . To ensure a fair comparison, we use two consecutive Transformer blocks where one is Glance attention and another is Gaze attention .","branch, depthwise, convolution, gaze, window, self-attention, attention, local"
93,"Glance or Gaze branch alone is far from enough, while only a combination of both can lead to a performance gain . Glance and Gaze branches miss important local or global cues which can be compensated by each other .","performance, branch, gain, alone, gaze"
94,"We replace MSA with GG-MSA for two DeiT variants . We show that, although MSA is an efficient alternative to MSA, it can also lead to a performance gain .","vit, gg-transformer, architec"
97,"Firstly, over-fitting is a common problem in Vision Transformers and can be alleviated by large-scale pretraining or strong augmentations and regularization . This problem is more serious for stronger models and in the tasks with relatively small dataset .","over-fitting, modeling, stronger, vision, transformer"
101,"GG-Transformer, inspired by how human beings learn from the world, is equipped with parallel and complementary Glance branch and Gaze branch . The two branches can specialize in their tasks and collaborate with each other, which leads to a much more efficient ViT design for vision tasks . Experiments on various architecture","gg-transformer, vision, tasks"
103,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154-6162, 2018. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. Transunet: Transformers make strong encoders for","image, transformers, vision, recognition, computer"
105,"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3464-3473, 2019. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646-661. Spring","vision, design, network, transformer, computer"
107,"Xiao Zheng, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1007610085, 2020. Hengshuang Zhao, Jianping Shi, ","image, transformers, vision, recognition, ade20k, dataset"
