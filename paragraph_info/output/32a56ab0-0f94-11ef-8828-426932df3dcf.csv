element_idx,keywords,summarized_text
3,"q-learning, dqn, values, algorithm","In this paper, we answer all these questions affirmatively . In particular, we first show that the recent DQN algorithm suffers from substantial overestimations in some games in the Atari 2600 domain ."
4,"q-learning, q-le",Q-learning is one of the most popular reinforcement learning algorithms . The goal of reinforcement learning is to learn good policies for sequential decision problems . It is known to sometimes learn unrealistically high action values .
5,"estimates, value, overestimation, error, approximation, noise","In previous work, overestimations have been attributed to insufficiently flexible function approximation and noise . Overestimation can occur when the action values are inaccurate . Of course, imprecise values are the norm during learning ."
6,"estimates, value, optimism",overoptimistic value estimates are not necessarily a problem in and of themselves . if all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse .
9,"deep, asymptotic, neural, al-, dqn, low, network, gorithm, error, approximation","DQN combines Q-learning with a flexible deep neural network . It was tested on deterministic Atari 2600 games, reaching human-level performance . In some ways, this setting is a best-case scenario for Q-Learning ."
10,"double, estimates, algorithm, value, dqn, domain, atari-learning, atari",Double Q-learning algorithm was first proposed in tabular setting . We then show that this algorithm leads to much higher scores on several games . This demonstrates that overestimations of DQN were indeed leading to poorer policies .
16,"learning, q-learning, action, ference, values, dif-, temporal, optimal","Q-learning is a form of temporal difference learning . Most interesting problems are too large to learn all action values separately . Instead, we can learn a parameterized value function Q ."
20,"target, deep, q, dqn, network","A deep Q network is a multi-layered neural network that outputs a vector of action values Q, where 0 are the parameters of the network . The target network, with parameters 0- is the same as the online network ex, cept that its parameters are copied every T steps from the network, SO that then "
24,"q-learning, value, functions","In the original Double Q-learning algorithm, two value functions are learned by assigning each experience randomly to update one of the two values functions . For each update, one set of weights is used to determine greedy policy and the other to determine its value ."
27,"of, action, the, online, value, argmax, weights","the selection of the action, in the argmax, is still due to the online weights 0t . This means that we are still estimating the value of the greedy policy according to the current values . The second set of weights can be updated symmetrically by switching the roles of 0 and 0"
29,"q-learning, q-le","Q-learning's overestimations were first investigated by Thrun and Schwartz . They showed that if the action values contain random errors uniformly distributed in an interval then each target is overestimatated up to YE m+1, where m is the number of actions . In addition, "
30,"approximation, function","estimation errors of any kind can induce upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity or any other source. This is important, because in practice any method will incur some inaccuracies during learning ."
32,"q, qt(s, qt","Qt be arbitrary value estimates that are on the whole unbiased in the sense that a - V*) = 0, but that are not all correct . Under these conditions, maxa Qt  V* + V m-1. This lower bound on the absolute error of the Double Q-learning estimate is zero"
40,"approximation, space, function, state","the true optimal action values depend only on state SO that in each state all actions have the same true value . The sampled states are spaced further apart near the left side of the left plots . In many ways this is a typical learning setting, where at each point in time we only have limited data."
41,"state, action, maximum, value, truth","The middle column of plots in Figure 2 shows estimated action value functions for all 10 actions . The true value function is the same for all actions, but the approximations differ because we have supplied different sets of sampled states . 1 The maximum is often higher than the ground truth shown in purple on the left ."
44,"bottom, true, value, rows, approximation, function","The difference between the top and middle rows is the true value function . The difference is the flexibility of the function approximation . In the left-middle plot, the estimates are even incorrect ."
45,"bottom, row, overestimation",In contrast to van Hasselt we did not rely on inflexible function approximation with irreducible asymptotic errors . The bottom row shows that a function that is flexible enough to cover all samples leads to high overestimations .
46,"estimates, bootstrapping, value, overestimation",overestimations occur even when assuming we have samples of the true action value at certain states . The value estimates can further deteriorate if we bootstrap off of action values that are already overoptimistic . Overestimation combined with bootstrapping then has the pernicious effect of propagating the wrong
47,"optimal, quality, policy, optimism","the overestimations discussed here occur only after updating, resulting in overoptimism in the face of apparent certainty . This was already observed by Thrun and Schwartz ."
52,"target, selection, action, dqn, network","The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation . The target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks . We therefore propose to evaluate the greedy policy according"
59,"visuals, game, algorithm","goal is for a single algorithm, with a fixed set of hyperparameters, to learn to play each of the games separately from interaction given only the screen pixels as input . Good solutions must therefore rely heavily on the learning algorithm - it is not practically feasible to overfit the domain by relying only on tuning"
60,"neural, convolutional, network, architecture",The network architecture is a convolutional neural network with 3 convolution layers and a fully-connected hidden layer . The network takes the last four frames as input and outputs the action value of each action .
62,"curves, dqn, learning, orange",DQN is consistently and sometimes vastly overoptimistic about the value of the current greedy policy . Double and Double were both trained under the exact conditions described by Mnih et al.
65,"double, averaged, dqn, values, ground, truth",The ground truth averaged values are obtained by running the best learned policies for several episodes . Without overestimations we would expect these quantities to match up . The learning curves for Double DQN are much closer to the blue straight line representing the true value of the final policy .
66,"imation, bottom, estimates, plot, dqn, value, approx-, function",DQN is highly unstable on the games Asterix and Wizard of Wor . Notice the log scale for the values on the y-axis . The bottom two plots shows the corresponding scores for these two games . This indicates that the overestimations are harming the quality .
71,"behavior, dqn, policy, optimal, quality",Overoptimism does not always adversely affect the quality of the learned policy . Double DQN achieves optimal behavior in Pong despite slightly overestimating the policy value . reducing overestimations can significantly benefit the stability of learning.
75,"dqn, emulator, time, overestimation","the learned policies are evaluated for 5 mins of emulator time with an 6greedy policy where E = 0.05 . The scores are averaged over 100 episodes . Double DQN is the target, using y,DoubleDQN ."
81,"lutions, so-","In deterministic games with a unique starting point the learner could potentially learn to remember sequences of actions without much need to generalize . By testing agents from various starting points, we can test whether the found SOlutions generalize well ."
83,"hyperparameters, dqn, double, tu","tuning is appropriate because the hyperparameters were tuned for DQN, which is a different algorithm . We increased the number of frames between each two copies of the target network from 10,000 to 30,000 ."
87,"al., reports, double, mnih, 2, table, et, dqn","Gorila DQN obtained a median of 78% and a mean of 259% . On several games the improvements are striking, in some cases bringing scores much closer to the average . Detailed results, plus results for an additional 8 games, are available in Figure 4 ."
92,"double, dqn, overoptimism, games, atari","This paper has five contributions . First, we have shown why Q-learning can be overoptimistic in large-scale problems . by analyzing the value estimates on Atari games, we show that these overestimations are more common and severe in practice ."
96,"problem, bandit, agents, policies, multiarmed, reactive, index","P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem . Machine learning, 47:235256, 2002 . L. Baird. Residual algorithms: Reinforcement learning with function approximation ."
97,"dynamic, learning, difference, temporal, programming, approximation, function","H. R. Maei. Gradient temporal-difference learning algorithms . In Deep Learning Workshop, ICML, 2015. M. Riedmiller. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method . Machine learning, 3:9-44, 1988"
99,"q, qt(s, qt","Qt be arbitrary value estimates that are on the whole unbiased in the sense that a - V*) = 0, but that are not all zero . Under these conditions, C This lower bound on the absolute error of the Double Q-learning estimate is zero."
100,"a, action, v*(s), ea, qt(s, each","Suppose that there exists a setting of Ea such that maxa Ea  V m-1 . Let   be the set of positive E of size n, and  E, a set of strictly negative 6 of size M - n . If n"
115,"rmsprop, convolution, layer, linear, projects, network",The input to the network is a 84x84x4 tensor containing a gray-scale version of the last four frames . The first convolution layer convolves the input with 32 filters of size 8 . the second layer has 64 layers of size 4 .
117,"exploration, learning, policy, updates, network, process","In all experiments, the discount was set to 2 = 0.99, and the learning rate to a = 0.00025. The number of steps between target network updates was T = 10, 000 . The memory gets sampled to update the network every 4 steps ."
120,"game, random, human, dqn",Game Random Human DQN Alien 227.80 6875.40 3069.33 2907.30 Amidar 5.80 1675.80 739.50 702.10 Assault 222.40 1496.40 3358.63 5022.90 Asterix 210.00 8503.30 6011.
123,"dqn, game",Game DQN Alien 42.75 % 40.31 % Amidar 43.93 % 41.69 % Assault 246.17 % 376.81 % Asterix 69.96 % 180.15 % Atlantis 451.85 % 320.85
124,"game, random, human, dqn",Asterix 164.50 7536.00 124.5 5285.0 16837.0 Asteroids 36517.30 697.1 1219.0 1193.2 Atlantis 13463.00 26575.00 76108.0 260556.0 319688.0 Bank Heist 21.70 644.50 176.3 4
126,"dqn, game",Asteroids -0.49% 0.98% 0.90% Atlantis 477.77% 1884.48% 2335.46% Bank Heist 24.82% 71.95% 138.78% Battle Zone 47.51% 73.57% 71.87% Beam Rider 57.24% 60.20% 116.70% Berzerk
