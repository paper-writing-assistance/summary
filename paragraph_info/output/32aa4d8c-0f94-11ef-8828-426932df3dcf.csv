element_idx,keywords,summarized_text
4,"judging, translation, automatic, human, machine",Human evaluations can take months to finish and involve human labor that can not be reused . We propose a method of automatic machine translation evaluation that correlates highly with human evaluation .
7,"translation, mt, evaluation, human, machine","Human evaluations of machine translation weigh many aspects of translation, including adequacy, fidelity , and fluency of the translation . A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder . For the most part, these various human evaluation approaches are quite expensive . Moreover, they"
14,"metric, word, rate, error, closeness",We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community . The main idea is to use a weighted average of variable length phrase matches against the reference translations . We have selected a promising baseline metric from this family .
24,"candidate, words, 2, 1, reference","Candidate 1 shares many words and phrases with these three reference translations . We will shortly quantify this notion of sharing in Section 2.1 . Candidates 2 exhibit far fewer matches, and their extent is less ."
29,"candidate, translation, unigram, precision","MT systems can overgenerate ""reasonable"" words, resulting in improbable, but high-precision translations . To compute precision, one simply counts up the number of candidate translation words which occur in any reference translation ."
35,"candidate, adequacy, modified, precision, n-gram, quality","All candidate n-gram counts are clipped by their corresponding reference maximum value, summed, and divided by the total number of candidates ngrams . In Example 1, Candidate 1 achieves a modified bigram precision of 10/17 ."
37,"system, mt, test, precision, multi-sentence, n-gram, set","We compute modified n-gram precision on a multi-sentence test set . A source sentence may translate to many target sentences . We use terminology and refer to target sentences as a ""sentence"""
38,"translator, human, judgment, asia, east","4BLEU only needs to match human judgment when averaged over a test corpus . a system which produces the fluent phrase ""East Asian economy"" is penalized heavily on the longer n-gram precisions ."
43,"translation, mt, strong, signal, quality, differentiating, machine",the difference becomes stronger as we go from unigram precision to 4-gram precision . It appears that any single n-gram precision score can distinguish between a good translation and a bad translation.
44,"proficiency, english, native, translation",We obtained human translations of the same documents by a native English speaker . We also obtained machine translations by three commercial systems . The average modified n-gram precision results are shown in Figure 2.
49,"(machine/system-3), (human-2), h2, s3, ranking, signal","H2 is better than H1 , and there is a big drop in quality between H1 and S3 . This is the same rank order assigned to these ""systems"" by humans ."
51,"modified, precision, trigram, precisions, n-gram",a weighted linear average of the modified precisions resulted in encouraging results for the 5 systems . The modified n-gram precision decays roughly exponentially with n .
57,"candidate, translation, precision, n-gram, penalized","N-gram precision penalizes spurious words in the candidate that do not appear in any of the reference translations . Additionally, modified precision is penalized if a word occurs more frequently in a candidate translation than its maximum reference count ."
64,"recall, translation, bleu, source, word","BLEU considers multiple reference translations, each of which may use a different word choice to translate the same source word . a good candidate translation will only use one of these possible choices ."
67,"synonymous, candidate, words, reference, translations","the first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate . naive recall computed over the set of all references is not a good measure ."
69,"brevity, candidate, multiplicative, penalty, translation, length, reference","a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order . Note that neither this brevity penalty nor the modified n-gram precision length effect directly considers the source length ."
71,"brevity, sentence, penalty","if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly . Instead, we compute the penalty over the entire corpus to allow some freedom at the sentence level ."
81,"metric, ranges, translation, bleu, reference","The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation . For this reason, even a human translator will not necessarily score 1 . It is important to note that the more reference translations per sentence there are, the higher"
88,"system, metric, bleu, variances, each",We divided the test corpus into 20 blocks of 25 sentences each . We computed the BLEU metric on these blocks individually . The t-statistic compares each system with its left neighbor in the table .
89,"metric, bleu, test, size, block, sizeable, set",Table 1 is the BLEU metric on aggregates of 500 sentences . The means in Table 2 are averages of the metric . These two sets of results are close for each system .
90,"test, single-reference, corpus","We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories . In this way, we ensure a degree of stylistic variation . The systems maintain the same rank order as with multiple references."
93,"translation, human, judgment, chinese","human judges judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus . We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chinese source and English translations . All judges used this same webpage and saw the sentence pairs in"
96,"system, consecutive, score, opinion, mean, difference","Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean . 7 The human H1 is much better than the best system, though a bit worse than human H2 ."
104,"monolin-gual, group, bleu, score, regression, linear",Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems . The high correlation coefficient of 0.99 indicates that BLUE tracks human judgment well . Particularly interesting is how well BL EU distinguishes between S2 and S
117,"group, bleu, translator, score, human, monolingual","BLEU, monolingual group, and bilingual group scores for the 5 systems were normalized by their corresponding range . The normalized scores are shown in Figure 7 . Of particular interest is the accuracy of the estimate of the small difference between S2 and S3 . This figure also highlights the relatively large gap between human translator"
122,"cycle, translation, mt, bleu, r&d, human, judgment",BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas . Our belief is reinforced by a recent statistical analysis of BLUE's correlation with human judgment for translation into English from four quite different languages representing 3 different languages .
