element_idx,keywords,summarized_text
4,"guage, of, g-eval, lan-, generation, natural, texts, quality","Conventional reference-based metrics, such as BLEU and ROUGE, have relatively low correlation with human judgments . Recent studies suggest using large language models as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references . However, these LLM-based e"
6,"generation, natural, systems, language, nlg","Evaluating the quality of natural language generation systems is a challenging problem even when large language models can generate high-quality and diverse texts that are often indistinguishable from human-written texts . traditional automatic metrics, such as BLEU , ROUGE , and METEOR , are widely used for"
9,"evaluators, generation, reference-free, probability, nlg","Recent studies propose directly using LLMs as reference-free NLG evaluators . The idea is to use the LLM to score the candidate output based on its generation probability without any reference target . However, the validity and reliability of using these LLM-based evalueators have not been systematically investigated "
10,"corre-lation, g-eval, generation, dialogue, llm","In this paper, we propose G-EVAL, a framework of using LLMs with chain-of-thoughts to evaluate the quality of generated texts in a form-filling paradigm . We use the prompt along with the generated CoT to evaluate NLG outputs . The evaluator output is"
15,"metrics, reference-based, human, llm-based, judgments, quality","LLM-based metrics generally outperform reference-based and reference-free baseline metrics in terms of correlation with human quality judgments . LLM metrics are sensitive to instructions and prompts, and chain-of-thought can improve the performance of LLM evaluators by providing more context and guidance ."
17,"g-eval, evaluation, prompt-based, evaluator",G-EVAL is a prompt-based evaluator with three main components . The prompt contains the definition of the evaluation task and the desired evaluation criteria .
25,"steps, text, generation, evaluation, coherence","The chain-of-thoughts is a sequence of intermediate representations that are generated by the LLM during the text generation process . For evaluation tasks, some criteria need a more detailed evaluation instruction beyond the simple definition, and it is time-consuming to manually design such evaluation steps . The CoT can provide more context"
30,"of, tasks, the, evaluation, scores, low, llm, variance","LLMs usually only output integer scores, even when the prompt explicitly requests decimal values . This leads to many ties in evaluation scores which do not capture the subtle difference between generated texts ."
56,"spearman, kendall-tau, summary, correlation, quality",We adopt the same approach as Zhong et al. to evaluate different summarization metrics using summary-level Spearman and Kendall-Tau correlation . These metrics perform poorly on most dimensions . The second part shows the results of metrics that use neural networks to learn from human ratings of summary quality .
57,"benchmark, summeval, gptscore",G-EVAL surpasses all previous state-of-the-art evaluators on SummEval benchmark . GPTScore also uses GPTs for evaluating summarization texts .
63,"context, summarization, aspect, input, consistency, nlg",QAGS meta-evaluation benchmark includes two different summarization datasets . CNN/DailyMail and XSum Table 3 shows that BARTScore performs well on the more extractive subset .
66,"human-written, g-eval, summaries, llm, texts","G-EVAL may prefer LLM-based outputs, rather than the high-quality human-written texts . To investigate this issue, we conduct an experiment on the summarization task . We compare the evaluation scores of the LLM generated and the human written summaries ."
70,"g-eval-4, human-written, summaries","the dataset can be divided in three categories . We use GEVAL-4 to evaluate the summaries in each category, 2 and compare the averaged scores ."
71,"g-eval-4, human-written, summaries",G-EVAL-4 assigns higher scores to humanwritten summaries when human judges also prefer human-written Summaries . We propose two potential reasons for this phenomenon .
72,"inter-annotator, summaries, llm-generated, agreement","NLG outputs from high-quality systems are in natural difficult to evaluate . authors found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07 ."
79,"control, chain-of-thoughts","Table 1 shows that G-EVAL-4 with CoT has higher correlation . This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help explain the evaluation process and results ."
83,"g-eval-4, sem","We believe this is related to the calculation of Kendall-Tau correlation . Direct scoring without probabilities can lead to many ties, which are not counted as either concordant or discordant . On the other hand, probability normalization can obtain more fine-grained, continuous scores ."
84,"g-eval, summeval, qags, benchmark","The Effect of Model Size We compare the performance of G-EVAL with different model sizes on the SummEval and QAGS benchmarks . Table 1 and Table 3 show that G- EVAL-4 has higher correlation than G-eVAL-3.5 on most dimensions and datasets, except for engagingness and groundedness on"
87,"recall-oriented, rouge, summarization, metric, summary, reference, nlg",ROUGE is a recall-oriented metric for summarization evaluation . It measures the n-gram overlap between a generated summary and a set of reference summaries . This metrics fail to measure content quality or capture syntactic errors .
88,"multi-sentence, nlg, texts, models",WMD measures the distance between two texts based on embeddings . propose a metric that evaluates multi-sentence texts by computing the similarity between the generated text and the reference text .
91,"qa, evaluator, quality, text","UniEval is a unified evaluator that can evaluate different aspects of text generation as QA tasks . By changing the question format, it can handle different evaluation tasks."
92,"evaluators, gptscore, llm-based",LLM-based Evaluators Fu et al. propose GPTScore . It assumes a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context .
94,"text, g-eval, sum, generation, evaluation, dialogue, texts, nlg, marization","In this paper, we propose G-EVAL, a framework of using LLM with chain-of-thoughts to evaluate the quality of generated texts . We conduct extensive experiments on two NLG tasks, text summarization and dialogue generation ."
110,"bart, translation, com-prehension","BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension . Association for Computational Linguistics, Online, July 5-10, 2020, pages 7871-7880."
132,"evaluation, information, sources, criteria","Read the news article carefully and identify the main topic and key points . Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest ."
137,"engagingness, engageness, eng",Evaluation Crieteria: Engagingness Is the response dull/interesting? A score ofl means that the response is generic and dull . A Score of 3 means the response could engage you in the conversation .
