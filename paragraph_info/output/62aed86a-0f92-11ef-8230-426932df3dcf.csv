element_idx,summarized_text,keywords
4,"We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks . To address this issue, we develop the CrossShaped Window self-attention mechanism . We provide a mathematical analysis of the effect of the stripe width . LePE naturally supports arbitrary input resolutions ","tasks, vision, cswin, network, common, transformer"
7,"Transformer architectures have recently achieved competitive performances compared to CNN counterparts in various vision tasks . By leveraging the multi-head self-attention mechanism, these vision Transformers demonstrate a high capability in modeling the longrange dependencies .","counterpart, mechanism, full-attention, architecture, transformer, cnn"
8,researchers further proposed halo and shift operations to exchange information through nearby windows . The receptive field is enlarged quite slowly and requires stacking a great number of blocks to achieve global self-attention.,"large, receptive, field, self-attention, shift, attention, operations"
9,"In this paper, we present the Cross-Shaped Window self-attention . The cross-shaped window is shown in Figure 1 . We calculate the stripe width in the horizontal and vertical stripes in parallel . Each stripe obtained by splitting the input feature into stripes equal width .","window, self-attention, mechanisms, cross-shaped"
14,CSWin self-attention mechanism is calculated in parallel . We split the multi-heads into parallel groups . This parallel strategy introduces no extra computation cost while enlarging the area .,"mechanism, cswin, self-attention, transformer, block"
15,"Based on the CSWin self-attention mechanism, we propose a new vision Transformer architecture named ""CSWin Transformer"" This architecture provides significantly stronger modeling power while limiting computation cost . We introduce an effective positional encoding, Locally-enhanced Positional Encoding .","cswin, transformer, vision"
17,"former variants significantly outperform previous stateof-the-art vision Transformers . For example, our base variant CSWin-B achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task,","swin, cswin, transformer, state-of-the-art"
19,"ViT demonstrates that pure Transformer-based architectures can also achieve very competitive results, indicating the potential of handling the vision tasks and natural language processing tasks under a unified framework . Rather than concentrating on one special task, some recent works try to design a general vision Transformer backbone for general-purpose vision tasks","natural, language, transformers, vision, processing, cnn"
23,"axial self-attention and criss-cross attention propose calculating attention within stripe windows along horizontal or/and vertical axis . They are the most related works with our CSWin, which could be seen as much general and efficient format of these previous works .","axial, attention, long, sequence"
24,"positional encoding is widely used in Transformers to add such positional information back . APE and RPE are often defined as the sinusoidal functions of a series of frequencies or the learnable parameters, which are designed for a specific input size and are not friendly to varying input resolutions . Then the","input, feature, control, encoding, self-attention, positional"
28,"The overall architecture of CSWin Transformer is illustrated in Figure 2. For an input image with size of H x W x3, we follow and leverage the overlapped convolutional token embedding . The whole network consists of four stages . A convolution layer is used between two adjacent stages to reduce the number of token","cross-shaped, branch, cswin, window, self-attention, transformer, block"
30,"to alleviate this issue, existing works suggest to perform self-attention in a local attention window and apply halo or shifted window to enlarge the receptive filed . however, the token within each Transformer block still has limited attention area and requires stacking more blocks .","window, mechanism, self-attention, attention"
31,APE and CPE introduce positional information before feeding into the Transformer blocks . RPE and our LePE operate in each Transformer block . We only draw the self-attention part to represent the Transformer block for simplicity .,"lepe, ape, transformer, block"
34,"horizontal stripes self-attention, X is evenly partitioned into non-overlapping horizontal stripes of equal width sw . Each of them contains X x W tokens . This is the stripe width and can be adjusted to balance learning capacity and computation complexity .","stripes, horizontal, self-attention"
37,"WO E RCxC is the commonly used projection matrix that projects the self-attention results into the target output dimension . In other words, the attention area of each token within one Transformer block is enlarged via multi-head grouping .","rcxc, wo, multi-head, e, groups, self-attention"
39,"adjusting sw provides the flexibility to enlarge the attention area of each token in later stages in an efficient way . To make the intermediate feature map size divisible . by default, we emphasise a set of 1, 2, 7, 7 for four stages by default . Locally-Enhanced Positional Encoding","calculation, encoding, self-attention, attention, locally-enhanced, positional"
50,"CSWin Transformer is designed by changing the base channel dimension C and the block number of each stage . The head number of the four stages is set as 2, 4, 8, 16 in the first three variants and 6, 12, 24, 48 in the last variant respectively .","cswin-l, (large), cswin-t, cswin, (tiny), transformer"
52,"To show the effectiveness of CSWin Transformer as a general vision backbone, we conduct experiments on ImageNet1K classification, COCO object detection, and ADE20K semantic segmentation .","eral, vision, gen-, cswin, backbone, transformer"
58,"We use the AdamW optimizer with weight decay of 0.05 for CSWin-T/S . The default batch size and initial learning rate are set to 1024 and 0.001, and the cosine learning rate scheduler with 20 epochs linear warm-up is used .","optimizer, size, training, adamw, batch, default, strategy"
63,"CSWin-T achieves 82.7% Top-1 accuracy with only 4.3G FLOPs . For the small and base model setting, our CSwin-S and CSWIN-B also achieve the best performance .","cswin-s, transformers, cswin-t, cswin, csw, cswin-b"
65,Models are trained for 90 epochs with the input size of 224 x 224. We use the AdamW optimizer with weight decay of 0.1 for CSWin-B and 0.2 . The default batch size and initial learning rate are set to 2048 and 0.001.,"cswin, transformer"
75,"Framework with ""1x"" and ""3 x +MS"" schedule . It shows that our CSWin Transformer variants clearly outperforms Swin-T . We also achieve similar performance gain on small and base configurations .","training, cswin, multiscale, transformer, swin-t"
78,"In Table 6, we report the results of different methods in terms of mIoU and Multi-scale tested . For fair comparison, we train Semantic FPN 80k iterations with batch size as 16 and Upernet 160k Iterations .","upernet, cswin, transformer"
81,"CSWin Transformers outperform previous state-of-the-arts under different configurations . We achieve +6.7, +4.0, +3.9 higher mIOU than the Swin counterparts with the Semantic FPN framework . The performance gain is very promising and demonstrates the potential of vision Transformers","cswin, msp, mlou, mis"
83,"In most cases, the speed of our model is only slightly slower than Swin . For downstream tasks, we report the FPS of Cascade Mask R-CNN for object detection on COCO and UperNet .","swin, cswin, swin-s, swin-t"
85,"To better understand CSWin Transformers, we compare each key component with the previous works under a completely fair setting that we use the same architecture and hyper-parameter for the following experiments . For time consideration, we use Mask R-CNN with 1x schedule as the default setting for detection and instance segmentation evaluation .","segmentation, cswin, transformers, evaluation"
92,"In Table.8, we find that the ""parallel multi-head grouping"" is efficient and effective . When we replace the Parallel manner with Sequential, the performance of CSWin degrades on all tasks . Our sw = 1 performs slightly better than Axial on ImageNet .","tasks, multi-head, parallel, cswin, downstream, grouping"
94,"Attention Mechanism Comparison . In detail, we use the Swin-T as backbone and only change the self-attention mechanism . The results are reported in Table 9 .","cswin, self-attention, swin-t"
100,"In Table 10, we compare our LePE with other recent positional encoding mechanisms for image classification, object detection and image segmentation . Besides, we test the variants without positional encode and CPE*, which is obtained by applying CPE before every Transformer block .","encoding, image, segmentation, cswin-t, positional"
103,the mathematical analysis allows us to increase the stripe width along the network depth . We further introduce locally-enhanced positional encoding into CSWin Transformer for downstream tasks .,"cswin, transformer, vision, tasks"
105,"Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer . arXiv preprint arxiv:2004.05150, 2020 . 3 Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn","transformer, vision, long-scale, long-document"
106,"Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, xiaolin Wei, and Chunhua Shen . Do we really need explicit position encodings for vision transformers? arXiv preprint arxiv:2102.","mobile, transformers, vision"
107,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017. 2 Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on Computer Vision, pages 646-661. Springer","vision, design, network, transformer, computer"
108,"Irwan Bello, Ashish Vaswani, Anselm Levskaya, and Jonathon Shlens. Standalone self-attention with routing transformers. Transaction of the Association for Computational Linguistics, 9:53-68, 2021. 3, 8 Aurko Roy,","sequence, visual, long-range, vision, backbones, computer"
110,"ArXiv preprint arxiv:2103.14031, 2021 . 2, 5, 6, 7, 12, 13 Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen . Aggregated residual","transformer, vision, computer"
111,"Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang . Random erasing data augmentation, 2017. 12 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Tor","transformers, ade20k, dataset"
114,ImageNet-1K Classification follows the training strategy in DeiT . We use the AdamW optimizer with weight decay of 0.05 for CSWin-T/S . The default batch size is set to 2048 and 2e - 3 respectively .,"optimizer, modelling, size, depth, stochastic, learning, rate, adamw, batch, dynamic, default"
117,"Mask R-CNN is based on the implementation from mmdetection . For 1x schedule, we train the model with single-scale input for 12 epochs . We use Adam W optimizer with a learning rate of 0.0001, weight decay of 0.05 and batch size of 16 ","cswin-s, mask, imagenet-1k, cascade, r-cnn"
118,UperNet and Semantic FPN are semantic segmentation frameworks . We follow the setting in and use AdamW optimizer with initial learning rate 6e-5 weight decay of 0.01 and batch size of 16 for 160K iterations .,"optimizer, upernet, segmentation, adamw, semantic"
119,"augmentation setting in mmsegmentation including random horizontal flipping, random re-scaling and random photo-metric distortion . All the models are trained with input size 512 x 512. The stochastic depth is set to 0.2, 0.4, 0.6 for CSWin-T","photometric, augmentation, distortion, flipping, random, setting, horizontal"
