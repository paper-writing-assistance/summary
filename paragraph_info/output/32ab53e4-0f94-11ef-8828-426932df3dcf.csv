element_idx,keywords,summarized_text
3,"spatial, natural, reasoning, state-of-the-art, language, models","Inferring spatial relations in natural language is a crucial ability an intelligent system should possess . The bAbI dataset tries to capture tasks relevant to this domain . Most importantly, they are limited in the number of reasoning steps required to solve them ."
5,"navigation, tasks, systems, conversational","Neural networks have been successful in a wide array of perceptual tasks, but it is often stated that they are incapable of solving tasks that require higher-level reasoning . Since spatial reasoning is ubiquitous in many scenarios such as autonomous navigation , situated dialog , and robotic manipulation, we need to make them able to understand and"
7,"system, spatial, inference, conversational, relation","Earlier works focused on spatial instruction understanding in a synthetic environment or in simulated world with spatial information annotation in texts . For example, imagine a user asking to a conversational system to recognize the location of an entity based on the description of other entities in the scene ."
8,"spatial, ability, model, reasoning, state-of-the-art, babi","BAbI contains 20 synthetic question answering tasks to test a variety of reasoning abilities in texts, like deduction, co-reference, and counting . Positional reasoning task and path finding task are designed to evaluate models' spatial reasoning ability ."
9,"learning, sets, test, babi, models","We find four major issues with bAbI's tasks 17 and 19: There is a data leakage between the train and test sets; that is, most of the test set samples appear in the training set . Named entities are fixed and only four relations are considered . Each text sample always contains the same four named"
11,"babi, multi-hop, dataset, reasoning","Palm, Paquet, and Winther pointed out that multi-hop reasoning is not necessary for the bAbI dataset . It is a synthetic dataset with a limited diversity of spatial relation descriptions ."
13,"templates, relations, crowdsourced, natural, description, ways",The StepGame dataset is based on crowdsourced descriptions of 8 potential spatial relations between 2 entities . These descriptions are then used as templates when generating the dataset . This was done to ensure that the crowdsourced templates cover most of the natural ways relations between two entities can be described in text .
14,"memory, multi-hop, recurrent, reasoning, architecture, tp-mann","The TP-MANN architecture is based on tensor product representations that are used in a recurrent memory module to store, update or delete the relation information . This architecture provides three key benefits: it enables the model to make inferences based upon the stored memory; it allows multi-hop"
17,"navigation, spatial, reasoning, 3d","The role of language in spatial reasoning has been investigated since the 1980s . reasoning about spatial relations has been studied in several contexts such as, 2D and 3D navigation ."
19,"babi, multi-hop, reasoning, spartqa",The bAbI dataset consists of several QA tasks . Solving these tasks requires logical reasoning steps and cannot be solved by simply word matching . Task 17 is about positional reasoning while task 19 is about path finding .
20,"qa, multi-hop, dataset","Commonly used multi-hop QA datasets are HotpotQA , Complex WebQuestions , and QAngaroo . The proposed StepGame dataset focuses on spatial reasoning ."
21,"bert, networks, neural, graph, reasoning, models","There are three types of reasoning models: memory-augmented neural networks, graph neural networks and transformer-based networks . Works of the first type augment neural networks with external memory, such as End to End Memory Networks , Differential Neural Computer , and Gated End-to-End Memory Network ."
23,"symbolic, information, tpr, structural, natural, language",Tensor Product Representation is a technique for encoding symbolic structural information and modelling symbolic reasoning in vector spaces . TPR has been used for tasks that require deductive reasoning abilities .
24,"third-order, tpr, tpr-rnn, rnn, gradient-based, babi",Schlag and Schmidhuber proposed a gradient-based RNN with third-order TPR . Self-Attentive Associative Memory uses a second-order item memory . STM takes a longer time to converge in practice.
25,"concatenated, stm, stm-rnn, memory",STM and TPR-RNN use an RNN-like architecture where each sentence is stored recurrently . This may result in a long-term dependency problem where necessary information would not interact with each other .
33,"spatial, ability, reasoning, benchmark, dataset","StepGame is a contextual QA dataset, where the system is required to interpret a story about several entities expressed in natural language . This reasoning task is trivial for humans, but to equip current NLU models with such a spatial-ability remains a challenge ."
35,"mechanical, image, turk, amazon, task, positional, crowdsourcing, relationship","This crowdsourcing task was performed in multiple runs . In the first run, we provided crowdworkers with an image and two entities . They were asked to describe their positional relation . From the data collected in this round, we manually removed bad answers ."
37,"language, natural, story-question, pair",The story describes a set of k spatial relations between k + 1 entities . The question requests the relative position of two entities among the k+1 ones mentioned in the story. To each story-question pair an answer is associated .
39,"of, entities, sequence","We generate a sequence of entities by sampling a set of k + 1 unique entities from E. Then, for each pair of entities in the sequence, k spatial relations are sampled . Because the sampling is unconstrained, entities can overlap with each other ."
41,"language, natural, k, sentence","From the chain generated in Step 1, we translate the k relations into k sentence descriptions in natural language . Each description is based on a randomly sampled crowdsourced template . These shuffled k sentences are a called a story."
42,"process, generation, entities","This is possible because entities can overlap . Given k relations, k + 1 entities sampled from 3 in any order . 8 possible relations between pairs of entities with 2 ways of describing them ."
43,"dataset, k","StepGame dataset uses 181 = 26 . For k = 1 we have 10,400 possible samples, for k= 2 we have more than 23 million samples . The sample complexity of the problem guarantees that when generating the dataset the probability of leaking samples from the training set to the test set diminishes with the increase"
48,"analysis, test, supporting, set, noise","We generate three kinds of distracting noise: disconnected, irrelevant, and supporting . Examples of all kinds of noise are provided in Figure 2 . The irrelevant noise extends the original chain by branching it out with new entities and relations ."
50,"story, model, question, tp-mann, decoder","TP-MANN model comprises three major components: a question and story encoder, a recurrent memory module, and a relation decoder . The encoder learns to represent entities and relations for each sentence in a story . It also updates the entity-relation pair representations based on"
51,"tpr-like, representations, memory, architecture","We use an example to illustrate the inspiration behind this architecture . A person may experience that when she goes back to her hometown and sees an old tree, her happy childhood memory about playing under that tree might be recalled ."
54,"tree, old, unbinding, module, decoder, vectors",the decoder module unbinds relevant memories given a question via a recurrent mechanism . This allows episodes in memories to interact with each other .
55,"encoder, embedding, question, word","Each sentence Si = is mapped to learnable embeddings . This operation defines S* E Rmxd where each row of S* repre, sends an encoded sentence and a question q ."
59,"rde, xdr, xde, m, e",Recurrent Memory Module uses T recurrent-layers to update memory representation M E Rde xdr xde . existing episodes stored in memory can interact with new inferences to generate new episodes .
60,"pseudo-entities, ps","at each layer t, given the keys Ks, we extract pseudoentities Ps for each sentence in S* In the first layer . The model just converts each sentence as an episode and stores them in it ."
70,"memory, recurrent-layers, recurrent, module, data, leakage",In this section we aim to address the following research questions: What is the degree of data leakage in the datasets? How does our model behave with respect to state-of-the-art NLU models in spatial reasoning tasks? How do these models behave when tested on examples more challenging than those used for training?
72,"stepgame, test, babi, validation, dataset, set","In the following experiments we will use two datasets, the bAbI dataset and the StepGame dataset . For task 17 and task 19 we only use the original train and test splits made of 10 000 samples for the training set and 1 000 for the validation and test sets ."
75,"tpr-rnn, t, rrn, tpr-ut","We compare our model against five baselines: Recurrent Relational Networks . Relational network , TPRRNN , Self-attentive Associative Memory and Universal Transformer ."
77,"order, lexicographical, test, set","To answer RQ1 we have calculated the degree of data leakage present in bAbI and StepGame datasets . For task 19, we counted how many samples in the test set appear also in the training set . The sentences in the story are sorted in lexicographical order ."
86,"appendix, accuracy, test, model, baseline",We report the average performance across k . Our model outperforms all baseline models . It is not surprising that the performance of all five baseline models decreases when k increases .
89,"generalization, task","In Table 3 we present the performance of different models on this generalization task . RN, RRN, UT and SAM fail to generalize to the test sets with higher k values . This demonstrates the better generalization ability of our model ."
91,"layer, test, recurrent, down, break, structure","To answer RQ4, we conduct an analysis of the hyperparameter T, the number of recurrent layers in our model . We train TP-MANN on the StepGame dataset with k between 1 and 5 with number of T between 1 to 6 and report the break down test accuracy for each value of k "
94,"increases, inference, multi-hop, t, cess, pro-",this analysis further corroborates that our recurrent structure supports multi-hop inference . It is worth noting that the number of trainable parameters in our model remains unchanged as T increases.
96,"multi-hop, reasoning, babi, tp-mann, dataset","In this paper, we proposed a new dataset named StepGame that requires a robust multi-hop spatial reasoning ability to be solved . Then, we introduced TP-MANN, a tensor product-based memory-augmented neural network architecture that achieve state-of-the-art performance on both datasets"
100,"analysis, blocks, intelligence, world, artificial, educational, 3d","AAAI Conference on Artificial Intelligence, and 8th AAAI Symposium on Educational Advances . AAAI Press, New Orleans, Louisiana, USA ."
125,"module, reasoning, relational, network","A simple neural network module for relational reasoning . In Guyon, I.; von Luxburg, U.; Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, V. V. N."
131,"technologies, computational, human, language, linguistics","In Walker, M. A.; Ji, H.; and Stent, A., eds., Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies ."
132,"spatial, intelligence, artificial, instruction, aaai-18","Source-Target Inference Models for Spatial Instruction Understanding . In Mcllraith, S. A.; and Weinberger, K. Q., eds., the 30th innovative Applications of Artificial Intelligence ."
