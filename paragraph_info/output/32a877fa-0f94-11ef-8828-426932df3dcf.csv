element_idx,keywords,summarized_text
5,"sequence, modeling, network, topologies, lstm",Long Short-Term Memory networks have obtained strong results on a variety of sequence modeling tasks . TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting semantic relatedness of two sentences .
9,"bag-of-words, sequence, representation, sentence, models","bag-of-words models, sequence models, and tree-structured models construct phrase and sentence representations from its constituent subphrases according to a given syntactic structure ."
10,"nlp, order-sensitive, model, sequential, models","Order-insensitive models are insufficient to fully capture semantics of natural language due to their inability to account for differences in word order or syntactic structure . Tree-structured models are a linguistically attractive option because of their relation to syntic interpretations of sentence structure. In this paper, we work towards"
13,"tasks, memory, sequence, long-term, modeling, prediction, lstm","RNNs with Long Short-Term Memory units have re-emerged as a popular architecture due to their representational power and effectiveness at capturing long-term dependencies . LSTM networks, which we review in Sec. 2, have been successfully applied to a variety of sequence modeling tasks ."
14,"lstm, tree-structured, topologies, network",tree-structured LSTM composes its state from an input vector and the hidden states of arbitrarily many child units . Tree-LSTM can then be considered a special case of the Tree-LTM where each internal node has exactly one child .
15,"pairs, tree-lstm, sentiment, classification, relatedness",Tree-LSTMs outperform existing systems and sequential LSTM baselines on both tasks . Implementations of our models and experiments are available at https : // github  com/treelstm.
18,"state, ht, networks, hidden, neural, recurrent, rnns","Recurrent neural networks are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht . At each time step t, the hidden state is a function of the input vector Xt that the network receives at time t"
25,"lstm, unit, logistic, sigmoid, function","forget gate controls the extent to which the previous memory cell is forgotten . input gate controls how much each unit is updated . The hidden state vector in an LSTM unit is a gated, partial view of the state of the unit's internal memory cell ."
34,"memory, tree-lstm, cell, unit, updates","Tree-LSTM unit contains input and output gates ij and a memory cell and hidden Cj Oj, state hj . The difference between standard LSTM units and Tree-LTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units . Tree"
38,"tree-lstm, vector, input, word","In our applications, each Tree-LSTM unit takes an input vector xj . The input word at each node depends on the tree structure used for the network . In a TreeLSTM over a dependency tree, the leaf nodes take the corresponding word vectors as input."
44,"hk, states, hidden","unit, the input xj, and the hidden states hk of the unit's children . For example, the model can learn parameters W such that the components of the input gate ij have values close to 1 when a semantically important content word is given as input ."
45,"tree-lstm, tree, child-sum, dependency",Dependency Tree-LSTM is a good choice for trees with high branching factor or whose children are desdered . We refer to a Child-Sum Tree LSTM applied to . a dependency tree as a Dependency . Trees with high branchesing factor can be highly variable.
51,"tree, conditioning, tree-lstm, sum, child-, application, phrase, verb, con-stituency",Model to learn fine-grained conditioning on the states of a unit's children . Consider a constituency tree application where the left child corresponds to a noun phrase . Then the Ukf parameters can be trained so that the components of fj1 are close to 0 .
52,"parameterization, forget, gate","In Eq. 10, we define a parameterization of the kth child's forget gate fjk that contains ""off-diagonal"" paramk  l . This parameterizaeter matrices U , tion allows for more flexible control of information propagation from child to parent"
54,"tree-lstms, constituency","Dependency Tree-LSTMs and Constituency Trees are closely related . These architectures are in fact closely related; since we consider only binarized constituency trees, the parameters of the two models are very similar ."
75,"movie, reviews, binary, sentences, classification","There are two subtasks: binary classification of sentences, and fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive . We use the standard train/dev/test splits of 6920/872/1821 for the binary classification subtask and 8544/1101/2210 for the"
82,"dependency, training, tree-lstms, constituency, set",Dependency Tree-LSTMs produce dependency parses3 of each sentence . Each node in a tree is given a sentiment label if its span matches a labeled span .
85,"sic, knowledge, involving, composition, pair, sentence, (sick)",Sentences Involving Compositional Knowledge dataset consisting of 9927 sentence pairs in a 4500/500/4927 train/dev/test split . The sentences are derived from existing image and video description datasets . Each sentence pair is annotated with a relatedness score y E .
93,"word, representations, tors5, vec-, glove",We initialized our word representations using publicly available 300-dimensional Glove vectors5 . For the semantic relatedness task we did not observe significant improvement when the representations were tuned.
94,"performance, dropout, adagrad, gains",Model parameters were regularized with a per-minibatch L2 regularization strength of 10-4 . The sentiment classifier was additionally regularized using dropout .
100,"tree-lstm, classification, subtask, constituency, fine-grained",Constituency Tree-LSTM outperforms existing systems on the fine-grained classification subtask and achieves accuracy comparable to the state-of-theart on the binary subtask . This performance gap is at least partially attributable to the fact that the Dependency Tree LSTMis trained on less data: about
101,"representations, word, vector, glove, classification, subtask, fine-grained",updating the word representations during training yields a significant boost in performance on the fine-grained classification subtask . These gains are to be expected since the Glove vectors used to initialize our word representation were not originally trained to capture sentiment .
105,"dt-rnn, nonlin-earity, model",The mean vector baseline computes sentence representations as a mean of the representations of the constituent words . The SDT-RNN is an extension of the DTRNN that uses a separate transformation for each dependency relation .
106,"ecnu, ec","We compare against four of the topperforming systems6 submitted to the SemEval 2014 semantic relatedness shared task: ECNU , The Meaning Factory , UNAL-NLP , and Illinois-LH ."
111,"supervision, tree-lstm, dependency, compact, structure","Dependency Tree-LSTM models only receive supervision at the root of the tree . tems without any additional feature engineering, with the best results achieved ."
115,"root, query, dependency, model","Dependency Tree-LSTM model exhibits several desirable properties . Note that in the dependency parse of the second query sentence, the word ""ocean"" is the second-furthest word from the root ."
120,"longer, structures, tree, sentences, lstm","In Figs. 3 and 4, we show the relationship between sentence length and performance . Each data point is a mean score over 5 runs, and error bars have been omitted for clarity."
121,"longer, tree-lstm, dependency, sequential, sentences, lstm",Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 . Tree-LSTMs encode semantically-useful structural information in the sentence representations that they compose .
126,"nlp, learning, representations, phrase, sentence, distributed",Pennington et al. have found wide applicability in a variety of NLP tasks . There has been substantial interest in the area of learning distributed phrase and sentence representations .
127,"works, net-, tree-rnn, neural, recursive",Tree-RNNs have been used to parse images of natural scenes . Trees are composed as a function of the vectors corresponding to the children of the node .
132,"deep, of, exploration, google, filtering, and, text, program, (deft), deft, inc.","Stanford University gratefully acknowledges the support of a Natural Language Understanding-focused gift from Google Inc. and the Defense Advanced Research Projects Agency . Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US"
