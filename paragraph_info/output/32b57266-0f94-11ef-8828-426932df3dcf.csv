element_idx,keywords,summarized_text
3,"pre-trained, analysis, generative, intelligence, artificial, models","Generative Artificial Intelligence has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs . This paper proposes a novel evaluation framework, GPTSCORE, which utilizes emergent abilities of generative pre-trained models to score generated texts ."
5,"pre-trained, generative, gpt3, models","The advent of generative pre-trained models, such as GPT3 , has precipitated a shift from analytical AI to generative AI across multiple domains ."
12,"of, fig, text, 1-(a), aspect, evaluation, quality","Existing studies have examined multi-aspect evaluation but have not given adequate attention to the definition of the evaluation aspect . Instead, the evaluation of an aspect is either empirically bound with metric variants or learned by supervised signals . Recently proposed evaluation methods usually necessitate a complicated training procedure or costly manual annotation"
13,"evaluation, gpt-3, multi-aspect, challenges","in this paper, we demonstrated the talent of the super large pre-trained language model in achieving multiaspect, customized, and training-free evaluation ."
19,"pre-trained, likely, generation, model, gptscore, large, probability","""likely"" can be measured by the conditional generation probability . Each evaluation sample will be presented with the evaluated protocol with optionally moderate exemplar samples . a large generative pre-trained model will be used to calculate how likely the text could be generated ."
20,"gpt3-text-davinci-003, generative, pretraining, models",Evaluating texts with generative pre-training models can be more reliable when instructed by the definition of task and aspect . Different evaluation aspects exhibit certain correlations . Combining definitions with other highly correlated aspects can improve evaluation performance . The performance of GPT3-text-davinci-001 is inferior to
25,"metrics, summarization, text, aspect, evaluation, task",where h represents the text to be evaluated . a denotes the evaluation aspect . Function f could be instantiated as a human evaluation process .
27,"metrics, spearman, human, judgment, correlation, auto-, mated",Meta evaluation aims to evaluate the reliability of automated metrics by calculating how well automated scores correlate with human judgment using correlation functions g such as spearman correlation . Spearman correlation measures the monotonic relationship between two variables based on their ranked values .
30,"summarization, text, aggregation, evaluation, strategies","Evaluation strategies define different aggregation methods when we calculate correlation scores . suppose that for each source text Si, i E, there are J system outputs hi,j, where j E . fauto is an automatic scoring , function ."
37,"pre-trained, encoder-only, language, attention, bidirectional, models","Existing pre-trained language models could be classified into three categories . encoder-only models , RoBerta . that encode inputs with bidirectional attention; encoderdecoder models ."
39,"emergent, guage, abilities, long-term, lan-, lan-guage, models",Emergent Ability Recent works progressively reveal a variety of emergent abilities of generative pre-trained language models with appropriate tuning or prompting methods . It's the appearance of these abilities that allows us to re-invent a new way for text evaluation-evaluating from the textual description .
41,"pre-training, gptscore, generative, model","the core idea of GPTSCORE is that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context . In our method, the instruction is composed of the task description d and the aspect definition a. Specifically, suppose that the text to be"
48,"summarization, text, openai, pre-training, naturalinstruction","NaturalInstruction is the main training source for those instruction-based pre-train models . We use prompts from Naturalinstruction to evaluate the fluency of the text summarization task . The final prompt template is ""Generate a fluent and grammat ical summary"""
49,"metrics, reliability, gptscore, automated","Selection of Scoring Dimension GPTSCORE exhibits different variants in terms of diverse choices of texts being calculated . For example, given a generated hypothesis, we can calculate GTPSCORE either based on the source text . In this paper, we will detail this based upon different human judgment datasets in the experiment"
53,"datasets, di-, alogue, generation, evaluation, natural, language, response, aspects","To achieve a comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks . Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset ."
54,"dialoge, evaluation, turn-level, informative, response","Text Summarization is a task of automatically generating informative and factual summary for a given long text . Here, we choose to use the following four datasets, SummEval , REALSumm , NEWSROOM , and QAGS_XSUM . Machine Translation aims to"
57,"flan-t5, gpt2, bartscore, mover's, word, distance, wmd",DynaEval is a unified automatic evaluation framework for dialogue response generation tasks on the turn level and dialogue level . BARTScore+CNN+Para is based on BART fine-tuned on CNNDM and Paraphrase2.0 . Five variants are explored for each framework .
61,"generation, open, dialogue, domain, fed-turn, task","src->hypo variant for aspects INT, ENG, SPC, REL, COR, SEM, UND and FLU of FED-Turn datasets from the open domain dialogue generation task ."
62,"data-to-text, task, bloom, src->hypo","For aspects INF, NAT, and QUA from the data-to-text task, we choose src->hypo . Because the source text of the machine translation is not in the standard text format . In this work, we mainly consider the evaluation of the English text ."
64,"text, multi-dimensional, mqm, gpt3, api, quality, dataset, cost","In this work, we studied 37 datasets according to 22 evaluation aspects . Due to the expensive API cost of GPT3, we randomly extract and construct sub-datasets for meta-evaluation ."
67,"ist, bootstrapping, test, significance","Significance Tests To examine the reliability and validity of the experiment results, we conducted the significance test based on bootstrapping . 4 Our significance test is to check whether the performance of IST is significantly better than VAL ."
76,"variant, models, gpt3, datasets, qxsum",Fig. 3 shows the evaluation results of five GPT3 variant models on four text summarization datasets . QXSUM uses the Pearson correlation and other datasets use the Spearman correlation metric .
78,"gpt3-d01, gpt3-d03, gpt3-c01, model, gpt3-d, gpt3-based","In Tab. 3, the average Spearman score of both the GPT2 and OPT models is better than the vanilla setting by using instruction . The equipment of instruction to the encoder-decoder model of FT5 on NEWSROOM fails to achieve gains ."
82,"translation, gpt3-based, dataset, models, machine",The Spearman correlations for the GPT3-based variants are shown in Fig. 4 . For the full evaluation results of 28 models can be found in Tab. 14 .
84,"gpt3-d01, gpt3-d03, gptscore, gp","In Tab. 4, the average performance of 19 GPTSCORE based evaluators with instruction significantly outperforms vanilla . The combination of instruction and demonstration brings gains . In Fig. 4 the performance of GPT3, GPT2, OPT, and FT5 improves ."
90,"spearman, sparman, datasets, sfres, bagel","We consider the BAGEL and SFRES datasets for the evaluation of data to text task . The average Spearman correlations of the GPT3-based, GPT2-based, OPT-based and FT5-based models are listed in Tab. 5. VAL, IST, and IDM de"
92,"decoder-only, demonstration, introducing, model, gpt3, instruction, dataset, bagel","In Tab. 5, the average performance on the three aspects is significantly improved . The decoder-only model is better at utilizing demonstration to achieve high performance . GPT3 has strong compatibility with unformatted text ."
96,"dialoge, generation, gptscore, dialogue, response","To test if GPTSCORE can generalize to more aspects, we choose the task of dialogue response generation as a testbed . To reduce the computational cost, we focus on GPT3-based metrics since they have achieved superior performance as we observed in the previous experiments ."
101,"generalization, generation, model, dialogue, gpt3-based, stronger","the average Spearman correlation of GPT3d01 is 40.8 on the FED Turn-level dataset, and 5.5 on the dialogue-level . The GPT-3-a01 with 350M parameters achieved comparable performance to FED and DE models on both FED turn-level and dialog-level data ."
105,"gains, demonstration, performance, dataset, mqm-2020","Spearman correlation on the MQM-2020 dataset with different demonstration sample size is shown in Fig. 6 . There is an upper bound on the performance gains from the introduction of the demonstration . For example, when K>4, the performance of ACC is hard to improve further ."
113,"spe, int","""Is this response interesting to the conversation? "" x=1 in Fig. 7- . When at INT combines with ENG, SPE . The best performance of 51.4 is achieved after combined five aspects ."
121,"emergent, multi-faceted, ability, evaluation, gptscore","The proposed framework, GPTSCORE, is studied on multiple pre-trained language models with different structures, including the GPT3 with a model size of 175B . This work opens a new way to audit generative AI by utilizing generative artificial intelligence ."
127,"summarization, text, 2020, empirical, evaluation, emnlp","Webber, B., Cohn, T., He, Y., Liu, P., and Neubig, G. Re-evaluating evaluation in text summarization . doi: 10.18653/v1/2020.emnlp-main.751."
128,"language, corr, models","CoRR, abs/2005.14165, 2020 . Language models are few-shot learners . CoRR is a coRR ."
130,"narang, a., narang-shou, narang-ai","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S ., Schuh"
134,bert,"Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 . doi: 10.18653/v1/n19-1423."
138,"technologies, computational, assessment, human, language, linguistics","A dataset of 1.3 million summaries with diverse extractive strategies . Association for Computational Linguistics, 2018. doi: 10.18653/v1/n18-1065 ."
141,"learning, computer, computational, engineering, language, linguistics","CoNLL 2019, Hong Kong, China, November 3-4, 2019, pp. 44-54. Association for Computational Linguistics, 2019. doi: 10.18653/v1/K19-1005."
142,"jmlr, embedding, and, conference, kusner, word, workshop, proceedings","Kusner, M. J., Sun, Y., Kolkin, N. I., and Weinberger, K. Q. From word embeddings to document distances ."
143,"com-, prehension, translation, acl-main, access","Association for Computational Linguistics, 2020 . doi: 10.18653/v1/2020.acl-main. 703."
144,"dynamic, information, long, acl-long, papers, flow, active","Conversations are not flat: Modeling the dynamic information flow across dialogue utterances . In Zong, C., Xia, F., Li, W., and Navigli, R., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11"
151,"graphical, learning, generation, statistical, active, language, models","ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pp. 1552-1561 ."
153,"sigdial, dialog, 2020, dialogue, interactive","Unsupervised evaluation of interactive dialog with dialogpt . In Pietquin, 0., Muresan, S., Chen, V., Kennington, C., Vandyke, D., Dethlefs, N., Inoue, K., Ekstedt, E."
159,"open-domain, dialogue, acl-main, generation","Association for Computational Linguistics, 2020 . doi: 10.18653/v1/2020.acl-main.333 ."
167,"a, model, open-access, multilingual, language, 176b-parameter",BLOOM: A 176b-parameter open-access multilingual language model . doi: 10.48550/arXiv.2211.05100 .
168,"2021, in, empirical, natural, methods, emnlp, language, processing","Summarization asks for fact-based evaluation . In Moens, M., Huang, X., Specia, L., and Yih, S. W., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing ."
169,"acl-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main-main, acl-main-main, acl-main","Association for Computational Linguistics, 2020 . doi: 10.18653/v1/2020.acl-main.704."
171,"empirical, translation, evaluation, translaction, machine","Automatic machine translation evaluation in many languages via zero-shot paraphrasing . Webber, B., Cohn, T., He, Y., and Liu, y."
176,"system, ural, generation, spoken, dialogue, emnlp, language, nat-","Wen, T., Gasic, M., Mrksic, N., Su, P., Vandyke, D., and Young, S. J. Semantically conditioned lstm-based natural language generation for spoken dialogue systems ."
178,"synthesis, aclanthology, emnlp, 2022, error","Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022."
183,"acl-long-long, acl-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long-long, acl-long","Association for Computational Linguistics, 2021 . doi: 10.18653/v1/2021.acl-long.441."
187,"mover, empirical, tance, emnlp-ijcnlp, methods, earth, dis-","In Inui, K., Jiang, J., Ng, V., and Wan, X., Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 2019. doi: 10.18653/y1/D19-1053."
195,"summarization, text, generation, evaluation, natural, dialogue, language, response","Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset . The definition of different aspects can be found in Tab. 8."
202,"reliability, coherence, relevance, fluency",SummEval collects human judgments on 16 model-generated summaries on CNN/Daily Mail dataset . REALSumm evaluates the reliability of automatic metrics by measuring the pyramid recall of text generated by 25 systems . NEWSROOM is another dataset focusing on factuality .
211,"summarization, text, generation, dialogue, response","Text summarization, machine translation, data-to-text, and dialogue response generation tasks are summarized in Tab. 11 and Tab. 12. Here, we convert the dialog response generation task as a boolean question-answering task ."
218,"convsersation, engaging, semantically, appropriate, understandable, seman","50.0 5 Relevant Is this an interesting response that is specific, engaging, relevant, correct, 50.9 and understandable? 7 Semantically appropriate Is the interesting response . 51.3 6 Understandable Is it an interesting . response that's specific, . engaging, pertinent, correct. 51.4 understand"
