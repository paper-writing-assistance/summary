element_idx,keywords,summarized_text
4,"image, synthesis, classifier, guidance, models, diffusion","diffusion models can achieve image sample quality superior to the current state-of-the-art generative models . For conditional image synthesis, we further improve sample quality . We achieve an FID of 2.97 on ImageNet 128 x 128, 4.59 on ImagesNet 256x256, and 7.72 on Image"
12,"image, gans, state-of-the-art, generation","GANs currently hold the state-of-the-art on most image generation tasks . However, some of these metrics do not fully capture diversity ."
13,"likelihood-based, sample, gan, visual, quality, models","GANs hold the state-of-the-art, but their drawbacks make them difficult to scale and apply to new domains . However, sampling from these models is slower in terms of wall-clock time ."
14,"coverage, likelihood-based, distribution, models, diffusion","Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images . These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound . Nichol and Dhariwal found"
15,"gan, quality, models, diffusion","We hypothesize that the gap between diffusion models and GANs stems from at least two factors . First, that the model architectures used by recent GAN literature have been heavily explored and refined . Second, that a GAN is able to trade off diversity for fidelity . With these improvements, we achieve"
16,"imagenet, classifier, improved, architecture, guidance, models","In Section 2, we introduce simple architecture improvements that give a substantial boost to FID . In Section 3, we describe a method for using gradients from a classifier to guide a diffusion model during sampling . We find that a single hyperparameter can be tuned to trade off diversity for fidelity ."
19,"ratio, level, diffusion, models, noise","diffusion models sample from a distribution by reversing a gradual noising process . Each timestep t corresponds to a certain noise level, and Xt can be thought of as a mixture of a signal xo with some noise E where the signal to noise ratio is determined by the timestep"
21,"component, noise, model, xt-1, function, diffusion","A diffusion model learns to produce a slightly more ""denoised"" Xt-1 . Each sample in a minibatch is produced by randomly drawing a data sample xo, a timestep t, and noise E . The training objective is then IIE - â‚¬||2 "
22,"gaussian, predictor, noise, ea, distribution, diffusion",Ho et al. show that diffusion sampling proceeds by repeatedly predicting Xt-1 from XT . The variance of this Gaussian distribution can be fixed to a known constant .
23,"diffusion, model, lsimple, quality, image","the simple mean-sqaured error objective, Lsimple, works better in practice than the actual variational lower bound Lvlb that can be derived from interpreting the denoising diffusion model as a VAE . We often use ""diffusion models"" as shorthand to refer to both classes"
27,"hybrid, objective, obj",Nichol and Dhariwal propose a hybrid objective for training both ea and Eo using the weighted sum Lsimple + Lvlb . Learning the reverse process variances with their hybrid objective allows sampling with less steps without much drop in sample quality .
28,"noising, ddim, non-markovian, ddpm, reverse, noise","Song et al. propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM . By setting this noise to 0, they provide a way to turn any model eo into a deterministic mapping from latents to images "
31,"imagenet, class, score, (is), (fid), inception, distribution",Inception Score was proposed by Salimans et al. to better capture diversity than IS . It measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class .
35,"relationships, spatial, symmetric, score, inception, distribution, measure, image","FID provides a symmetric measure of the distance between two image distributions in the Inception- V3 latent space . Recently, Nash et al. proposed sFID as a version of FID that uses spatial features rather than the standard pooled features . Kynkaanniem"
36,"fid, generative, modeling, state-of-the-art, work","We use Precision or IS to measure diversity and fidelity . When comparing against other methods, we re-compute these metrics using public samples or models whenever possible . To ensure consistent comparisons, we use the entire training set as the reference batch ."
39,"imagenet, embedding, architecture, timestep, models, diffusion","Ho et al. introduced the UNet architecture for diffusion models . The UNet model uses a stack of residual layers and downsampling convolutions . In addition, they use a global attention layer at the 16x 16 resolution with a single head ."
50,"fid, helps, depth, training","FID is evaluated at two different points of training, in Table 1 . Aside from rescaling residual connections, all of the other modifications improve performance ."
51,"attention, transformer, configurations, architecture","For the rest of the architecture, we use 128 base channels, 2 residual blocks per resolution, multi-resolution attention, and BigGAN up/downsampling . Table 2 shows our results, indicating that more heads or fewer channels per head improves FID . In Figure 2, we see 64 channels is"
54,"adaptive, normalization, instance, embedding, norm, group, timestep","adaptive group normalization incorporates the timestep and class embedding into each residual block . We define this layer as AdaGN = Ys GroupNorm + yb, where h is the intermediate activations of the residual block following the first convolution ."
55,"adaptive, normalization, layer, group, multi-resolution, attention","In Table 3, we explicitly ablate this choice, and find that the adaptive group normalization layer indeed improved FID . Both models use 128 base channels and 2 residual blocks per resolution, multi-resolution attention with 64 channels per head ."
58,"label-limited, synthetic, synthesis, regime, gan, conditional, labels, image",GANs for conditional image synthesis make heavy use of class labels . This often takes the form of class-conditional normalization statistics as well as discriminators with heads that are explicitly designed to behave like classifiers p .
59,"generator, information, layer, gradients, diffusion","Sohl-Dickstein and Song et al. show one way to achieve this, wherein a pre-trained diffusion model can be conditioned using the gradients of a classifier . In particular, we can train a classificationifier p on noisy images Xt ."
60,"timestep, classifiers, t, sampling, conditional, processes","In this section, we first review two ways of deriving conditional sampling processes using classifiers . We then describe how we use such classifier in practice to improve sample quality ."
78,"c4, constant, term","We can safely ignore the constant term C4 since it corresponds to the normalizing coefficient Z in Equation 2 . We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by g."
80,"matching, stochastic, score, ddim, diffusion","The above derivation for conditional sampling is only valid for stochastic diffusion sampling process, and cannot be applied to deterministic sampling methods like DDIM . To this end, we use a score-based conditioning trick adapted from Song et al."
88,"imagenet, scale, generative, model, large, classification, task, models","Classifier architecture is simply the downsampling trunk of the UNet model . We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting ."
89,"imagenet, model, classifier, unconditional, gradients","In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1 . Scaling up the gradients remedied this problem, and the class probabilities from the Classifier increased to nearly 100% ."
90,"classifier, gradients, larger, values","s  Vx log p = Vx ps, where Z is an arbitrary constant . This distribution becomes sharper than p, since larger values are amplified by the exponent ."
91,"model, classifier, underlying, conditional, guidance, diffusion","We assumed that the underlying diffusion model was unconditional, modeling p . It is also possible to train conditional diffusion models, p, and use classifier guidance in the exact same way ."
100,"gradient, scale, classifier, precision/recall, biggan, guidance, trade-off",the gradient scale in Figure 4 shows that scaling the gradients beyond 1.0 smoothly trades off recall for higher precision and IS . We compare our guidance with the truncation trick from BigGAN in Figure 5 .
105,adm-g,"ADM refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance . ImageNet diffusion models are sampled using 250 steps, except when we use the DDIM sampler with 25 steps ."
107,"imagenet, image, generation, model, state-of-the-art, diffusion","Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task . For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs ."
108,"of, model, biggan-deep, quality, diffusion","Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model . The diffusion model contains more modes than the GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers ."
110,"low-resolution, diffusion, state-of-the-art, model","Nichol and Dhariwal and Saharia et al. train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model . In this approach, the model is trained to upsample images from the training set, and conditions on low-"
116,"classifier, stack, guidance, upsampling","Table 6: Comparing our single, upsampling and classifier guided models . The base resolution for the two-stage models is 64 and 128 for the 256 and 512 models, respectively."
119,"dynamics, langevin","Song and Ermon introduced Score based generative models as a way of modeling a data distribution using its gradients, and then sampling using Langevin dynamics ."
121,"image, setup, gan-like, speed, quality, models, diffusion",Song et al. explored ways to leverage techniques from stochastic differential equations to improve sample quality obtained by score-based models . Nichol and Dhariwal proposed methods to improve sampling speed . Saharia and Nichol demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models.
122,"low-temperature, fidelity, model, gan, sampling, diffusion","Brock et al. introduced the truncation trick for GANs, wherein the latent vector is sampled from a normal distribution . They found that increasing trunk in diversity naturally led to a decrease in diversity but an increase in fidelity ."
123,"quality, likelihood-based, models, image","NVAE and VDVAE are another promising class of likelihood-based models with a rich history . These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics ."
124,"clip, model","Song et al. uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model . In some cases, classifiers can act as stand-alone generative models ."
126,"steps, model, ddim, denoising, multiple, diffusion",diffusion models are an extremely promising direction for generative modeling . The samples from the single step model are not yet competitive with GANs . Future work in this direction might be able to completely close sampling speed gap .
127,"unlabeled, classifier, labeled, guidance, datasets",our proposed classifier guidance technique is currently limited to labeled datasets . Our method could be extended to unlabeled data by clustering samples to produce synthetic labels .
132,"state-of-the-art, architecture, gans, models, diffusion",diffusion models can obtain better sample quality than state-of-the-art GANs . Our improved architecture is sufficient to achieve this on unconditional image generation tasks . We find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity .
136,"machine, synthesis, natural, active, visual, boltzmann, processing, function, image","A learning algorithm for boltzmann machines. Cognitive science, 9:147-169, 1985. Adverb. The big sleep . Shane Barratt and Rishi Sharma. A note on the inception score . arXiv:1801.01973, 2018. Andrew Brock, Jeff Donahue, and Karen Simon"
138,"of, adversarial, recovery, diffusion, networks, generative, dynamics, image","Anirudh Goyal, Nan Rosemary Ke, Bing Xu, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net . ArXiv:arXi"
140,"supervision, analysis, networks, generative, natural, recognition, visual, language, adversarial","Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization . ArXiv:1711.05101, 2017. Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain"
142,"analysis, audio, generative, modeling, raw","Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks . arXiv:1606.03498, 2016. Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleks"
147,"nvidia, v100, thop, thops, tesla","We first benchmark the throughput of our models in Table 7 . For the theoretical throughput, we measure the theoretical FLOPs for our model . We assume 100% utilization of an NVIDIA Tesla V100 ."
148,"imagenet, pytorch, batch, size, 1.7, per-gpu","a naive implementation of our models in PyTorch 1.7 is very inefficient, utilizing only 20-30% of the hardware . We benchmark our optimized version, which uses larger per-GPU batch sizes, fused GroupNorm-Swish and fused Adam CUDA ops ."
152,"imagenet, 128x128, 256x256, model, biggan-deep, training, superior, quality",Table 8 and 9 evaluate our ImageNet 128x128 and 256x256 models throughout training . The ImageNet model beats BigGAN-deep's FID after 500K training iterations .
159,"biggan2, lightweight, table, 10, biggan-deep",In Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep . We convert their TPU-v3 estimates to V100 days according to 2 TPU v3 day = 1 V100 day .
161,"model, biggan-deep, biggan, training, diffusion",Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and BigGAN-deep . Compute is measured in V100-days .
168,"q(xo), gaussian, neural, data, network, distribution","Sohl-Dickstein et al. note that q approaches a Gaussian distribution as T  8 and correspondingly Bt  0, so it is sufficient to train a neural network to predict a mean ou and a diagonal covariance matrix o:"
177,"classifier, dataset, training, model",Models achieve best FID when using a classifier to reduce diversity of generations . One might fear that such a process could cause the model to recall existing images from the training dataset . Figure 7 shows that the samples are indeed unique and not stored in the training set .
190,"reverse, approximation, steps, ode","reverse ODE approximation gives latents with reasonable reconstructions, even with as few as 250 reverse steps . reversing the first 249 steps gives much better reconstructions ."
191,"ddim, space, latent, interpolation",Figures 10a to 10c show DDIM latent space interpolations on a class-conditional 256x256 model . The left and rightmost images are ground truth dataset examples . We see that the model with no guidance has almost perfect reconstructions .
201,"temperature, diffusion, model","The temperature parameter T is typically setup SO that T = 1.0 corresponds to standard sampling, and T  1.0 focuses more on high-density samples . We experimented with two ways of implementing this for diffusion models ."
202,"imagenet, fid, temperature, scaling, affects","To measure how temperature scaling affects samples, we experimented with our ImageNet 128x 128 model, evaluating FID, Precision, and Recall across different temperatures . We also find that low temperatures have both low precision and low recall, indicating that the model is not focusing on modes of the real data distribution"
221,"approximation, network",The q term can be treated as a constant since it does not depend on xt . We want to sample from the distribution Zqq where Z is a normalizing constant . All that is left is an approximation of q.
225,"w, state, model, optimizer, adam, training, diffusion",Hyperparameters used to train upsampling models are in Table 13 . We train all models using Adam or Adam W with B1 = 0.9 and B2 = 0.999 .
226,"adaptive, normalization, heads, group, architecture, attention, ablations","for all architecture ablations, we train with batch size 256 . We sample using 250 sampling steps . By default, all of our experiments use adaptive group normalization ."
239,"bedrooms, schedule, lsun, time, models, noise","To address this, we conducted a sweep over sampling-time noise schedules . We swept over schedules on LSUN bedrooms, and selected the schedule with the best FID for use on the other two datasets. Table 15 details the findings of this sweep, and Table 16 applies this schedule to three LSun dataset"
242,"bedroom, sweeping, timesteps, lsun, process, diffusion",Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms . The schedule is expressed as a sequence of five integers . Each integer is the number of steps allocated to one fifth of the diffusion process .
