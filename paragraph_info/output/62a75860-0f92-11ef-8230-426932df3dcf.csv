element_idx,summarized_text,keywords
3,"Abstract-The past few years have witnessed the rapid development of applying the Transformer module to vision problems . While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited . The results we","problems, visual, vision, recognition, visformer"
6,"In the past decade, convolution used to play a central role the deep learning models - for visual recognition . This situation starts to change when the Transformer is transplanted to the vision scenarios . It was shown in the ViT model that an image can be partitioned into a grid of patches .","word, convolution, visual, recognition"
15,vision Transformers have higher 'upper-bound' while convolution-based models are better . Both upper-bound and lower-bound are important properties for neural networks .,"models, vision, lower-bound, convolution-based"
16,"The gap between Transformer-based and convolution-based networks can be revealed with two different training settings on ImageNet . The first is the base setting . It is the standard setting for convolutionbased models, i.e., the training schedule is shorter and the data augmentation is stronger .","upper-bound, imagenet, convolution-based, models, lower-bound"
17,"DeiT-S and ResNet-50 employ comparable FLOPs and parameters . However, they behave very differently trained on the full data under these two settings . Deit-S has higher elite performance, but the improvement for the elite setting is merely 1.3% . This motivates us to study the difference between these models","resnet-50, models, transformer-based"
22,"From DeiT-S to ResNet-50, one should use global average pooling . introduce step-wise patch embeddings . adopt stage-wise backbone design .","shape, visual, resnet-50, recognition, deit-s, network"
23,"Visformer-S outperforms DeiT-S and ResNet-50 by 2.12% and 3.46%, respectively, under a comparable model complexity . Different from Deit-S, the model survives two extra challenges, namely, when the model is trained with 10% labels and 10% classes .","visformer-s, resnet, model, visformer-ti"
24,"The contribution of this paper is three-fold . First, we introduce the lower-bound and upper-bound to investigate the performance of Transformer-based vision models . second, we close the gap between the Transformerbased and convolution-based models by a gradual transition process .","upper-bound, convolution-based, models, transformer, lower-bound"
26,We optimize the architecture of Visformer according to the experimental observations . We propose an efficient method to avoid overflow without degrading the performance .,"problem, overflow, visformerv2"
28,"In the deep learning era, the most popular method is to use deep neural networks . a number of convolutional kernels are used to capture repeatable local patterns in the input image and intermediate data .","image, vision, classification, computer"
30,"Since Transformers achieved remarkable success in natural language processing, many efforts have been made to introduce Transformers to vision tasks . The first category consists of pure attention models . These models usually only use self-attention and attempt to build vision models without convolutions . However, it is computationally expensive to relate all pixels with self","natural, language, vision, models, processing"
31,"Self-attention was first introduced to CNNs by non-local neural networks . These networks aim to capture global dependencies in images and videos . Afterwards, Transformers achieve remarkable success in NLP tasks .","neural, convolution, self-attention, layer, networks"
34,"CaiT find that it is very efficient to scale up vision Transformer in depth dimension with LayerScale . Zhai et al. observe that most vision Transformers can benefit from increasing compute resources and larger datasets . Steiner and al. further study data, augmentation and regularization in vision Transformer .","autoformer, layerscale, transformer, vision"
35,"Self-attention has been used in many downstream vision tasks . For example, the frameworks in COCO usually utilize 1280 x 800 inputs . This is a critical problem for vision Transformer, since the complexity increases quadratically with pixel numbers .","exchange, cswin, self-attention, information, transformer"
39,"Convolution originates from the intuition to capture local patterns which are believed more repeatable than global patterns . It uses a number of learnable kernels to compute the responses of the input to different patterns, for which a sliding window is moved along both axes .","skip-connection, convolutional, convolution, network, layer"
41,"generating three features for each token, named the query, key, and value . The response of each token is calculated as a weighted sum . This is often called multi-head self-attention .","token, multi-head, each, (mhsa), self-attention, value"
42,DeiT-S and ResNet-50 are the representative of Transformer-based and convolution-based models . The impact of these details will be elaborated in Section III-C .,"resnet-50, models, transformer-based"
44,"DeiT-S has changed the training strategy significantly, e.g., the number of epochs is enlarged by more than 3x . In what follows, we provide a comprehensive study on this phenomenon .","training, deit-s, large-scale, carefully-tuned, set, strategy"
46,"Specifically, the model is trained for 90 epochs with the SGD optimizer . The learning rate starts with 0.2 for batch size 512 and gradually decays to 0.00001 following the cosine annealing function .","optimizer, convolution-based, recognition, adamw, models"
53,The first step of the transition is to remove the classification token and add global average pooling to the Transformer-based models . Transformers usually add a classification token to the inputs and utilize the corresponding output token to perform classification .,"token, global, average, pooling, classification"
54,the Transformer can be equivalent translated to the convolutional version as shown in Figure 1 . The kernel size and stride of the intermediate features can be converted from a sequence of tokens to a bundle feature maps .,"version, convolutional, convolution, token"
55,The performance of the obtained network is shown in Table II . This transition can substantially improve the base performance . Our further experiments show that adding global pooling itself can improve base performance from 64.17% to 69.44%.,"global, nin, average, pooling, operation"
57,DeiT and ViT models directly encode the image pixels with a patch embedding layer . This operation flattens the image patches to a sequence of tokens SO that Transformers can handle images .,"model, patch, step-wise, flattening, deit, embedding"
58,"We first add the stem layer in ResNet to the Transformer, which is a 7 x 7 convolution layer with a stride of two . The stem layer can be seen as a 2 x 2 embedding operation with pixel overlap . Since the patch size in the original DeiT model is 16,","patch, convolution, step-wise, small, convolution-based, models, embedding"
62,"A MODIFICATION CAN IMPACT THE BASE AND ELITE PERFORMANCE DIFFERENTLY . THOUGH THE NUMBER OF PARAMETERS INCREASES CONSIDERABLY AT THE INTERMEDIATE STATUS, THE COMPUTATIONAL COSTS","imagenet, procedure, during, modification, accuracy, modifica"
65,"In this section, we split networks into stages like ResNets . The blocks in the same stage share the same feature resolution . This transition in this section is to reassign the blocks to different stages .","feature, resnets, self-attention, head, resolution, dimension"
66,"This transition leads to interesting results. The base performance is further improved . It is conjectured that the stagewise design leverages the image local priors . However, the elite performance of the network decreases markedly .","ablation, large, experiments, self-attention, resolution"
67,Transformer-based models usually use BatchNorm to stabilize training process . LayerNorm is independent of batch size and more friendly for specific tasks compared with BatchNorm . BatchNorm usually can achieve better performance given appropriate batch size .,"size, batchnorm, layer-norm, layernorm, batch, models, transformer-based"
70,"Since the tokens of the network are present as feature maps, it is natural to introduce convolutions with kernel sizes larger than 1 x 1 . The specific meaning of large kernel convolution is illustrated at the bottom right of Figure 1. When global selfattentions attempt to build the relations between all the . tokens within local neighborhoods","kernel, large, convolution, network, 3, x"
71,3 x 3 convolutions can leverage local priors in images further improve network base performance . The base performance becomes comparable with ResNet-50 .,"performance, convolutions, base, network, 3, x"
75,We test to remove the position embedding of DeiT-S and elite performance decreases significantly by 3.95% . The base performance is almost unchanged and the elite performance declines slightly . It reveals that position embeddeding is less important in the transition model than that in pure Transformer-based models .,"position, performance, elite, deit-s, embedding"
77,the pure convolution-based network performs much worse in base performance and elite performance . It indicates that self-attentions drive neural networks to higher elite performance and is not responsible for the poor base performance in ViT or DeiT .,"performance, elite, network, self-attention, base"
78,"Net7 is different from ResNet-50 . Their depths, widths, bottleneck ratios and block numbers are different . ResNet 50 down-samples the features with bottleneck blocks .","bot-tleneck, block, resnet-50"
79,ResNet-50 has better network architecture and can perform better with fewer FLOPs . It indicates that the inconsistencies between base performance and elite performance exist .,"base, performance, elite"
86,"Stage-wise design re-arrange the blocks from one stage to three stages . Thus, for elite performance, some blocks in the new two stages must work less efficiently than those in the original stage . The replacement of self-attention in all three stages reduces both the base performance and the elite performance .","self-attention, performance, elite, base"
87,"The second problem is adding 3 x 3 convolutions to the feed-forward blocks . We replace MLP blocks with bottleneck blocks in each stage separately . On the high-resolution stage, self-attentions have difficulty handling all tokens .","performance, elite, convolutions, self-attention, 3, x"
88,Visformer adopts the stage-wise design for higher base performance . But self-attentions are only used in the last two stages .,"visformer, transformer, vision-friendly"
95,"architecture configurations of Visformer are not carefully tuned . For example, when splitting the network into different stages, expect that we utilize 3 more blocks in the first stage to compensate for the removal of self-attention . In other words, the stage configuration is not carefully designed . We conduct many experiments to explore the architecture of Vis","visformerv2, network, performance, architecture"
97,"Quantization has been widely used to accelerate the training process and save GPU memory . Specifically, halfprecision floating-point , the lowest precision that can preserve the network performance, has been adopted by many researchers .","performance, memory, quantization, network, fp16"
111,"We first compare Visformer against DeiT, the direct baseline . Results are summarized in Table VII . The Advantages are 2.12% and 6.41%, while under the base setting, the numbers grow to 14.08% .","visformer-s, visformer, visformer-ti"
114,"Four subsets of ImageNet are used, with 10% and 1% randomly chosen classes . We still use the elite setting with 300 epochs .","imagenet, visformer, visual, recognition"
119,"We use relative position bias to Visformer, which improves the results to 82.39% as shown in Table IV-C . Then we test whether we need to utilize an extra early stage . This stage can improve the detection and segmentation results, which will be detailed in Section IV-F .","visformerv2, visformerv2-s"
120,"the deep and narrow architecture has a better parallelization property . To compensate for the loss in runtime, we use fewer FLOPs for 'deep-narrow' networks .","runtime, deep, architecture"
122,"Visformer-Ti performs much better than most of the models with similar FLOPs . For larger models, Visformeder-S outperforms other Vision Transformer models . VisformformerV2-S further improves the performance .","visformerv2-ti, visformerv2-s, visformerv"
130,"VisformerV2-S and EfficientNet-B4 have similar FLOPs and performance . It is shown that Visformner-S is significantly faster than a Vision Transformer model . However, it is not as efficient in runtime .",visformerv2-s
135,Swin Transformers are our important baseline models . It should be emphasized that the self-attention in Visformer can also be replaced with other resolution-friendly selfattentions .,"swin, cswin, self-attention, transformer"
136,The models are evaluated with two frameworks: MaskRCNN and Cascade Mask-RCNN . We inherit the training settings in : the AdamW optimizer with a learning rate of 0.0001 and the weight decay of 0.05 . The batch size is 16 and we show the results of 1x and 3x,"optimizer, fps, adamw, mask-rcnn, cascade, models, flop"
144,"This paper presents Visformer, a Transformer-based model that is friendly to visual recognition . We propose to use two protocols, the base and elite setting, to evaluate the performance of each model . To study the reason why Transformerbased models behave differently, we decompose the gap between these models and design an eight-","visformer-s, visformer, model"
148,"C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Erhan, V. Vanhoucke, and A. Rabinovich, ""Deep residual learning for image recognition,"" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015,","training, image, recognition, network, deep"
150,"A. Krizhevsky, I. Sutskever, and G. E. Hinton, ""Imagenet classification with deep convolutional neural networks,"" in Advance in neural information processing systems, 2012, pp. 1097-1105 . I. Bello, A. Levskaya, and J","visual, conference, vision, recognition, on, and, pattern, international, ieee/cvf, transformer, computer"
151,"W. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C-Xu . xu, and W. Gao, ""Swin transformer: Hierarchical vision transformer using shifted windows,"" in European conference on computer vision .","conference, vision, design, on, cvf, network, transformer, computer"
