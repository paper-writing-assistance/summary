element_idx,keywords,summarized_text
4,"deep, feedback, networks, neural, implicit, recognition, speech","deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing . In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation collaborative filtering on the basis of implicit feedback."
5,"re, information, recommendation, auxil-, iary","some recent work has employed deep learning for recommendation . They primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics."
6,"neural, collaborative, architecture, filtering, network-based, ncf",NCF is generic and can express and generalize matrix factorization . Extensive experiments on real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods.
19,"system, recommender, filtering, factorization, collaborative, matrix","the key to a personalized recommender system is in modelling users' preference on items based on their past interactions . matrix factorization is the most popular one, which projects users and items into a shared latent space . This involves a vector of latent features to represent a user ."
20,"feature, interaction, orative, mf, la-tent, collab-, filtering, function",MF has become the de facto approach to latent factor model-based recommendation . Much research effort has been devoted to enhancing MF . It is well-known that its performance can be hindered by the simple choice of the interaction function inner product .
21,"text, interaction, dnns, processing, function",the neural network has been proven to be capable of approximating any continuous function . there is relatively little work on employing DNNs for recommendation .
24,"effe, mf, methods, filtering, effect","some recent advances have applied DNNs to recommendation tasks . They mostly used DNN to model auxiliary information, such as textual description of items, audio features of musics, and visual content of images ."
25,"feedback, modelling, implicit, dnn, network","We focus on implicit feedback, which indirectly reflects users' preference through behaviours like watching videos, purchasing products and clicking items . In this paper, we explore the central theme of how to utilize DNNs to model noisy implicit feedback signals ."
27,"perceptron, multi-layer, collaborative, filtering, ncf",MF can be interpreted as a specialization of NCF and use a multi-layer perceptron to endow NCF modelling with a high level of non-linearities . 3. We perform extensive experiments on two real-world datasets to demonstrate the effectiveness of our NCF approaches .
32,"item, user, implicit, data, yui","a value of 1 for Yui indicates that there is an interaction between user u and item 2; however, it does not mean u actually likes i . This poses challenges in learning from implicit data, since it provides only noisy signals about users' preference ."
37,"pointwise, entries, learning, information, unobserved","To estimate parameters , existing approaches generally follow the machine learning paradigm that optimizes an objective function . Methods on pointwise learning usually follow a regression framework by minimizing the squared loss between Yui and its target value Yui To handle the absence of negative data, they have either treated all unobserved entries"
42,"mf, innovation",Figure 1 illustrates how the inner product function can limit the expressiveness of MF . There are two settings to be stated beforehand to understand the example well .
47,"first, three, line, model, mf, (users), rows, dashed","We can have S41 > S43 > S42 , meaning that u4 is most similar to u1, followed by u3, and lastly u2 . However, if a MF model places p4 closest to P1, it will result in a large ranking loss ."
48,"space, dnns, mf, low-dimensional, latent, limitation","The above example shows the possible limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space . We note that one way to resolve the issue is to use a large number of latent factors K . However, it may adversely"
50,"perceptron, multi-layer, mlp, ncf","We first present the general NCF framework, elaborating how to learn NCF . We then propose an instantiation of NCF using a multi-layer perceptron to learn the user-item interaction function ."
52,"interaction, multi-layer, representation, user-item, collaborative, yui, filtering",we adopt a multi-layer representation to model a user-item interaction Yui . The bottom input layer consists of two feature vectors VU and VI that describe user u and item 2 respectively .
54,"features, content-based, input","this work focuses on the pure collaborative filtering setting . We use only the identity of a user and an item as the input feature, transforming it to a binarized sparse vector with one-hot encoding ."
55,"modelling, pairwise, learning, bayesian, personalized, neural, ranking, network",the obtained user embedding can be seen as the latent vector for user in the context of latent factor model . Each layer of the neural CF layers can be customized to discover certain latent structures of user-item interactions . The dimension of the last hidden layer X determines the model's capability .
61,"instance, implicit, data, training, ncf","the squared loss can be explained by assuming observations are generated from a Gaussian distribution . For implicit data, the target value Yui is a binarized 1 or 0 denoting whether u has interacted with i ."
65,"cross-entropy, gradient, log, descent, stochas-tic, binary, loss, ncf","This is the objective function to minimize for the NCF methods . We address recommendation with implicit feedback as a binary classification problem . For negative instances y-, we uniformly sample them from unobserved interactions in each iteration and control the sampling ratio w.r.t."
71,"non-linear, mf, framework, nonlinear, setting, ncf","Under the NCF framework, MF can be easily generalized and extended . For example, if we allow h to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions . In this work, we implement a generalized"
74,"deep, learning, concate-nated, vector, multimodal, work, ncf","NCF adopts two pathways to model users and items . This design has been widely adopted in multimodal deep learning work . To address this issue, we propose to add hidden layers on the concatenated vector ."
75,"hyperbolic, saturation, bias, activation, vector, tangent, sigmoid, function","Wx, bx, and ax denote the weight matrix, bias vector, and activation function for the x-th layer's perceptron . We would like to analyze each function: 1) The sigmoid function restricts each neuron to be in , which may limit the model"
76,"bottom, layer, tower, pattern","a common solution is to follow a tower pattern, where the bottom layer is the widest and each successive layer has a smaller number of neurons . We empirically implement the tower structure, halving the layer size for each successive higher layer ."
85,"embedding, factorization, user, neural, mlp, matrix",pG and pM denote user embedding for GMF and MLP parts . We use ReLU as the activation function of MLP layers .
91,"adaptive, moment, methods, mlp, training, (adam), neumf, estimation","Adam method yields faster convergence for both models than the vanilla SGD . After feeding pre-trained parameters into NeuMF, we optimize it . This is because Adam needs to save momentum information for updating parameters properly ."
100,"rating, movie, movielens, dataset","MovieLens has been widely used to evaluate collaborative filtering algorithms . We used the version containing one million ratings, where each user has at least 20 ratings . To this end, we transformed it into implicit data ."
105,"algorithms, filtering, movielens, interactions","The original data is very large but highly sparse . For example, over 20% of users have only one pin . This results in a subset of the data that contains 55, 187 users and 1,500, 809 interactions ."
106,"ndcg, metrics, item, test, both","To evaluate the performance of item recommendation, we adopted the leave-one-out evaluation . For each user, we held-out her latest interaction as the test set . The performance of a ranked list is judged by Hit Ratio and Normalized Discounted Cumulative Gain ."
115,"factors, predictive, neural, data, mlp, cf, validation, ncf",All NCF models are learnt by optimizing the log loss of Equation 7 . We randomly initialized model parameters with a Gaussian distribution . It is worth noting that large factors may cause overfitting .
117,"ndcg, ndcg@, mf, methods, ndcg@10","Figure 4 shows the performance of HR@10 and NDCG@10 . For MF methods BPR and eALS, the number of predictive factors is equal to latent factors . We tested different neighbor sizes and reported the best performance ."
118,"of, state, the, model, mlp, art, gmf, ncf, method",NeuMF significantly outperforms eALS and BPR with a large factor of 64 . This indicates the high expressiveness of NeuMF by fusing the linear MF and non-linear MLP models .
119,"position, neumf, k, ranking","Figure 5 shows the performance of Top-K recommended lists where the ranking position K ranges from 1 to 10 . NeuMF demonstrates consistent improvements over other methods across positions . We further conducted one-sample paired t-tests, verifying that all improvements are statistically significant for p  0.01"
126,"pre-training, movielens, adam","NeuMF with pretraining achieves better performance in most cases . For MovieLens with a small predictive factors of 8, the pretraining method performs slightly worse . This result justifies the usefulness of our pre-training method for initializing NeuMF."
130,"feedback, movielens, recommendation, implicit, performance",Figure 6 shows the training loss and recommendation performance of NCF methods . Results on Pinterest show the same trend and thus they are omitted due to space limitation .
131,"ratio, pointwise, log, negative, methods, sampling, loss, ncf","pairwise objective functions can pair only one sampled negative instance with a positive instance . but we can flexibly control the sampling ratio of a pointwise loss . To illustrate the impact of negative sampling, we show the performance of NCF methods w.r.t. different negative sampling ratios ."
138,"identity, deep, layer, linear, network, stacking, function, structure","The results are summarized in Table 3 and 4 . The MLP-3 indicates the MLP method with three hidden layers . This result is highly encouraging, indicating the effectiveness of using deep models for collaborative recommendation ."
139,"performance, layer, mlp-0, hidden",MLP-0 has no hidden layers . This verifies our argument in Section 3.3 that simply concatenating user and item latent vectors is insufficient for modelling .
143,"feedback, weighting, implicit, collaborative, filtering","While early literature on recommendation has largely focused on explicit feedback , recent attention is increasingly shifting towards implicit data . The collaborative filtering task with implicit feedback is usually formulated as an item recommendation problem, for which the aim is to recommend a short list of items to users . One key insight is to model the missing data,"
147,"modelling, networks, autorec, neural, autoencoders","The early pioneer work by Salakhutdinov et al. proposed a two-layer Restricted Boltzmann Machines to model users' explicit ratings on items . The work was been later extended to model the ordinal nature of ratings . Recently, autoencoders have become a popular"
148,"dae, feedback, multi-layer, feedforward, neural, implicit, network","the work that is most relevant to our work is , which presents a collaborative denoising autoencoder for CF with implicit feedback . In contrast to the DAE-based CF, CDAE additionally plugs a user node to the input of autoencodingrs for reconstructing the user's"
150,"learning, neural, mlp, network, tensor, neumf","Neural Tensor Network uses neural networks to learn the interaction of two entities . NeuMF is more flexible and generic than NTN, in terms of allowing MF and MLP learning different sets of embeddings."
151,"deep, ing, learn-, approach, mlp, dnn, network",Google Publicized their Wide & Deep learning approach for App recommendation . The deep component also uses a MLP on feature embeddings .
153,"deep, learning, recommendation, neural, architectures, network","In this work, we explored neural network architectures for collaborative filtering . We devised a general framework NCF and proposed three instantiations GMF, MLP and NeuMF that model user-item interactions in different ways . Our framework is simple and generic; it is not limited to the models presented in this"
154,"system, recommender, data, multi-modal, multi-media, ncf","In future, we will study pairwise learners for NCF models and extend NCF to model auxiliary information, such as user reviews , knowledge bases, and temporal signals ."
158,"x, x., he","I. Bayer, X. He, B. Kanagal, and S. Rendle . A generic coordinate descent framework for learning from implicit feedback . In www, 2017. A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and 0. Yakhn"
159,"deep, multi-view, feedback, learning, implicit","In ICML, pages 160-167, 2008. A. M. Elkahky, Y. Song, and X. He. A multi-view deep learning approach for cross domain user modeling in recommendation systems. In ICCV, pages 4274-4282, 2015. K. He, M. Gao,"
161,"deep, learning, recommendation, graph-based, visual, classification","In NIPS, pages 1-8, 2008. R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted boltzmann machines for collaborative filtering . In ICDM, pages 791-798, 2007. B. Sarwar, G. Karypis, J"
