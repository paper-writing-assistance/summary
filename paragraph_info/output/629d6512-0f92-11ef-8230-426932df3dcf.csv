element_idx,summarized_text,keywords
6,We introduce a teacher-student strategy specific to transformers . It relies on a distillation token ensuring that the student learns from the teacher through attention . We show the interest of this token-based distillation .,"convnet, transformers"
8,"Convolutional neural networks have been the main design paradigm for image understanding tasks . One of the ingredient to their success was the availability of a large training set, namely Imagenet .","understanding, convolutional, tasks, neural, image, networks"
15,"In this paper, we train a vision transformer on a single 8-GPU node in two to three days that is competitive with convnets having a similar number of parameters and efficiency . We build upon the visual transformer architecture from Dosovitskiy et al.","image, transformer, vision, data-efficient"
19,"We show that our neural networks that contains no convolutional layer can achieve competitive results against the state of the art on ImageNet with no external data . We introduce a new distillation procedure based on a distillation token, which plays the same role as the class token, except that it aims at reproducing the label","token, imagenet, image, distillation, transformer"
20,This paper is organized as follows: we review related works in Section 2 and focus on transformers for image classification in Section 3 . The experimental section 5 provides analysis and comparisons against both convnets and recent transformers . Section 6 details our training scheme .,"data-efficient, training, image, transformer, classification"
22,Image Classification is so core to computer vision that it is often used as a benchmark to measure progress in image understanding . Any progress usually translates to improvement in other related tasks such as detection or segmentation .,"image, understanding, vision, computer"
23,"hybrid architectures that combine convnets and transformers have recently exhibited competitive results in image classification , detection , video processing , unsupervised object discovery , and unified text-vision tasks .","image, transformers, classification"
26,Recently Vision transformers closed the gap with the state of the art on ImageNet without using any convolution . This performance is remarkable since convnet methods for image classification have benefited from years of tuning and optimization .,"imagenet, convolution, vision, transformer"
27,"Transformer architecture, introduced by Vaswani et al. for machine translation, are currently the reference model for all natural language processing tasks . For example, Squeeze and Excitation , Selective Kernel and Split-Attention Networks exploit mechanism akin to transformers self-attention mechanism.","nlp, transformer, architecture"
28,"Knowledge Distillation , introduced by Hinton et al., refers to the training paradigm in which a student model leverages ""soft"" labels coming from a strong teacher network . This is the output vector of the teacher's softmax function rather than just the maximum of scores, wich gives ","convolutional, model, way, label, kd, soft"
34,"Multi-head self-attention layer is defined by considering h attention ""heads"" Each head provides a sequence of size N x d . These h sequences are rearranged into a N X dh sequence .","self-, self-attention, layer, multi-head"
35,FFN is composed of two linear layers separated by a GeLu activation . The second layer reduces the dimension from 4D back to D . Both MSA and FFN are operating as residual operators .,"image, transformer, block, ffn"
36,"In order to get a transformer to process images, our work builds upon the ViT model . The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 x 16 pixels . Each patch is projected with a linear layer that conserves its overall dimension 3 ","input, model, image, vit, transformer, block"
37,the class token is appended to the patch tokens before the first layer . The transformer then process batches of tokens of dimension D . At training time the supervision signal comes only from the class embedding .,"token, training, nlp, patch, time, vision, computer"
39,Touvron et al. show that it is desirable to use a lower training resolution and fine-tune the network at the larger resolution . This speeds up the full training and improves the accuracy under prevailing data augmentation schemes .,"fine-tuning, encoding, positional, transformer, block"
41,"In this section we assume we have access to a strong image classifier as a teacher model . It could be a convnet, or a mixture of classifiers . This section covers two axes of distillation .","convolutional, neural, image, network, classifier, transformer"
50,Distillation token is used similarly as the class token . It interacts with other embeddings through self-attention . The distillation embeddering allows our model to learn from the output of the teacher .,"token, distillation"
51,the learned class and distillation tokens converge towards different vectors . The average cosine similarity between these tokens equals 0.06 . This is expected since as they aim at producing targets that are similar but not identical .,"similarity, class, distillation, network, and, tokens"
53,"our distillation token adds something to the model . Instead of a teacher pseudo-label, we experimented with a transformer with two class tokens . The output embedding are also quasi-identical .","class, token, distillation, additional, transformer"
55,"Classification with our approach: joint classifiers . At test time, both the class or the distillation embeddings produced by the transformer are associated with linear classes and can infer the image label .","image, label, joint, classifiers, classification"
59,"Our architecture design is identical to the one proposed by Dosovitskiy et al. Our only differences are the training strategies, and the distillation token . Also we do not use a MLP head for the pre-training but only a linear classifier .","deit, design, architecture, training"
67,Our distillation method produces a vision transformer that becomes on par with the best convnets in terms of the trade-off between accuracy and throughput . Our best model on ImageNet-1k is 85.2% top-1 accuracy outperforms the best Vit-B model pretrained on JFT-300M at resolution,"imagenet-1k, accuracy, transformer, vision"
68,Table 2 compares distillation results with different teacher architectures . The fact that the convnet is a better teacher is probably due to the inductive bias inherited by the transformers through distillation .,"convnet, transformer"
70,"Table 3: Distillation experiments on Imagenet with DeiT . We report the results for our new distillation method in the last three rows . In the last row, the result correspond to the late fusion of the class and distillation classifiers.","distillation, experiments, method, imagenet"
72,"Hard distillation reaches 83.0% at resolution 224x224, compared to soft distillation accuracy of 81.8% . Our distillation strategy from Section 4 further improves the performance .","different, distillation, soft, hard"
74,"Convnet teacher, our image transformer DeiT learned from labels only . Does it inherit existing inductive bias that would facilitate the training?","inductive, bias, deit, transformer"
75,"our distilled model is more correlated to the convnet than with a transformer learned from scratch . Unsurprisingly, the joint class+distil classifier offers a middle ground .","distillation, transformer, embedding"
83,"DeiT is slightly below EfficientNet, which shows that we have almost closed the gap between vision transformers and convnets when training with Imagenet only . This results are a major improvement over previous ViT models trained on Imagenet1k .","vitm, vit, deit, imagenet"
87,"Table 5: Throughput on and accuracy on Imagenet , Imagenet Real and Imagenet V2 matched frequency of DeiT and of several state-of-the-art convnets . The throughput is measured as the number of images that we can process per second on one 16GB V100 GPU .","frequency, imagenet, matched, real, v2, throughput"
93,DeiT perform very well on ImageNetit is important to evaluate them on other datasets with transfer learning in order to measure the power of generalization . Table 7 compares transfer learning results to those of ViT and state of the art convolutional architectures .,"imagenetit, learning, 7, table, transfer"
102,ules so that the network has been fed a comparable number of images in total . We rescale images to 224 x 224 to ensure that we have the same augmentation . The results are not as good as with Imagenet pre-training .,"image, network, cifar-10, imagenet"
113,"Auto-Augment , Rand-Ugment and random erasing improve results . For the two latter we use the timm customizations . After ablation we choose Rand-Agment instead .","rand-augment, auto-augment, rand-a"
117,"Regularization & Optimizers. We have considered different optimizers and cross-validated different learning rates and weight decays . We scale the learning rate according to the lr batchsize, similar to Goyal et batch size .","optimization, weight, decay"
119,"We have employed stochastic depth , which facilitates the convergence of transformers, especially deep ones . For vision transformer, they were first adopted in the training procedure by Wightman .","depth, augmentation, stochastic"
125,"Bilinear interpolation of a vector from its neighbors reduces its l2-norm compared to its neighbors . We adopt a bicubic interpolations that approximately preserves the norm of the vectors, before fine-tuning the network .","bilinear, transformers, mentation, network, interpolation"
127,"A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B . A similar training with a RegNetY-16GF is 20% slower . Then, optionally fine-tune the model at a larger resolution .","fixdeit-b, augmentation, model, training, time, larger, repeated"
132,"Convolutional neural networks have optimized, both in terms of architecture and optimization during almost a decade . We have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token .","token, augmentation, distillation, novel, data"
136,"Ross Wightman shared his ViT code and bootstrapping training method with the community . Thanks to Vinicius Reis, Mannat Singh, Ari Morcos, Mark Tygert, Gabriel Synnaeve .","training, method, bootstrapping, vit, code"
139,"An image is worth 16x16 words: Transformers for image recognition at scale . ArXiv preprint arxiv:2006.00555, 2020 . Jie Hu andLi Shen and Gang Sun. Squeeze-and-excitation networks .","imagenet, image, image-text, noise, quantization, representation"
141,"Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. How to start training: The effect of initialization and architecture . In Conference on Computer Vision and Pattern Recognition, June 2016. Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyu","image, vision, representations, 3d, visional, networks, classification"
143,"In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2008. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, and V. Shankar. Do imagenet","nips, tasks, image, vision, representations, vision-and-language"
145,"Xin Chen, Xiaopeng Zhang, and Qi Tian. Circumventing outliers of autoaugment with knowledge distillation . European Conference on Computer Vision, 2020 . Bichen Wu, Chenfeng Xu, Alvin Wan, Peizhao Zhang,","image, vision, computer, representation"
