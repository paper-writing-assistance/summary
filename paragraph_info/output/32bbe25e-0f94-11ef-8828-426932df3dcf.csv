element_idx,keywords,summarized_text
6,"probabilistic, ddpms, generation, implicit, models, image","Denoising diffusion implicit models have achieved high quality image generation without adversarial training . In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process . These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models"
8,"generation, gans, quality, models, image","Deep generative models have demonstrated the ability to produce high quality samples in many domains . In terms of image generation, generative adversarial networks currently exhibits higher sample quality than likelihood-based methods such as variational autoencoders , autoregressive models and normalizing flows ."
9,"denoise, markov, networks, score, generative, conditional, chain, models, noise","Recent works on iterative generative models have demonstrated the ability to produce samples comparable to that of GANs . To achieve this, many denoising autoencoding models are trained to denoise samples corrupted by various levels of Gaussian noise . Samples are then produced by a Markov chain"
10,"2080, nvidia, ddpm, gpu, ti, n","DDPMs require many iterations to produce a high quality sample . For example, it takes 20 hours to sample 50k images of size 32 x 32 . This takes less than a minute to do SO from a GAN ."
17,"markov, generative, markovian, ddpm, chain, reverse, diffusion","In Section 3, we generalize the forward diffusion process used by DDPMs, which is Markovian, to non-Markovian ones . We can choose from a large family of generative models using the same neural network ."
18,"ddims, image, ddpms, ddim, ddpm, interpolation","In Section 5, we demonstrate several empirical benefits of DDIMs over DDPMs . first, we accelerate sampling by 10x to 100x using our proposed method . second, we can perform semantically meaningful image interpolation by manipulating the initial latent variable ."
22,"gaussian, markov, ddpm, transitions, chain, cha","DDPMs are learned with a fixed inference procedure q . For example, Ho et al. considered the following Markov chain with Gaussian transitions parameterized by a decreasing sequence 1:T E (0, 1]T:T ."
23,"covariance, observation, matrix, noise",Latent variable model po is a Markov chain that samples from XT to x0 . This is called the forward process due to the autoregressive nature of the sampling procedure .
29,"networks, score, trained, model, generation, conditional, performance, noise","EA := et)t=1 is a set of T functions . Each 93 : X  X is an function with trainable parameters 0, and 2 : = a vector of positive coefficients in the objective that depends on 1:T . This is also"
30,"gaussian, t, ddpm, large, process, reverse","The length T of the forward process is an important hyperparameter in DDPMs . From a variational perspective, a large T allows the reverse process to be close to a Gaussian . This motivates the choice of large T values, such as T = 1000 in Ho et al."
32,"inference, generative, distributions, model, ddpm, process","We need to rethink the inference process in order to reduce the number of iterations required by the generative model . Our key observation is that the DDPM objective in the form of Ly only depends on the marginals2 q, but not directly on the joint q . These non-Markovian"
40,"forward, process, gaussian","the forward process here is no longer Markovian . The magnitude of 0 controls the how stochastic it is . When 0  0, we reach an extreme case where as long as we observe x0 and xo for some t, then Xt-1 become known and fixed ."
49,"variational, bound, ly, lower, model, ea","The variational objective Ly is special in the sense that if parameters 0 of the models 93 are not shared across different t, then the optimal solution for EA will not depend on the weights 2 . On the one hand, this justified the use of L1 as a surrogate objective function for the variation"
51,"inference, generative, model, markovian, ddpm, process",We are learning a generative process for the Markovian inference process considered in Sohl-Dickstein and Ho et al. We can essentially use pretrained DDPM models as the solutions to the new objectives .
61,"pronounced, objective, pro, model, ddim, ddpm","the forward process becomes deterministic given Xt-1 and x0 . In the generative process, the coefficient before the random noise Et becomes zero . The resulting model becomes an implicit probabilistic model trained with the DDPM objective ."
63,"generative, process, reverse","the generative process is considered as the approximation to the reverse process . since of the forward process has T steps, we may also consider forward processes with lengths smaller than T, which accelerates the corresponding generative processes ."
64,"forward, process, trajectory","generative process now samples latent variables according to reversed, which we term trajectory . When the length of the sampling trajectory is much smaller than T, we may achieve significant increases in computational efficiency due to its iterative nature ."
73,"xt, generation, ddim, ddpm, process","this suggests that unlike DDPM, we can use DDIM to obtain encodings of the observations . This suggests that with enough discretization steps, the we can also reverse the generation process, which encodes xo to XT and simulates the reverse of the ODE ."
79,"stochastic, generation, ddim, ddpm, sampling, image","In this section, we show that DDIMs outperform DDPMs in terms of image generation when fewer iterations are considered . This gives speeds ups of 10x to 100x over the original dPM generation process . DDims can also be used to encode samples that reconstruct them from"
80,"ddim, ddpm, d","For each dataset, we use the same trained model with T = 1000 . The objective is Ly from Eq with 2 = 1 ."
82,"ddim, ddpm, d",7 E R0 is a hyperparameter that we can directly control . This includes an original DDPM generative process when 7 = 1 . We also consider DDIM where the random noise has a larger standard deviation than .
90,"ddim, ddpm, d","In Table 1, we report the quality of the generated samples with models trained on CIFAR10 and CelebA, as measured by Frechet Inception Distance . As expected, the sample quality becomes higher as we increase dim, presenting a tradeoff between sample quality and computational costs ."
91,"celeba, cifar10","In Figure 3, we show CIFAR10 and CelebA samples with the same number of sampling steps and varying 0. For the DDPM, the sample quality deteriorates rapidly when the sampling trajectory has 10 steps . This explains why the FID scores are much worse than other methods ."
92,"steps, comparable, to, ddim, 1000, quality","In Figure 4, we show that the amount of time needed to produce a sample scales linearly with the length of the sample trajectory . DDIM is able to produce samples with quality comparable to 1000 step models within 20 to 100 steps, which is a 10x to 50x speed up compared to the original DD"
94,"ddim, trajectory, gen, generative","In Figure 5, we observe the generated images under different generative trajectories while starting with the same initial XT . Most high-level features are similar, regardless of the generative trajectory . In many cases, samples generated with only 20 steps are already very similar to ones generated with 1000 steps in terms of high-"
110,"gans, gan, interpolation","In Figure 6, we show that simple interpolations in XT can lead to  semantically meaningful interpolation between two samples . This allows DDIM to control the generated images on a high level directly through the latent variables ."
112,"ddims, nature, stochastic, ddim, ddpm, encoding, error",We consider encoding and decoding on the CIFAR-10 test set . We report the per-dimension mean squared error in Table 2 . DDIMs have lower reconstruction error for larger S values .
114,"markov, ddpms, networks, score, conditional, chains, noise","Our work is based on a large family of existing methods on learning generative models as transition operators of Markov chains . Among them, denoising diffusion probabilistic models ) and noise conditional score networks ) have recently achieved high sample quality comparable to GANs ."
115,"ncsn, noise, methods, levels, dynamics, langevin",Langevin dynamics is a discretization of a gradient flow . Both DDPM and NCSN require many steps to achieve good sample quality . This aligns with the observation that existing NCSN methods have trouble generating high-quality samples .
116,"ddim, gan, gans, dynamics, langevin",DDIM is an implicit generative model where samples are uniquely determined from the latent variables . We derive the model from a purely variational perspective . This could partially explain why we are able to observe superior sample quality compared to DDPM under fewer iterations.
122,"appendix, a, ncsn, process, quality",the non-Markovian forward process seems to suggest continuous forward processes other than Gaussian . We also demonstrated a discrete case with a multinomial forward process in Appendix A .
123,"multi-step, model, ddim, implicit, method",the sampling procedure of DDIMs is similar to that of an ODE . It would be interesting to see if methods that decrease the discretization error in ODEs could be helpful for further improving sample quality in fewer steps .
240,"al., (2020), hyperparameter, et, hyperparam, ho, bt9, diffusion","In this paper, we have used the notation at to represent the variable t in Ho et al. for three reasons: First, it makes it more clear that we only need to choose one set of hyperparameters . second, it allows us to introduce the generalization as well as the acceleration case easier, because"
254,"lsun, church, ddpm, celeba, dataset, image","We consider 4 image datasets with various resolutions . For each dataset, we set the hyperparameters a according to the heuristic in . We use the same model for each dataset ."
255,"celeba, celeb, celeba-hq","Our architecture for 33 follows that in Ho et al., which is a U-Net based on a Wide ResNet ."
