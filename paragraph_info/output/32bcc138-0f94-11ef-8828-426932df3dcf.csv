element_idx,keywords,summarized_text
4,"answer-, ing, dataset, model, regression, question, logistic","The Stanford Question Answering Dataset consists of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles . The answer to each question is a segment of text from the corresponding reading passage . We analyze the dataset to understand the types of reasoning required to answer the questions ."
6,"under, precipitation, natural, rc, language, gravity","Reading Comprehension, or the ability to read text and answer questions about it, is a challenging task for machines . Consider the question ""what causes precipitation to fall?"" posed on the passage in Figure 1 ."
8,"water, sleet, vapor, precipitation, atmospheric, s, snow","In meteorology, precipitation is any product of condensation of atmospheric water vapor that falls under gravity . The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail . Short, intense periods of rain in scattered locations are called ""showers"""
13,"imagenet, object, rc, recognition, for, datasets",Existing datasets for RC have one of two shortcomings: those that are high in quality are too small for training modern data-intensive models . those large are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions .
16,"question-answering, question, ford, answering, dataset, v1.0","ford Question Answering Dataset v1.0 contains 107,785 question-answer pairs on 536 articles . The answer to every question is a segment of text, or span, from the corresponding reading passage . SQuAD is almost two orders of magnitude larger than previous manually labeled RC dataset"
17,"squad, answ, span, and","SQuAD does not provide a list of answer choices for each question . Rather, systems must select the answer from all possible spans in the passage . The span constraint comes with the important benefit that span-based answers are easier to evaluate than freeform answers ."
18,"model, regression, module, logistic, squad, network-based","Our best model achieves an F1 score of 51.0%,1 which is much better than the sliding window baseline . This results are still well behind human performance, which is 86.8% F1 based on interannotator agreement ."
24,"data-driven, approach, model, reading, regression, comprehension",Hirschman et al. curated a dataset of 600 real 3rd6th grade reading comprehension questions . Their pattern matching baseline was improved by a rule-based system and a logistic regression model .
27,"answering, open-domain, qa, question","The goal of open-domain QA is to answer a question from a large collection of documents . Recently, Yang et al. created the WikiQA dataset, which uses Wikipedia passages as a source of answers ."
28,"extraction, answer","answer extraction typically exploits the fact that the answer occurs in multiple documents, which is more lenient than in our setting . a system only has access to a single reading passage ."
29,"questions, cloze, style, datasets","Recently, researchers have constructed cloze datasets in which the goal is to predict the missing word in a passage . The Children's Book Test involves predicting a blanked-out word given the 20 previous sentences ."
32,"gas, oxidizing, oxygen, dioxygen, agent","Oxygen is a chemical element with symbol O and atomic number 8 . By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium . At standard temperature and pressure, two atoms of the element bind to form dioxygen ."
40,"curation, pasage, glish, cles, arti-, quality, en-","To retrieve high-quality articles, we used Project Nayuki's Wikipedia's internal PageRanks . From each article, we extracted individual paragraphs, stripping away images, figures, tables, and discarding paragraphs shorter than 500 characters . The result was 23,215 paragraphs for the 536 articles covering"
42,"mechanical, daemo, question-answer, turk, ama-zon, platform, collection","Crowdworkers were required to have a 97% HIT acceptance rate, a minimum of 1000 HITs . The task was reviewed favorably by crowdworkers, receiving positive comments on Turkopticon."
43,"functionality, work-ers, copy-paste, questions, answering, crowdworker","On each paragraph, crowdworkers were tasked with asking and answering up to 5 questions on the content of that paragraph . The questions had to be entered in a text field, and the answers needed to be highlighted . On the interface, this was reinforced by a reminder prompt ."
44,"questions, answers, marked, additional, collection, unanswerable","In the secondary answer generation task, each crowdworker was shown only the questions along with the paragraphs of an article . If a question was not answered by a span in the paragraph, workers were asked to submit the question without marking an answer . Over the development and test sets, 2.6% of questions were marked unans"
49,"ner, number, numerical, tags, diversity, answers","We first separate the numerical and non-numerical answers . The correct noun phrases are further split into person, location and other entities using NER tags . In Table 2, we can see dates and other numbers make up 19.8% of the data ."
50,answork,We sampled 4 questions from each of the 48 articles in the development set . The results show that all examples have some sort of lexical or syntactic divergence .
57,"lexical, variation, divergence, syntactic",We measure the edit distance between these two paths . The syntactic divergence is then defined as the minimum edit distance over all possible anchors . We also show a concrete example where edit distance is 0 .
67,"candidate, correct, shortest, answer, constituent","77.3% of the correct answers in the development set are constituents . This puts an effective ceiling on the accuracy of our methods . During training, when the correct answer of an example is not a constituent, we use the shortest constituent ."
73,"bigram, root, span, word, features, length, match, frequencies",the matching word and bigram frequencies and root match features help the model pick the correct sentences . Length features bias the model towards picking common lengths and positions for answer spans . Constituent label and span POS tag features guide the model toward the correct answer types .
84,"test, human, performance, set, squad","resulting human performance score on the test set is 77.0% for the exact match metric, and 86.8% for F1. Mismatch occurs mostly due to inclusion/exclusion of non-essential phrases rather than fundamental disagreements about the answer ."
92,"tree, model, dependency, features, regression, path, l2, logistic, regularization","In order to understand the features that are responsible for the performance of the logistic regression model, we perform a feature ablation where we remove one group of features from our model at a time . The results indicate that lexicalized and dependency tree path features are most important ."
98,"answer, types","the model performs best on dates and other numbers, categories for which there are usually only a few plausible candidates, and most answers are single tokens . The model is challenged more on other named entities because there are many more plausible candidates ."
100,"divergence, model, syntactic, regression, logistic, dataset","Figure 5 shows that the more divergence there is, the lower the performance of the logistic regression model . This suggests deep understanding is not distracted by superficial differences . Measuring the degree of degradation could therefore be useful in determining the extent to which a model is generalizing in the right way ."
102,"squad, natural, understanding, language","SQuAD features a large reading comprehension dataset on Wikipedia articles . The performance of our logistic regression model, with 51.0% F1, against the human F1 of 86.8% suggests ample opportunity for improvement . Since the release of our dataset, we have already seen considerable interest in building models on this dataset ."
108,"cnn, cvpr, askmsr",A thorough examination of the AskMSR question-answering system . In Association for Computational Linguistics . P. Clark and 0. Etzioni. 2016. My computer is an honor student but how intelligent is it? standardized tests as measure of AI .
109,"entraile, answer","Computational Linguistics, 19:313-330. K. Narasimhan and R. Barzilay. 1993. Building a large annotated corpus of English: the Penn Treebank . Computationals, 19, pages 133-132. D. Ravichandran and E. Ho"
