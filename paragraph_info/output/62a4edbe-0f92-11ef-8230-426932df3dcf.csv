element_idx,summarized_text,keywords
4,"This paper presents a new vision Transformer, called Swin Transformer, that serves as a general-purpose backbone for computer vision . Challenges arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text . To address these","swin, transformer, vision, backbone"
12,"Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data . Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision .","network, architecture, vision"
16,"Unlike the word tokens that serve as the basic elements of processing in language Transformers, visual elements can vary substantially in scale . One difference is the much higher resolution of pixels in images compared to words in passages of text . To overcome these issues, we propose a generalpurpose Transformer backbone, called Swin Transformer","swin, visual, vision, backbone, transformer, domain"
17,"Swin Transformer shifts the window partition between consecutive self-attention layers . The shifted windows bridge the windows of the preceding layer, providing connections between them that significantly enhance modeling power . This strategy is also efficient in regards to real-world latency: all query patches within a window share the same key set1 which facilitate","swin, software, hardware, shift, transformer"
22,"Swin Transformer surpasses the previous state-of-the-art results by +2.7 box AP and +2.6 mask AP . On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set .","segmentation, image, imagenet-1k, semantic, classification"
23,Swin Transformer's strong performance on various vision problems can drive this belief deeper in the community . It is our belief that a unified architecture across computer vision and language processing could benefit both fields .,"swin, natural, language, vision, processing, transformer"
25,"CNN and variants CNNs serve as the standard network model throughout computer vision . Since then, deeper and more effective convolutional neural architectures have been proposed to further propel the deep learning wave .","variants, cnn"
28,"HRNet , and EfficientNet . In addition to these architectural advances, there has also been much work on improving individual convolution layers, such as depthwise convolution .","hrnet, vision, shift, cnn, computer"
29,Self-attention based backbone architectures Inspired by the success of self-attention layers and Transformer architectures in the NLP field . These works are computed within a local window of each pixel to expedite optimization . They achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture,"self-attention, layer, architecture, backbone"
30,Self-attention/Transformers to complement CNNs Another line of work is to augment a standard CNN architecture with self-attention layers or Transformers . Our work explores the adaptation of Transformers for basic visual feature extraction and is complementary to these works .,"self-attention, backbones, layer, cnn"
31,ViT directly applies a Transformer architecture on nonoverlapping medium-sized image patches . It achieves an impressive speed-accuracy tradeoff on image classification compared to convolutional networks . DeiT introduces several training strategies that allow ViT to be effective using the smaller ImageNet-1K dataset .,"image, vision, backbones, transformer, classification"
32,"We find our Swin Transformer architecture to achieve the best speedaccuracy trade-off among these methods on image classification . Its complexity is still quadratic to image size, while ours is linear and also operates locally which has proven beneficial in modeling the high correlation in visual signals .","swin, multi-resolution, feature, image, architecture, maps, transformer, classification"
35,"Swin Transformer splits input RGB image into non-overlapping patches by a patch splitting module . Each patch is treated as a ""token"" and the feature dimension of each patch is 4 x 4 = 48 .","swin, pixel, raw, architecture, transformer, rgb"
37,The first patch merging layer concatenates the features of each group of 2 x 2 neighboring patches . This reduces the number of tokens by a multiple of2x 2 = 4 . The output dimension is set to 2C .,"olution, patch, representation, layer, merging, hierarchical"
42,"Swin Transformer block is built by replacing the standard multi-head self attention module in a Transformer block by a module based on shifted windows . A LayerNorm layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.","swin, window, msa, transformer, shifted"
44,Transformer architecture and its adaptation for image classification conduct global selfattention . The global computation leads to quadratic complexity with respect to the number of tokens .,"token, image, architecture, transformer, classification"
48,"Shifted window partitioning in successive blocks The window-based self-attention module lacks connections across windows, which limits its modeling power . To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted Window partitioning approach which alternates between two partitioning configurations in consecutive","swin, shifted, window, partitioning, transformer, block"
49,the first module uses a regular window partitioning strategy which starts from the top-left pixel . The 8 x 8 feature map is evenly partitioned into 2 x 2 windows of size 4 x 4 . Next module adopts a windowing configuration that is shifted from the preceding layer .,"module, window, first, partitioning, strategy"
59,"Efficient batch computation for shifted configuration An issue with shifted window partitioning is that it will result in more windows . Some of the smaller windows will be smaller than M x M4  . Here, we propose a more efficient batch computation approach by cyclic-shifting toward the top-left direction","batch, configuration, shifted, computation"
66,"We build our base model, called Swin-B, to have of model size and computation complexity similar to ViTB/DeiT-B . We also introduce Swin t, Swin S and Swin L, which are versions of about 0.25x, 0.5x and 2x the model size . The","swin, swin-l, resnet-101, swin-s, swin-b"
75,"In ImageNet-1K fine-tuning, we train models for 30 epochs with a batch size of 1024, an initial learning rate of 0.001 and a weight decay of 0.01 . We also train on the larger ImageNet-22K dataset, which contains 14.2 million images and 22K","fine-tuning, scheduler, learning, imagenet-1k, linear, rate, decay"
79,"ImageNet-22K pre-training brings 1.8%1.9% gains over training on ImageNet-1K from scratch . The larger Swin-L model achieves 87.3% top-1 accuracy, +0.9% better than that of ViT with similar inference throughput .","larger-capacity, imagenet-22k, swin-l, model"
86,"COCO 2017 contains 118K training, 5K validation and 20K test-dev images . An ablation study is performed using the validation set . For this study, we consider four typical object detection frameworks . We adopt an improved HTC with instaboost , stronger multi-scale training .","ablation, system-level, study, detection, object, comparison"
96,Swin Transformer and ResNet are directly applicable to all the above frameworks . The comparisons are conducted by changing only the backbones with other settings unchanged .,"layers, decon, resne(x)t, deit, layer, deconvolution"
101,"Swin Transformer achieves a high detection accuracy of 51.9 box AP and 45.0 mask AP . On a higher baseline, the gains by Swin transformer are also high . ResNet is built by highly optimized Cudnn functions . A thorough kernel optimization is beyond the scope of this paper .","resnext101-64x4d, mask, resne, cascade, r-cnn"
110,"Results Table 3 lists the mloU, model size , FLOPs and FPS for different method/backbone pairs . Our Swin-L model with ImageNet-22K pre-training achieves 53.5 mIoU on the val set, surpassing the previous best model by +3.2 ","size, model, mlou, swin-l, swin-s, imagenet-22k"
113,Shifted windows Ablations of the shifted window approach on the three tasks are reported in Table 4 . The results indicate the effectiveness of using shifted windows to build connections between windows in the preceding layers .,"coun-, window, building, terpart, shifted"
117,"COCO, +2.3/+2.9 mloU on ADE20K in relation to those without position encoding . Also note that while the inclusion of absolute position embedding improves image classification accuracy, it harms object detection .","position, segmentation, image, accuracy, absolute, semantic, embedding, classification"
120,"the self-attention modules built on the proposed shifted window approach are 40.8x/2.5x, 20.2x/2.4x, 9.3 x/2.1 x, and 7.6x/1.8x more efficient than sliding windows . Overall, the Swin Transformer architectures built on shifted windows are 4.1/1.5","swin, windows, architecture, transformer, shifted"
132,"the detailed architecture specifications are shown in Table 7 . ""Concat n x n"" indicates a concatenation of n neighboring features . This operation results in a downsampling of the feature map .","architecture, specifications"
137,"AdamW optimizer for 300 epochs using a COsine decay learning rate scheduler . A batch size of 1024, an initial learning rate of 0.001, a weight decay of 0.05, and gradient clipping with a max norm of 1 are used . We include most of the augmentation","sine, depth, co-, stochastic, adamw, decay"
139,"ImageNet-22K training is done in two stages . For the first stage with 2242 input, we employ an AdamW optimizer for 90 epochs using a linear decay learning rate scheduler . A batch size of 4096, an initial learning rate of 0.001 and a weight decay of 0.01","imagenet-22k, dataset"
141,"for an ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN , ATSS , RepPoints v2 , and Sparse RCNN in mmdetection .","cascade, mask, r-cnn"
142,"HTC with instaboost , stronger multi-scale training , 6x schedule , softNMS , and an extra global self-attention layer . ImageNet-22K pre-trained ImageNet 22K .","htc++, htc+, imagenet-22k"
149,"In training, we employ the AdamW optimizer with an initial learning rate of 6 x 10-5 . Models are trained on 8 GPUs with 2 images per GPU for 160K iterations . For augmentations, we adopt the default setting in mmsegmentation of random horizontal flipping, random","optimizer, swin, model, learning, linear, adamw, rate, transformer"
159,Table 9 compares the AdamW and SGD optimizers of ResNet backbones on COCO object detection . The Cascade Mask R-CNN framework is used in this comparison .,"optimizer, swin, adamw, architecture, transformer"
167,"Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention augmented convolutional networks, 2020. 3 Alexey Bochkovskiy, Chien- Yao Wang, and HongYuan Mark Liao.","conference, vision, on, international, quality, works, detection, object, ieee, computer, net-"
168,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 213-229. Springer, 2020. 3, 6, 9 Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yuiong, Xiaoxiao Li, Shuyang Sun, Wansen F","image, segmentation, vision, semantic, computer"
171,"Spinenet: Learning scale-permuted backbone for recognition and localization . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11592-11601, 2020 . 6, 9 Jun Fu, Jing Liu, Haijie Tian, Yong Li, yong","repetition, conference, segmentation, vision, recognition, on, and, pattern, international, instance, backbone, ieee/cvf, computer"
173,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017. 1, 2 Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computers vision, pages 646-661. Spring","visual, natural, language, vision, models, supervision, computer"
175,"In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1042810436, 2020 . 6 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Li","visual, image, vision, recognition, large-scale"
176,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario","visual, conference, transformers, vision, recognition, on, and, pattern, ieee, computer"
179,"Xiao Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features . In Proceedings of the IEEE/C","ade20k, dataset"
