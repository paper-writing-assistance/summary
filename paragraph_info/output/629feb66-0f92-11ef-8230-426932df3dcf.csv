element_idx,summarized_text,keywords
9,"Pyramid Vision Transformer is a ""columnar"" structure specifically designed for image classification . Many CNN backbones use a pyramid structure for dense prediction tasks such as object detection .","structure, vision, pyramid, columnar, transformer, cnn"
11,"Unlike the recentlyproposed Vision Transformer, we introduce the Pyramid Vision Transformer . PVT has several merits compared to current state of the arts . Different from ViT that typically yields lowresolution outputs .","convolutional, neural, vision, network, backbone, transformer, networks"
13,"ous vision tasks without convolutions can be used as a direct replacement for CNN backbones . We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks . For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP","tasks, vision, backbone, pvt, cnn"
21,"researchers have explored its application in computer vision . For example, some works model the vision task as a dictionary lookup problem with learnable queries . Others use the Transformer decoder as an task-specific head on top of the CNN backbone .","natural, language, vision, processing, transformer, computer"
22,Dosovitskiy et al. introduced the Vision Transformer for image classification . This is an interesting and meaningful attempt to replace the CNN backbone with a convolution-free model .,"image, vision, cnn, transformer, classification"
25,"Pyramid Vision Transformer can serve as an alternative to the CNN backbone in many downstream tasks, including image-level prediction and pixel-level dense predictions . Our PVT overcomes the difficulties of the conventional Transformer by taking fine-grained image patches as input to learn high-resolution representation, which is essential for dense prediction tasks ","image-level, vision, prediction, pyramid, pvt, transformer"
26,"PVT always produces a global receptive field, which is more suitable for detection and segmentation . To our knowledge, this is the first entirely convolution-free object detection pipeline .","pipeline, structure, convolution-free, backbones, pyramid, struc, cnn"
30,PVT-Small achieves 40.4 AP on COCO val201 7 . We compare it with popular ResNets and ResNeXts .,"image, segmentation, resnexts, semantic, resnet, classification"
34,"The standard CNN was first introduced in to distinguish handwritten numbers . The model contains convolutional kernels with a certain receptive field that captures favorable visual context . To provide translation equivariance, the weights of convolutionals are shared over the entire image space .","visual, neural, recognition, deep, networks, cnn"
38,CNNs have become the dominant framework for object detection . Most of these popular object detectors are built on high-resolution or multi-scale feature maps .,"learning, deep, detector, detection, object, cnn, multi-stage"
40,"Semantic Segmentation introduced a fully convolutional architecture to generate a spatial segmentation map for a given image of any size . After that, the deconvolution operation was introduced by Noh et al.","image, segmentation, semantic, fpn, cnn"
42,The non-local block attempts to model long-range dependencies in space and time . Criss-cross further reduces the complexity by generating sparse attention maps through a crisscross path .,"filter, convolutional, dynamic, self-attention, layer, cnn"
48,"Our goal is to introduce the pyramid structure into the Transformer framework, SO that it can generate multi-scale feature maps for dense prediction tasks . All stages share a similar architecture, which consists of a patch embedding layer and Li Transformer encoder layers.","framework, pvt, pyramid, structure"
49,"In the first stage, given an input image of size HxW x3, HW we first divide it into patches,2 each of size 4x4x 3 . After that, 42 the embedded patches along with a position embedding are passed through a Transformer encoder with L1 layers . In the same","f4, f1"
54,"In stage 2, we divide the input feaHi-1 Wi-1 ture map Fi-1 E RHi-1XWi-1XCi-1 into patches . Each patch is flatten and projected to a Ci-dimensional embedding .","wi-1, ci, hi-1, embedding"
63,input sequence x to a sequence of size x R2 Ws E RXCi is a linear projection that reduces the dimension of the input sequence to Ci. Norm refers to layer normalization .,"stage, reshape, layer, attention, reshape(x, reshape(x)"
72,"The most related work to our model is ViT . First, both PVT and ViT are pure Transformer models without convolutions . The primary difference between them is the pyramid structure .","multi-scale, feature, vit, maps, pvt"
73,Our PVT breaks the routine of Transformer by introducing a progressive shrinking pyramid . It can generate multi-scale feature maps like a traditional CNN backbone . We also designed a simple but effective attention layer-SRA to process high-resolution feature maps .,"multi-scale, feature, computational/memory, costs, pvt, maps, multiscale, cnn, bone, back-"
83,RetinaNet is a widely used single-stage detector . Mask R-CNN is the most popular two-stage instance segmentation framework . Semantic FPN is the vanilla semantic segmentation method .,"model, representative, method, prediction, pvt, dense"
84,"We initialize the PVT backbone with the weights pre-trained on ImageNet . We use the output feature pyramid F1, F2, F3, F4 as the input of FPN . When training the detection/segmentation model, none of the layers in PVT are frozen .","detection/segmentation, pvt, model, imagenet"
89,"Image classification experiments are performed on the ImageNet 2012 dataset . All models are trained on the training set, and report the top-1 error on the validation set . We follow DeiT and apply random cropping, random horizontal flipping , label-smoothing regularization , mixup , CutMix ","validation, imagenet, backbones, 2012, setting, cnn"
92,"the GFLOPs are roughly similar, the top-1 error of PVTSmall reaches 20.2, which is 1.3 points higher than that of ResNet50 . Meanwhile, PVT models archive performances comparable to the recently proposed Transformer-based models, such as ViT and DeiT -Base/16: 18.","pvt-base/16, pvt, gflop, pvt-large"
95,Settings. Object detection experiments are conducted on the challenging COCO benchmark . All models are trained on COCO train2017 and evaluated on val201 7 .,"val201, benchmark, 7, coco, backbone, pvt"
96,"Our models are trained with a batch size of 16 on 8 V100 GPUs and optimized by AdamW with an initial learning rate of 1 x 10-4 . The training image is resized to have a shorter side of 800 pixels, while the longer side does not exceed 1,333 pixels.","adamw, model, detection, training"
97,"PVT-based models significantly surpasses their counterparts . For example, with the 1 x training schedule, the AP of PVT is 4.9 points better than ResNet18 .","pvt-based, models, detection, backbone, object, retinanet, cnn"
98,"Similar results are found in instance segmentation experiments based on Mask R-CNN . The best mask APm obtained by PVT-Tiny is 40.7, which is 1.0 points higher than ResNeXt101-64x4d .","segmentation, experiments, instance, pvt-large"
100,"ADE20K contains 150 fine-grained semantic categories, with 20,210, 2,000, and 3,352 images for training, validation, and testing respectively . In the training phase, the backbone is initialized with the weights pre-trained on ImageNet .","segmentation, semantic, ade20k, backbone, pvt"
108,our PVT-Tiny/Small/Medium are at least 2.8 points higher than ResNet-18/50/101 . The mIoU of 44.8 is close to the state-of-the-art performance of the ADE20K benchmark .,"pvt-large, segmentation, semantic, backbone, cnn"
113,We train models on COCO train2017 for 50 epochs with an initial learning rate of 1 x 10-4 . The learning rate is divided by 10 at the 33rd . We use random flipping and multi-scale training as data augmentation .,"pvt-based, head-detr, coco, val2017, detection, pvt"
119,"Trans2Seg achieves 42.6 mIoU, outperforming ResNet50-d8+DeeplabV3+ . In addition, our method has only 31.6 GFLOPs, which is 4 times fewer .","segmentation, resnet50-d8+trans2seg, trans2seg, head, transformer, deeplabv3+"
127,ViT can process high-resolution feature maps in shallow stages . This results in a promising AP of 40.4 on COCO val2017 .,"mance, coco, pvt, val2017"
132,"In Table 9, we multiply the hidden dimensions C1, C2, C3, C4 of PVT-Small by a scale factor 1.4 to make it have an equivalent parameter number to the deep model . Therefore, going deeper is more effective than going wider in PVT design .","deep, model, wide"
133,"Most dense prediction models rely on the backbone whose weights are pre-trained on ImageNet . In the top of Figure 5, we plot the validation AP curves of RetinaNet-PVT-Small . We find that the model w/ pretrained weights converges","pvt, weights, pre-training, imagenet"
134,"PVT vs. ""CNN w/ Non-Local"" Compare the performance of our PVT and GCNet . We compare the performance using Mask R-CNN for instance segmentation . There are two possible reasons for this result:","g, gcnet, gflop"
135,a single global attention layer can acquire global-receptive-field features . This indicates that stacking multiple MHAs can further enhance representation capabilities offeatures .,"multi-head, multihead, layer, attention, pvt"
139,"Regular convolutions can be deemed as special instantiations of spatial attention mechanisms . For different inputs, the weights of the convolution are fixed, but the attention weights change dynamically with the input . Thus, the features learned by the pure Transformer backbone full of MHA layers, could be more flexible and","mechanisms, attention, regular, convolutions"
141,"On COCO, the shorter side of the input image is 800 pixels . The inference speed of RetinaNet based on PVT-Small is slower than the ResNet50based model .","input, pvt-small, scale, self-attention, layer"
142,"In Figure 7, we present some qualitative object detection and instance segmentation results on COCO val201 7 . These results indicate that a pure Transformer backbone without convolutions can also be easily plugged in dense prediction models .","val201, segmentation, coco, 7, semantic, detection"
148,PVT can serve as an alternative to CNN backbones . There are still some specific modules and operations designed for CNNs and not considered in this work .,"model, back-bones, pvt, cnn, transformer-based"
152,"Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional networks . In Proc. IEEE Int. Conf. Comp. Vis., 2019. 3 Yue Cao, Jiarui Xu","image, segmentation, recognition, network, semantic, attention"
155,"IEEE Trans. Pattern Anal. Mach. Intell., 2021. 11 Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp","shao, intell., mach., trans., ling, pattern, architecture, backbone, transformer, ieee, anal."
157,"IEEE Int. Conf. Comp. Vis., 2017. 1, 2, 3, 6, 7, 8, 10, 12 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition .","neural, image, recognition, deep, transformer, networks"
159,"In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017. 3 Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Efficient 3D point cloud feature learning for large-scale place","convolutional, estimation, networks, neural, recog-, qua, deep, large-scale, place, quality, nition"
160,"In Proc. Eur. Conf. Comp. Vis., 2014. 2, 7, 9, 10, 12 Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Autodeeplab: Hier","large, visual, image, scale, recognition"
161,"Pvtv2: Improved baselines with pyramid vision transformer . ArXiv preprint arxiv:2012.05780, 2020 . 3 Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Ta","convolutional, visual, neural, vision, networks"
163,"Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks . In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020 . 2, 9, 10 Enze xie, Peize","transformations, image, aggregated, transformers, vision, recognition, residual"
