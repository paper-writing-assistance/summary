element_idx,keywords,summarized_text
7,"q-learning, theory, convergence","We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states . We also sketch extensions to cases of non-discounted, but absorbing environments ."
11,"state, differences, td, q-learning, temporal","Q-learning is a primitive form of learning, but can operate as the basis of far more sophisticated devices . Examples of its use include Barto and Singh , Sutton , Chapman and Kaelbling , Mahadevan and Connell , and Lin ."
12,"q-learning, converges","This paper presents the proof outlined by Watkins that Q-learning converges . Section 2 describes the problem, the method, and the notation, section 3 gives an overview of the proof and section 4 discusses two extensions . Formal details are left as far as possible to the appendix ."
17,"probabilistic, reward, world, collection, finite, agent","The world constitutes a controlled Markov process with the agent as a controller . At step n, the agent is equipped to register the state Xn of the world, an can choose its action an 1 accordingly ."
21,"x, dynamic, learning, state, q, programming",DP provides a number of methods for calculating V* and one . The task facing a Q learner is that of determining xy a without initially knowing these values .
27,"q, optimal, policy, values","Values for an optimal policy can be defined as Q*  Q * , Vx, a. * is an action at which V* = maxa Q* is attained . There may be more than one optimal policy or a ."
33,"theorem, states, and, way, actions, convergence","the sequence of episodes that forms the basis of learning must include an infinite number of episodes for each starting state and action . This may be considered a strong condition on the way states and actions are selected-however, under the stochastic conditions of the convergence theorem, no method could be guaranteed to find an optimal policy"
43,"appendix, a, arp, application","A state of the ARP, x, n, consists of a card number n, together with a state x from the real process . The actions permitted in the aRP are the same as those permitted in real process."
44,"a, match, arp, action","The next state of the ARP, given current state . The current state of ARP is given a state of current status ."
53,"valu, state, qn, value, arp","two lemmas form the heart of the proof . One shows that, effectively by construction, the optimal Q value for ARP state and action a is just Qn ."
69,"transition, expected, arp, converge, rewards, matrices","Probabilities P and expected rewards R in the ARP converge . This makes it appropriate to consider P rather than the transition matrices P, essentially ignoring the level at which the aRP enters state y."
89,"theorem, of, above, values, clarity",theorem proved above was somewhat restricted . Two particular extensions to the version of Q-learning described above have been used in practice . One is the non-discounted case . the other is to the case where many of the Q values are updated .
90,"x, absorbing, states, state, goal","A process with absorbing goal states has one or more states bound in the end to trap the agent . This ultimate certainty of being trapped plays the role that Y  1 played in the earlier proof, in ensuring that the value of state x under any policy , V , is bounded ."
97,"action, minor, modification, arp, replay, process, rewards","Changing more than one Q value on each iteration requires a minor modification to the action replay process ARP . As long as the stochastic convergence conditions in equation 3 are satisfied, the proof requires no non-trivial modification . The Qn values are still optimal for the modified ARP, and this still"
98,"q-learning, process, methods","the paper has presented an apparent dichotomy between Q-learning and methods based on certainty equivalence, such as Sato, Abe and Takeda . If the agent can remember the details of its learning episodes, then, after altering the learning rates, it can use each of them more than once"
99,"q-learning, algorithm","Theorem above only proves the convergence of restricted version of Watkins' comprehensive Q-learning algorithm . It does not permit updates based on rewards from more than one iteration . This addition was pioneered by Sutton in his TD algorithm, in which a reward from a step taken "
108,"actions, available, arp","In general, the set of available actions may differ from state to state . Here we assume it does not, to simplify the notation . The discount factor for the ARP will be taken to be Y ."
114,"arp, rnie, reward, action","If ie = 0, then the reward is set at Qo and the ARP absorbs, as above . Otherwise, taking an action causes a state transition to (ynie, nie - 1>) This last point is crucial . The discount factor is 7 the same as in the real process."
144,"spaces, action, and, state","the state and action spaces are finite, given 7, there exists some level n1 such that starting above there from any leads to a level above 1 with probability at least 1 - 7 . 7 can be chosen appropriately to set the overall probability of straying below 1 ."
163,"of, transition, markov, the, value, s-step, chain, matrices","Consider the s-step chain formed from the concatenation of thesei.e., starting from state x1 . move to state X2 according to Px1x2 . then state Y3 according P2x2x3 to, and SO on ."
175,"dynamic, asynchronous, learning, reinforcement, stochastic, programming","Sutton, R.S. . Learning to predict by the methods of temporal difference . PhD Thesis, University of Massachusetts, Amherst, MA. Werbos, C.J.C.H. Advanced forecasting methods for global crisis warning and models of intelligence ."
