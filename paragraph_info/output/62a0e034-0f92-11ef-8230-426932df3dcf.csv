element_idx,summarized_text,keywords
5,"Segment Anything project: a new task, model, and dataset for image segmentation . We built the largest segmentation dataset to date with over 1 billion masks on 11M licensed and privacy respecting images .","task, numerous, image, segmentation, loop, segment, data, collection, anything"
7,"Large language models pre-trained on web-scale datasets revolutionize NLP . These ""foundation models"" can generalize to tasks and data distributions . This capability is often implemented with prompt engineering .","nlp, text, hand-crafted"
9,"ALIGN and CLIP use contrastive learning to train text and image encoders that align the two modalities . Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions .","concept, visual, image, vision, generation, computer"
15,"We define a promptable segmentation task that is general enough to provide a powerful pretraining objective . This task requires a model that supports flexible prompting and can output segmentation masks in realtime . To train our model, we need a diverse, large-scale source of data .","task, segmentation, engine, masks, data"
16,"In NLP and computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using ""prompting"" techniques . We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering","task, model, segmentation, nlp, foundation"
17,"The promptable segmentation task and the goal of real-world use impose constraints on the model architecture . The model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use . We refer to this model as the Segment Anything Model, or SAM .","segmentation, task, sam"
18,"Data engine has three stages: assisted-manual, semi-automatic, and fully automatic . In the first stage, SAM assists annotators in annotating masks . We prompt SAM with a regular grid of foreground points .","segmentation, mask, engine, data, diversity, sam"
19,"SA-1B, collected fully automatically using the final stage of our data engine, has 400x more masks than any existing segmentation dataset . As we verify extensively, the masks are of high quality and diversity .","segmentation, sa-1b, final, dataset"
20,"Images in SA-1B span a geographically and economically diverse set of countries . We found that SAM performs similarly across different groups of people . Together, we hope this will make our work more equitable for real-world use cases.","fairness, concerns, sam, potential"
21,"SAM produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth . Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol . These results suggest that SAM can be used out","edge, sam, detection"
29,"The promptable segmentation task is to return a valid segmentation mask given any prompt . The requirement of a ""valid"" mask simply means that even when a prompt is ambiguous, the output should be a reasonable mask for at least one of those objects .","segmentation, nlp, task"
30,The promptable segmentation task suggests a natural pre-training algorithm . We adapt this method from interactive segmentation . This ensures that a pre-trained model is effective in use cases that involve ambiguity .,"segmentation, interactive, mask, pre-training, valid"
31,"our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time . In general, a wide array of practical segmentation tasks can be cast as prompting .","in-stance, segmentation, cat, zero-shot, transfer"
35,"This capability is a form of task generalization . In a multi-task system, a single model performs a fixed set of tasks . The training and test tasks are the same .","combined, task, segmentation, model"
36,"Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design . This approach is analogous to how other foundation models are used, e.g., how CLIP is the text-image alignment component of the DALL","prompting, segmentation, image, clip, dallÂ·e, promptable, generation"
44,"Mask decoder uses prompt self-attention and cross-attention in two directions to update all embeddings . After running two blocks, we upsample the image embedder and an MLP maps the output token to a dynamic linear classifier .","mlp, image, mask, embedding, decoder"
45,"Model will average multiple valid masks if given an ambiguous prompt . To address this, we modify the model to predict multiple mask outputs . We found 3 mask output is sufficient to address most common cases .","resolving, output, multiple, masks, ambiguity"
48,We supervise mask prediction with the linear combination of focal loss and dice loss used in . We train for the promptable segmentation task using a mixture of geometric prompts .,"loss, geometric, training, prompts, focal"
50,"the data engine has three stages: a model-assisted manual annotation stage, a semi-automatic stage with a mix of automatically predicted masks . a fully automatic stage in which model generates masks without annotator input .","segmentation, mask, engine, masks, data"
51,Professional annotators label masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by SAM . Our model-assisted annotation runs in real-time enabling a truly interactive experience . We did not impose semantic constraints for labeling objects .,"segmentation, interactive, tool, browser-based, sam"
53,"After sufficient data annotation, SAM was retrained using only newly annotated masks . Average annotation time per mask decreased from 34 to 14 seconds as the model improved . Overall, we collected 4.3M masks from 120k images .","segmentation, public, common, sam, datasets"
54,"In this stage, we aimed to increase the diversity of masks in order to improve our model's ability to segment anything . To detect confident masks, we trained a bounding box detector on all first stage masks using a generic ""object"" category .","confident, masks, automatic, segment, stage, anything"
55,"In the final stage, annotation was fully automatic . This was feasible due to two major enhancements to our model . First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage . Second, by this stage we had developed the ambiguity","model, fully, automatic, valid, ambiguity-aware, stage, objects"
59,"SA-1B consists of 11M diverse, highresolution, licensed, and privacy protection images and 1.1B high-quality segmentation masks collected with our data engine .","license, privacy, images, dataset, protection"
60,"We licensed a new set of 11M images from a provider that works directly with photographers . These images are high resolution and the resulting data size can present accessibility and storage challenges . Even after downsampling, our images are significantly higher resolution than many existing vision datasets .","image, coco, vision, dataset"
61,"Our data engine produced 1.1B masks, 99.1% of which were generated fully automatically . The quality of the automatic masks is centrally important . We compare them directly to professional annotations .","model, training, mask, masks, automatic"
62,"To estimate mask quality, we randomly sampled 500 images and asked our professional annotators to improve the quality of all masks in these images . This procedure resulted in pairs of automatically predicted and professionally corrected masks . For comparison, prior work estimates inter-annotator consistency at 85-91% .","mask, automatic, masks, quality"
68,"In Fig. 5 we plot the spatial distribution of object centers in SA-1B compared to the largest existing segmentation datasets . Common photographer biases are present in all datasets, compared with LVIS v1 and ADE20K . On average, it has 36x more masks per image than","size, image, mask, image-relative, aviation, complexity"
73,In Fig. 7 we visualize the per-country image counts in SA-1B and the 50 countries with the most images . We infer the country images were photographed in using standard methods .,"income, geographic, and, africa, representation"
77,We use the More Inclusive Annotations for People dataset for gender presentation and age and a proprietary dataset for perceived skin tone . Our evaluation uses simulated interactive segmentation with random sampling of 1 and 3 points . Table 2 shows results for perceived gender presentation .,"fitzpatrick, perceived, skin, type, age, tone"
81,"Our experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt . Next, we present a sequence of experiments that traverse low, mid, and highlevel image understanding and roughly parallel the historical development of the field .","ever, everything, segment, sam"
84,We evaluate segmenting an object from a single foreground point . This task is ill-posed as one point refers to multiple objects . Ground truth masks in most datasets do not enumerate all possible masks .,"ground, task, truth, segmenting"
85,"We sample points from the ""center"" of ground truth masks . The baselines are all single-mask methods . We compare mainly to RITM .","segmentation, interactive, ritm"
86,"We use a newly compiled suite of 23 datasets for mIoU evaluation . For the human study, we use the subset listed in Fig. 9b .","miou, evaluation, datasets"
92,"SAM yields higher results on 16 of the 23 datasets, by as much as 47 IoU . We present an ""oracle"" result, in which the most relevant of SAM's 3 masks is selected . This reveals the impact of ambiguity on automatic evaluation .","results, miou, evaluation, automatic, per-dataset"
93,"Error bars are 95% confidence intervals for mean mask ratings . An ablated, ""ambiguity-unaware"" version of SAM has consistently lower ratings, though still higher than RITM .","ratings, mask, error, bar, mean, sam"
94,"SAM has learned to segment valid masks from a single point . Note that for datasets like DRAM and IBD, SAM receives consistently higher ratings in the human study .","rating, qualitative, masks, valid, sam"
95,"Fig. 9c shows additional baselines, SimpleClick and FocalClick . As the number of points increases from 1 to 9 we observe that the gap between methods decreases . This is expected as the task becomes easier .","baselines, point, baseline, random, dynamic, grows, sam"
102,We evaluate SAM on the classic low-level task of edge detection using BSDS500 . We prompt SAM with a 16x 16 regular grid of foreground points resulting in 768 predicted masks .,"edge, bsds500, lightweight, postpro-cessing, detection"
103,"SAM predicts more edges, including sensible ones that are not annotated in BSDS500 . This bias is reflected quantitatively in Table 3: recall at 50% precision is high, at the cost of precision . SAM naturally lags behind state-of-the-art methods that learn the biases","edge, representative, bsds500, methods, maps, detection, state-of-the-art"
109,We compute the standard average recall metric on LVIS v1 . We focus on Lvis because its large number of categories presents a challenging test. We compare to a strong baseline implemented as a ViTDet detector.,"recall, strong, baseline, standard, average, (ar)"
110,"In Table 4 we see unsurprisingly that using the detections from ViTDet-H as object proposals performs the best overall . In fact, SAM only underperforms SAM on small objects and frequent objects . We compare against an ablated ambiguityunaware version of SAM .","biases, vitdet-h, annotation, method, lvis, lvis-, specific, dmp, vitdet"
113,"We compare the masks predicted by SAM and ViTDet on COCO and LVIS in Table 5 . We observe gaps on both datasets, where SAM is reasonably close . In Fig. 11 we observe that SAM consistently outperforms ViTDÃ©t in the human study.","mask, sam, lvis, vitdet"
118,"ViTDet learns the specific biases of COCO masks . We hypothesize that on COCO, where the mask AP gap is larger and the ground truth quality is relatively low . SAM, being a zero-shot method, is unable to exploit them .","coco, lvi, lvis, dataset"
120,"SAM's training procedure is modified to make it text-aware, but in a way that does not require new text annotations . For each manually collected mask with area larger than 1002 we extract the CLIP image embedding . This is, at inference time we run text through CLIP's text","text, image, clip, prompt, embedding"
125,We perform several ablations on our 23 dataset suite with the single center point prompt protocol . Recall that a single point may be ambiguous and that ambiguity may not be represented in the ground truth .,"point, ground, prompt, single, center, sam, truth"
126,Fig. 13 plots SAM's performance when trained on data engine stages . The automatic masks vastly outnumber the manual and semi-automatic masks . This setup complicates training.,"cumulative, masks, automatic, data"
127,"In Fig. 13 we look at the impact of data volume . The full SA-1B contains 11M images, which we uniformly subsample to 1M and 0.1M for this ablation . At 0.1m images, we observe a large mIoU decline under all settings .","volume, full, dataset, data"
133,"Pre-trained models have been adapted to downstream tasks since the early days of machine learning . We contrast one aspect of our approach with , which emphasizes the role of self-supervised learning in foundation models.","machine, model, self-supervised, learning, foundation"
134,Pre-trained models can power new capabilities even beyond those imagined at the moment of training . Our goal is to make this kind of composition straightforward with SAM . We require SAM to predict a valid mask for a wide range of segmentation prompts .,"systems, segmentation, larger, compositionality, clip, mcc, sam"
135,"SAM can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that ""zoom-in' Unlike these methods, SAM is designed for generality and breadth of use rather than high IoU interactive segmentation .","segmentation, interactive, small, disconnected, sam, components"
136,"Segment Anything project is an attempt to lift image segmentation into the era of foundation models . Our principal contributions are a new task , model , and dataset that make this leap possible .","image, model, segmentation"
137,"We would like to thank Aaron Adcock and Jitendra Malik for helpful discussion . We thank Cheng-Yang Fu, Jiabo Hu, and Robert Kuo for help with scaling the model .","dataset, viewer"
140,"CVPR, 2010. 4, 10 Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. TPAMI, 2010 . 4, 10, 21, 28 Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton","machines, segmentation, image, vision, bert, of"
141,"CVPR, 2022. 5, 16, 17 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, and Giuseppe Am","image, segmentation, vision, cvpr, ijcv"
143,"CVPR, 2019. 2, 6, 7, 9, 10, 11, 19, 20, 21, 24 Abner Guzman-Rivera, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners.",gis
144,"Scaling up visual and vision-language representation learning with noisy text supervision. ICML, 2021. 1 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amode","visual, natural, image, relationship, detection"
146,"ICCV, 2017. 6 David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, LluisMiquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training . CVPR, 2022. 10 Mattia Pugli","natural, language, recognition, action, supervision"
147,"NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation . CVPR, 2021. 9, 19, 20, 23, 24 Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and Antonio Torralba .","image, activity, index, workbank"
153,"Image encoder can be any network that outputs a Cx H x W image embedding . Motivated by scalability and access to strong pre-training, we use an MAE pre-trained Vision Transformer with minimal adaptations to process high resolution inputs . We can afford a high number","image, encoder"
154,"To reduce the channel dimension, we use a 1 x 1 convolution to get to 256 channels . Each convolution is followed by a layer normalization .","following, image, standard, embedding, practices"
155,Sparse prompts are mapped to 256dimensional vectorial embeddings . A point is represented as the sum of a positional encoding of the point's location . We use the text encoder from CLIP .,"text, geometric, clip, prompts, free-form"
158,"a two-layer mask decoder updates both the image embedding and prompt tokens via cross-attention . Then the image embeddeding is upscaled, from which the updated output tokens are used to dynamically predict masks.","mask, lightweight, decoder"
160,"Lightweight mask decoder maps image embeddings to output mask . To combine inputs, we first insert a learned output token embedder . For simplicity, we refer to these embeddedings as ""tokens""","segmentation, image, mask, output, models, embedding, transformer"
161,"Each decoder layer performs 4 steps: self-attention on tokens, cross-attention from tokens to the image embedding . This last step updates the image embeddeding with prompt information . Each self/cross-attention and MLP has a residual connection , layer normalization, and dropout of ","mlp, image, tokens, embedding, decoder"
162,the original prompt tokens are re-added to the updated tokens whenever they participate in an attention layer . This allows for a strong dependence on both the prompt token's geometric location and type .,"token, image, prompt, layer, attention, embedding"
166,The transformer MLP blocks have a large internal dimension of 2048 . The MLP is applied only to prompt tokens for which there are relatively few .,"mlp, transformer, dimension, embedding"
168,"a single input prompt may be ambiguous in the sense that it corresponds to multiple valid masks . By default we predict three masks, since we observe that three layers are often enough to describe nested masks. During training, we compute the loss between the ground truth and each of the predicted masks but","outputs, multiple, masks, valid, ambiguity-aware"
169,"Ambiguity is much rarer with multiple prompts and the three output masks usually become similar . To minimize computation of degenerate losses at training, we only predict a single mask when more than one prompt is given . This is accomplished by adding a fourth output token for an additional mask prediction .","multiple, ambiguity, prompts"
170,We supervise mask prediction with a linear combination of focal loss and dice loss . We observe that auxiliary deep supervision after each decoder layer is unhelpful . The IoU prediction head is trained with mean-square-error loss between the prediction and the predicted mask .,"dice, loss, mask, ground, iou, truth"
171,We simulate an interactive segmentation setup during training . Points are sampled uniformly from the ground truth mask's bounding box . This noise profile is a reasonable compromise between applications like instance segmentation .,"training, segmentation, algorithm, bounding, instance, box"
172,"Each new point is foreground or background if the error region is false negative or false positive respectively . To provide the next iteration with maximal information, we supply the unthresholded mask logits instead of the binarized mask .","iou, ground, mask, truth"
173,We find diminishing returns after 8 iteratively sampled points . One of these iterations is randomly inserted among the 8 samples . The other is always at the end .,"so, model, mask, identification, lightweight, decoder"
174,"The initial learning rate , after warmup, is 8e-4 We train for 90k iterations . To regularize SAM, we set weight decay to 0.1 and apply drop path with a rate of 0.4 . No data augmentation is applied .","optimizer, training, rate, learning, adamw, sam, recipe"
177,"To train ViT-B, we use 180k iterations with batch size 128 distributed across 128 GPUs . We set lr = 8e-4/4e-4, ld = 0.6/0.8, wd = 0.0, and dp = 0.0.6/0.4 .","augmentation, ablations, vit-b/l, large-scale, data, jitter"
180,"Masks were generated from a regular grid of 32x32 points on the full image . We applied standard greedy box-based NMS in two phases . When applying NMS across crops, we ranked masks from most zoomed-in .","cropping, windows"
181,filtering thresholds were selected to achieve both a large number of masks and high mask quality as judged by professional annotators using the method described in 5. To keep only confident masks we filtered by the model's predicted IoU score .,"mask, iou, quality, filtering, soft"
184,Automatic mask generation model was trained on manual and semi-automatic data only . simulated interactive training used only point and mask prompts and sampled only 4 points per mask during training .,"model, mask, automatic, generation, decoder"
187,Inferring geographic information for SA-1B . Each image has a caption describing its contents and where it was taken . We infer approximate image geo-locations from these captions using an Elmo-based named entity recognition model .,"geo-location, geo-tagging, entity"
188,The COCO and Open Images datasets do not provide geo-locations . We retrieved locations for 24% of the COCO training set and for Open Images 18% of the training set .,"flickr, images, api, open"
193,"To investigate SAM's fairness at segmenting people we use the More Inclusive Annotations for People test set annotations for Open Images . To get ground truth masks, we select each person-category mask if its bounding box is within a 1% margin of an annotated bounding","images, mask, person-category, open"
194,"In Table 6 we compare performance across perceived gender presentation and age group . We find that SAM is better at segmenting clothing on those who present predominantly masculine, with disjoint 95% confidence intervals .","gender, segmentation, perceived, group, presentation, age"
196,"We built a new segmentation benchmark to evaluate the zero-shot transfer capabilities of our model using a suite of 23 diverse segmentation datasets from prior work . This suite covers a range of domains including egocentric , microscopy , X-ray , underwater , aerial , simulation ","economy, capabilities, segmentation, benchmark, zero-shot, transfer"
197,Point sampling follows standard practice in interactive segmentation . Each subsequent point is chosen deterministically as the point farthest from the object boundary . This setting better reflects use cases in which the first point is not reliably near the center of the mask.,"point, interactive, segmentation, sampling, default"
198,"The per-dataset mIoU is the per-mask IoU averaged across all objects in the dataset . Our evaluation differs from the standard interactive segmentation evaluation protocol . We focus on predictions after just one, or possibly a few points, since many of our use cases involve a single or","segmentation, interactive, miou, evaluation, sam, per-dataset"
199,"For RITM, we use HRNet32 IT-M trained on the combination of COCO and LVIS introduced by the authors . We follow the suggested default strategies for data pre-processing and do not change any parameters for our evaluation .","largest, ritm, baseline, models, default, combined, dataset"
200,"SAM's ""oracle"" performance is assessed at 1 point . This protocol addresses possible single point prompt ambiguity and oracle evaluation .","point, prompt, iou, ambiguity, single"
208,We perform zero-shot edge detection experiments on BSDS500 . The ground truth for each image comes from the manual annotations of five different subjects .,"metrics, bsds500, dataset"
209,SAM prompts SAM with a 16x16 regular grid of foreground points . We do not filter by predicted IoU or stability . Then we apply a Sobel filter to the remaining masks' unthresholded probability maps .,"pipeline, mask, automatic, generation, zero-shot, transfer"
210,"In Fig. 15, we show additional examples of zero-shot edge predictions from SAM . These qualitative examples further illustrate how SAM tends to output sensible edge maps despite not being trained for edge detection .","edge, sam, detection, sensible"
212,We report the standard average recall metric for masks at 1000 proposals . We focus on AR@ 1000 due to the open-world nature of our model .,"ar, lvis, validation, set"
214,"We use cascade ViTDet-H as a baseline, the strongest model from AP on LVIS . To produce 1000 proposals, we disable score thresholding in the three cascade stages .","baseline, cascade, vitdet-h"
215,We use a modified version of SAM's automatic mask generation pipeline for zero-shot transfer . To make inference time comparable to that of ViTDet we do not process image crops . We remove filtering by predicted IoU and stability .,"pipeline, image, mask, crops, automatic, generation, zero-shot, transfer"
216,"To test this, we compare to an ablated version SAM that only outputs a single mask instead of three . We further increase the number of points sampled and NMS threshold to 128 x 128 and 0.95 respectively .","sam-sam, sam, sam-sam-sam-sam-sam-sam-sam-sam-sam-sam-sam-sam-sam-sam-"
221,"Compared to ViTDet, SAM tends to produce higher quality masks with cleaner boundaries . We show zero-shot instance segmentations predicted on LVIS in Fig. 16 . SAM makes a valid modal prediction for the plate .","validation, lvis, cleaner, splits, v1, boundaries"
223,"Model and training use the largest publicly available CLIP model . To train SAM, we discard masks with an area smaller than 1002 pixels . All other training parameters follow our default settings .","model, clip, large-scale, jitter, sam"
224,"To extract an input prompt we first expand the bounding box around each mask by a random factor from 1 x to 2x, square-crop the expanded box to maintain its aspect ratio, and resize it to 336x336 pixels . To ensure the embedding focuses on the object, we use","training, image, encoder, clip, prompt"
229,"In Fig. 17, we show 3 examples of a query mask and similar masks in the same image . To do SO, we compute mask embeddings using an image crop around a mask and its horizontally flipped version .","semantics, mask, qualitative, query, sam"
233,"Human review can obtain a measure of mask quality independent of an underlying ground truth mask in order to alleviate these issues . The first limitation is that for ambiguous inputs, the model may be strongly penalized for returning a valid mask of a different object than the ground truth .","mask, ground, iou, quality, truth"
234,"RITM , single-output SAM, and SAM test two hypotheses . First, we hypothesize that SAM produces visually higher quality masks than baseline interactive segmentation models when given a single point .","sam, ritm, single-output"
236,"LVIS v0.5 , VISOR , DRAM , IBD , NDD20 , OVIS , and iShape . This set includes datasets where SAM outperforms RITM with IoU metrics .","vdd20, vdd, lvis"
238,"pany collected manually annotated masks for the data engine . An annotator was provided access to an image, the predicted mask of a single model, and the input to the model . They then submitted a rating from 1-10 indicating the overall mask quality .","engine, manually, data"
239,"score of 1 indicates a mask that corresponds to no object at all . a low score indicates that the mask has huge errors . Annotators were provided with five different views, each designed to help identify different error types .","analysis, no, mask, visible, with, errors"
240,"1000 masks per dataset were selected randomly from the same subsets used for benchmarking zero-shot interactive segmentation . The model input was the centermost point, calculated as the largest value of the distance transform from the edge of the mask . In all experiments, masks with a size smaller than 242 pixels were excluded","validation, segmentation, interactive, ground, set, v1, zero-shot, box, truth, lvis"
242,the average standard deviation in score over the five annotators was 0.83 . The annotation company deployed quality assurance testers . Each job was completed by only a single annotator .,"testing, assurance, deviation, standard, average, quality"
249,"P-values are calculated via a paired t-test on the means of the model scores . All statistical tests are strongly significant, and all confidence intervals exclude zero.","confidence, model, interval, baseline, sam, p-values, single-output"
250,"Fig. 11 of the main text shows the histogram for ratings . For fair comparison, results for LVIS were subsampled to the same 794 inputs for each model and ground truth .","ratings, segmentation, ground, coco, instance, truth, lvis"
256,"We release a dataset of 11M images and 1.1B masks, by far the largest segmentation dataset to date . The dataset we release is privacy protection: we have blurred faces and license plates in all images .","meta, community, vision, ai"
258,"All of the instances in the dataset are photos . The photos vary in subject matter; common themes of the photo include: locations, objects, scenes . If the dataset is a sample, then what is the larger set? If so, please describe how this representativeness was validated .","photo, image, larger, geographic, coverage, provider, set, dataset"
259,"No, there are no known relationships between instances in the dataset . If so, please provide a description . Errors: The masks are generated by a segmentation model, so there may be errors or inconsistencies in the masks .",age
260,"the released masks associated with each image were automatically inferred by our segmentation model, SAM . Quality was validated as described in 5. 2. What mechanisms or procedures were used to collect the data?","programs, software, apis, image, sam"
262,"If the dataset is a sample from a larger set, what was the sampling strategy ? We withheld 2k randomly selected images for testing purposes . The rest of the licensed images are included in the dataset .","analysis, impact, information, dataset, protection"
265,"We resized the high-resolution licensed images such that the shorter side is 1500 pixels . If so, please provide a link or other access point to the ""raw"" data . No, as we removed the data for safety reasons .","license, preprocessing, software, plates, raw, data"
267,"The dataset was used to train our segmentation model, SAM . If so, please provide a description . All users of the dataset must cite it, so its use is trackable via citation explorers .","segmentation, model, sam, dataset"
269,"Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created? If so, please provide a description . The dataset will be available for the research community .","system, information, dataset"
271,"The dataset will be hosted at https://ailscebook.com/danaets/sagment-onyting and maintained by Meta AI . If so, please tell how often, by whom, and how updates will be communicated to dataset consumers .","sagment, sagment-onyting, ai, meta, management, dataset"
274,"If the dataset relates to people, are there applicable limits on data retention . We took measures to remove personally identifiable information from any images of people . Users may report content for potential removal here: segment-anything@meta.com . No, as the only updates will be to remove potentially harmful content .","consumer, retention, dataset"
278,"Depending on annotators' skills, the quality of the mask and the number of masks per image are different . Annotators worked full time on our annotation task with very small attrition rate . This made it possible to train the annotator providing feedback and answering their questions on a regular basis .","segmentation, tool, annotation, annotators"
281,We worked with 130 annotators based in Kenya . We do not believe sociodemographic characteristics of annotator meaningfully impacted the data . The Segment Anything 1B dataset is to be used for research purposes only .,"annotators, sociodemographic, characteristics, statistics"
283,Annotators were compensated with an hourly wage set by the vendor . We manually reviewed annotations and shared feedback with the annotators on a weekly basis .,"channel, annotation, communication, annotators, compensation"
285,"Annotators graduated from training to production after the vendor QA team manually spotchecked the annotator's masks to ensure quality . On average, annotators spent one week in training before graduating . Have you conducted any analysis on disagreement patterns?","annotations, model, assessment, quality, sam, of"
287,"Do you have reason to believe the annotations in this dataset may change over time? Do you plan to update your dataset? No, except to remove objectionable images .","software, license, agreement, dataset"
290,"SAM is intended to be used for any prompt-based segmentation task . We explored its use in segmenting objects from a point , edge detection , segmenting all objects , and segmenting detected objects . The license for SAM can be found at http://ghti.com/Kacebook","large, consciderations, scale, vision, ethical, sam"
318,"Overall mask quality is subjective . Each of the above errors may hurt mask qualityonly a little or a lot, depending on how large the error is . A score of 1: It is not possible to tell what object this mask corresponds to . This includes the case that there is no mask visible at all .","mask, quality"
