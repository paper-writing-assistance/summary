element_idx,keywords,summarized_text
6,"computation, sequence, memorization, sequential, grid, lstm","Grid Long Short-Term Memory is a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images . We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able"
8,"cells, memory, networks, neural, recurrent, lstm","Long-Term Memory networks are recurrent neural networks equipped with a special gating mechanism that controls access to memory cells . Since the gates can prevent the rest of the network from modifying the contents of the memory cells for multiple time steps, LSTM networks preserve signals and propagate errors for much longer than ordinary re"
9,"deep, computation, networks, sequential, lstm",Deep networks suffer from exactly the same problems as recurrent networks applied to long sequences . LSTM seems attractive to generalise the advantages of deep computation .
10,"cells, way, network, architecture, robust, lstm","We introduce Grid LSTM, a network that is arranged in a grid of one or more dimensions . The depth dimension is treated like the other dimensions and uses LSMC cells to communicate directly from one layer to the next."
17,"feed-forward, networks, train, multidimensional, lstm",is used to train feed-forward networks with up to 900 layers of depth . Grid LSTM with two dimensions is analogous to the Stacked-LSTM . but it adds cells along the depth dimension too .
18,"two-dimensional, integer, grid, lstm",We study some of the learning properties of Grid LSTM in various algorithmic tasks . We compare the performance of two-dimensional grids to Stacked LS on computing the addition of two 15-digit integers without curriculum learning .
19,"btec, translation, neural, mnist, iwslt, image",Grid LSTM achieves 1.47 bits-per-character in the 100M characters Wikipedia dataset . The network outperforms the reference phrase-based CDEC system on the IWSLT BTEC Chinese-to-Ensligh translation task .
33,"memory, gu, mi-1, gc, networks, gates, vector, lstm",the gate gf can delete parts of the previous memory vector mi-1 . The gate gu controls what is then read from the new memory mi onto the hidden vector hi . This ensures that the forward signals from one step to the other are not repeatedly squashed by a non-linearity such as t
39,"lstm, stacked, network","Stacked LSTM adds capacity by stacking layers on top of each other . The output hidden vector hi in Eq. 1 is taken as the input to the LSMC above in place of I * Xi . Note that although the cells are present along the sequential computation, they are not present ."
41,"multidimensional, grid, lstm","The network concatenates the transformed input I * x and the N hidden vectors h1, ..., hN into a vector H and as in Eq. 1 computes gu, gÂ° and gc, as well as N forget gates g.f . At each input "
42,"grow, grid, values","values in m can grow at the same rate due to the unconstrained summation in Eq. 4. This can cause instability for large grids, and adding cells along the depth dimension increases N and exacerbates the problem ."
44,"multidimensional, grid, blocks, lstm","Grid LSTM deploys cells along any or all of the dimensions including the depth of the network . To modulate the interaction of the cells in the two dimensions, the grid proposes a simple mechanism where the values in the cells cannot grow combinatorially as in Eq."
51,"n-dimensional, n-di, grid, lstm","Each transform has different weight matrices Wu W.f, Wi, Wc in RdxNd . Note how the vector H that contains all the input hidden vectors is shared across the transforms . As for a block, the grid has N sides with incoming hidden and memory vectors and N sides"
53,"output, transforms, hidden, vector, block, n-dimensional","In a N-dimensional block the transforms for all dimensions are computed in parallel . But it can be useful for a dimension to know the outputs of the transformations from the other dimensions . For example, to prioritize the first dimension of the network, the block first computes the N - 1 transformations for"
56,"weight, matrix, grid, lstm","In Grid LSTM networks that have only a few blocks along a given dimension in the grid, it can be useful to just have regular connections along that dimension without the use of cells . This can be naturally accomplished inside the block by using for that dimension in Eq. 6 a simple transformation with a nonlinear"
57,"transfer, grid, nonlinear, function, lstm, 3d",A 2d Grid LSTM applied to temporal sequences with cells in the temporal dimension but not in the vertical depth dimension corresponds to stacked with one or more layers .
63,"translation, model, neural, block, n-dimensional","As the blocks are arranged in a grid, this separation extends to the grid as a whole; each side of the grid has either input or output vectors associated with it . The mechanism inside the blocks ensures that the hidden and memory vectors from the different sides will interact closely without being conflated ."
65,"of, sharing, model, grid, weights, lstm, image","if multiple sides of a grid need to share weights, capacity can be added to the model by introducing into the grid a new dimension without sharing of weights ."
68,"target, networks, numbers, padding, 2-lstm, input","We first experiment with 2-LSTM networks on learning to sum two 15-digit integers . The problem formulation is similar to that in , where each number is given to the network one digit at a time ."
70,"network, output, integers, input",we fix the number of digits from the partially predicted output to 15 . We do not use curriculum learning strategies and do not put them back into the network .
76,"optimizer, network, adam, training","We train the networks for up to 5 million samples or until they reach 100% accuracy on a random sample of 100 unseen addition problems . Note that since during training all samples are randomly generated, samples are seen only once and it is not possible for the network to overfit on training data."
77,"problem, addition, networks, 2-lstm, tied","the best performing tied 2-LSTM network is 18 layers deep and learns to solve the task in less than 550K training samples . The best untied 2-LTM network has 5 layers, learns more slowly and achieves a per-digit accuracy of 67% after 5 million examples ."
79,"algorithmic, networks, 2-lstm, network, task",The sequences are 20 symbols long and we use a vocabulary of 64 symbols encoded as one-hot vectors and given to the network one symbol per step . The setup is similar to the one for addition above.
82,"accuracy, network, adam, training",All networks have 100 hidden units and have between 1 and 50 layers . We train each network for up to 5 million samples or until they reach 100% accuracy on 100 unseen samples .
88,"networks, 2-lstm, untied, tied","the 43-layer tied 2LSTM network learns a solution with less than 150K samples . There is fairly high variance amid the solving networks, deeper networks tend to learn faster ."
91,"cell, hidden, 2-lstm, vector, tied",We use a tied 2-LSTM with 1000 hidden units and 6 layers of depth . The characters are projected to form the initial input hidden and cell vectors . We back propagate the errors every 50 characters .
94,"translation, source, neural, grid, sentence, lstm","In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence . The mapping is usually performed within the encoder-decoder framework . This approach has yielded strong empirical results, but it can suffer from a bottleneck ."
99,"translation, two-dimensional, attention-based, model",Grid LSTM uses Reencoder network to view translation in a novel fashion . One dimension processes the source sentence whereas the other dimension produces the target sentence . The resulting network repeatedly re-encodes the source sentences . This function as an implicit attention mechanism .
100,"two-dimensional, grid, setup, lstm","The two-dimensional setup aims at explicitly capturing the invariance present in translation . Translation patterns between two languages are invariant above all to position and scale of the pattern . For example, reordering patterns should be detected and applied independently of where they occur in the source sentence or of the number of words involved"
101,"btec, b, iwslt","We evaluate the Grid LSTM translation model on the IWSLT BTEC Chinese-to-English corpus . The corpus has about 0.5M words in each language, a source vocabulary of 7055 Chinese words and a target vocabulary of 5646 English words . We use regular identity connections without nonlinear transfer"
104,"valid-15, test-15, valid-1, dglstm-attention","Valid-1 Test-1 Valid-15 Test-15 DGLSTM-Attention - 34.5 - CDEC 30.1 41 50.1 58.9 3-LSTM 30.3 42.4 51.8 60.2 Reference thank you , ma , am  please give this bill to the cashier  Generated thank you"
105,"system, mechanism, translation, neural, cdec, attention","the first table contains BLEU-4 scores of the 3-LSTM neural translation model, the CDEC system and the Depth-Gated LSTM with attention mechanism . the scores are calculated against either the main reference translation or against the 15 available reference translations in the BTEC corpus ."
106,"validation, cdec, source, lstm","The processing is bidirectional, in that the first grid processes the source sentence from beginning to end and the second one from end to beginning . This allows for the shortest distance that the signal travels between input and output target words to be constant and independent of the length of the source . We train seven models with vectors of size"
108,"interaction, multi-way, lstm","Grid LSTM is a network that uses cells along all of the dimensions and modulates in a novel fashion the multi-way interaction . We have described powerful and flexible ways of applying the model to character prediction, machine translation and image classification ."
145,"analysis, document, networks, neural, icdar, visual, convolutional","In 7th International Conference on Document Analysis and Recognition , 2-Volume Set, 3-6 August 2003, Edinburgh, Scotland, UK, pp. 958-962, 2003. doi: 10.1109/ ICDAR.2003.1227801."
157,gis,"We apply one-dimensional Grid LSTM to learning parity . Given a string b1,  , bk of k bits 0 or 1, the parity or generalized XOR of the string is defined to be 1 if the sum of the bits is odd, and 0 "
163,"parity, k-bit, activation, values, network",Figure 10: The right figure is a heat map of activation values of selected counter neurons in a 1-LSTM network that has 25 layers . The specific values are obtained by a feed-forward pass through the network using as input the bit string 010140 .
165,"weights, 1-lstm, networks, untied",The 1-LSTM networks are trained with either 500 or 1500 hidden units . The feed-forward ReLU and tanh networks have from 1 to 150 hidden layers . Each network is trained with a maximum of 10 million samples or four days of computation on a Tesla K40m GPU.
166,"weights, 1-lstm, networks, untied",Figure 9 depicts the results of the experiments with 1-LSTM networks . Figure 10 relates the best performing networks of each type . Some networks in the search space find solutions for up to k = 250 bits .
172,"bit, string, networks, neurons, 1-lstm, input, counting",linear suggests that more than a single bit of the input is considered at every step . We visualized the activations of the memory vectors obtained via a feed-forward pass through one of the 1-LSTM networks using selected input bit strings . This revealed the prominent presence of counting neurons that keep a counter for the number
174,"3-lstm, network, image","The 3-LSTM performs computations with LSTM cells along three different dimensions . Two of the dimensions correspond to the two spatial dimensions of the grid, whereas the remaining dimension is the depth of the network . Due to the unbounded context size, the computations of features at one end of the image can be "
175,"softmax, layer, 3-lstm, network, image","We divide the 28 x 28 MNIST image into pxp pixel patches, where p is a small number such as 2 or 4 . The patches are then linearized and projected into two vectors of the size of the hidden layer of the 3-LSTM; the projected vectors are the input hidden and"
182,"validation, images, mnist, training","The MNIST dataset consists of 50000 training images, 10000 validation images and 10000 test images . The pixel values are normalized by dividing them by 255 . Data augmentation is performed by shifting training images from 0 to 4 pixels in the horizontal and vertical directions and padding with zero values ."
183,"approaches, modeling, 3-lstm, competing","the 3-LSTM without the cells also performs well . The other approaches, with the exception of ReNet, are convolutional neural networks ."
