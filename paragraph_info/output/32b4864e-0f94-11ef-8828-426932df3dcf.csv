element_idx,keywords,summarized_text
5,"amt, story, text, generation, model-generated",We first conduct a survey of 45 open-ended text generation papers . The vast majority fail to report crucial details about their AMT tasks . We then run a series of story evaluation experiments with both AMT workers and English teachers .
7,"story, information, generation, neural, modeling, language","Recent advances in neural language modeling have spurred research into open-ended text generation tasks such as story generation , style transfer , and pun generation . Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU and ROUGE that measure"
9,"tasks, text, generated, open-ended, amazon",Model-generated text is critical for openended tasks given the unreliability of automatic metrics . Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text .
10,"amt, text, generation, reliability, open-ended, reproducibility","In this paper, we study the reliability and reproducibility of AMT evaluations of open-ended text generation . We first conduct a survey of papers between 2018-2020 and find many critical details often go unreported ."
11,"amt, story, generation, model, gpt-2, language","AMT workers and raters perform a series of story generation evaluations . Unlike previous work in this area, we ask ratesrs to evaluate both stories generated by a fine-tuned GPT-2 language model and human-written reference stories on the same scale ."
16,"amt, text, low-quality, generated, gpt-2, reference, workers",42% of AMT workers take fewer than 40 seconds to complete each task . Filtering out these workers can make a significant impact to the overall ratings . Even expert raters struggle to read and judge model-generated text .
17,"amt, tasks, text, generation, open-ended","for future human evaluations, we urge researchers to obtain expert raters whenever possible . If AMT is the only feasible option, we recommend that available reference outputs also be evaluated alongside modelgenerated ones to improve rating calibration ."
20,"amt, sarcasm, translation, generation, task, machine","Each paper was published between 2018 and 2020 at ACL, NAACL, or EMNLP . We exclude papers that use AMT to evaluate more wellestablished generation tasks like machine translation ."
21,"evaluation, quality, criteria",The most common criteria include fluency and/or grammaticality . Stories tend to also be evaluated on some notion of coherence and likability .
24,"rating, likert, scale","More than half of papers employ a 5-point Likert scale to evaluate the above criteria . Of these, 19 provided labels for just the end points of the scale . The next most common type is ranking two or more system outputs ."
25,"amt, rating, raters, workers","Most papers do not even report the number of raters and/or items used for evaluation . Of the remaining, most papers obtain ratings from 3 separate AMT workers per item ."
26,"qualifications, amt, compensation, worker",The majority of papers do not report AMT worker qualifications or worker compensation . Only 11 papers mentioned restricting workers to those from English-speaking countries or applying a language test .
29,"amt, open-ended, generation, text","Mechanical Turk task design asks AMT workers to rate various properties of generated text on a 5-point Likert scale . In this section, we conduct a series of AMT evaluations for the open-ended problem of story generation by varying different parameters within this standard task design . Importantly, we evaluate both model"
31,"text, hit, stories, reference, model-generated",ence stories provides a pseudo upper bound for the ratings . Our experiments reveal that worker qualifications do not impact judgments or spam rate on reference stories .
34,"rompts, dataset, toothpaste, writingp","This dataset is a collection of 303,358 English language stories written by Reddit users on the r /WritingP rompts subreddit . We randomly select 200 prompts from the test set for all of our experiments . Since the human-written stories in the dataset are already"
35,"model-generated, stories, gpt-2","We use a batch size of approximately 50k tokens, a learning rate of 5e - 5 with a linear learning rate schedule, and train for 3 epochs . We train for the WritingPrompts dataset using the HuggingFace Transformers library ."
38,"perplexity, machine-generated, stories, converges","After validation perplexity converges to  19 . Each training example consists of a concatenation of an prompt, separator token , and reference story ."
42,"experiments, amt, and, worker, human, machine-generated, stories, judgments",Each of our AMT experiments shows workers the same 200 prompts paired with human and/or machine-generated stories . We solicit three worker judgments per HIT . Each experiment used a completely different set of workers .
46,"human-written, amt, text, stories, reference",our first set of experiments concerns only humanwritten reference stories . We move to machinegenerated text in the next subsection . This is supported by the expert teacher assessment in Section 4 .
47,"qualifications, hit, worker, rate, approval","we run four experiments evaluating the previously-described set of 200 prompts with reference story fragments . no qualifications, including only workers with HIT approval rate > 90% . at least 1000 approved HITs located in English-speaking countries ."
48,"likability, amt, worker, grammar, coherence",The results in the top portion of Table 1 suggest that applying all of the qualifications has a positive effect on the quality of workers . This setting yielded the highest scores out of the four experiments for coherence and relevance while ratings for grammar were also considerably high .
55,"qualifications, amt, enabled","each worker can only participate in one experiment, each experiment has a different subset of qualified workers . the second day received lower ratings across the board and poor IAA ."
57,"amt, inseconds, reports, actual, work-time, time",AMT reports WorkTimeInSeconds in the results file made available to task requesters . These times are artificially inflated due to workers who accept multiple HITs at the same time .
61,"amt, out, lit-tle, ratings, filtering, time","ratings when filtering out workers who spend too little time per HIT . Specifically, we remove judgments from workers whose median time is below 40s , and find that on average about 42% of our ratings are filtered out ."
62,"filter-, english, of, ing, country, worker, origin","All of the surveyed papers evaluate only English text, but only 11 reported using some sort of filtering to ensure that workers have sufficient knowledge of English . The default AMT setting does not filter workers by country of origin, which potentially increases the variance of results depending on the English proficiency of workers who accept HITs . To measure"
65,"filters, qualification, quality, acceptance, rate, approval",11We also ran experiments with even stricter qualification filters . This is most likely due to the fact that most requesters are reluctant to reject HITs regardless of quality .
67,"text, hit, generated, gpt-2, reference, model-generated, prompt","HITs contain a prompt and a GPT-2 generated text . In the latter case, we ask AMT workers to rate both texts . Overall, we observe that workers cannot effectively distinguish between reference and model-generated stories when they are evaluated separately ."
68,"likability, amt, text, worker, generated, stories, gpt-2, reference","In our first experiment, we follow the protocol from our experiments with human-written reference stories . The results of this evaluation are presented in the upper row of Table 2 ."
69,"human-written, output, text, stories, gpt-2, model-generated","workers in Day 2 rated human-written stories similarly to the GPT-2 generated stories in terms of grammar and coherence . Depending on which reference day we compare the GTP-2 output to, GPT2 is rated similarly to human-writing stories in . terms of all four properties ."
70,"worker, assking, stories, model-generated, workers","We hypothesize that the previous result is due to scale calibration differences between the two settings . When confronted with incoherent model-generated text, a worker may be more generous with their ratings compared to if they only see coherent human-written text . Thus, we explore whether their ratings can be better"
73,"settings, qualification, gpt-2","Workers score GPT-2 generated stories significantly lower than reference stories on coherence , relevance , and likability . Their ratings for grammar are similar for both types of text ."
75,"amt, ended, text, generation, open-","The experiments in the previous section demonstrate the unreliability of AMT ratings for openended text generation . In this section, we compare the ratings produced by AMT workers to those of expert raters, specifically a set of three English teachers ."
77,"certificate, grammatical, celta, low-level, mistakes",the three teachers were recruited from the authors' personal networks . each of them either has a degree in teaching English as a Second Language or a CELTA certificate .
78,"amt, text, generated, machine-generated, stories, gpt-2, reference, workers","teachers rate reference stories higher than GPT-2 generated ones . We asked teachers to first rate 200 reference stories and then a week later . Just like the AMT workers, they were not told that the text in the second task was machinegenerated ."
82,"relevance, gpt-2, sim-, grammar, ratings, ilar","teachers' ratings of human-written stories are considerably higher than AMT ratings for all attributes except likability which depending on the day was rated lower or higher by the AMT workers . Similarly, teachers' rating of GPT-2 generated stories as similar in terms of grammar ."
84,"text, erence, machine-generated, stories, gpt-2, ref-, coherence","teachers unanimously report that while coherence is easy to rate for reference stories, it is the most difficult property to rate . Relevance turned out to be the easiest property of machine-generated text for teachers to rate, which is expected ."
85,"teacher, rating, average, gpt-2, time",All teachers reported struggling more when rating GPT-2 stories . average rating time decreased from 69.8 seconds to 87.3 seconds . Teachers also reported having to recalibrate their scale when rating .
87,"human-written, rating, text, machine-generated, gpt-2","the GPT-2 generated stories, as the stories were significantly worse than the human-written text . Consequently, they suggested that it would be easier to calibrate their scale . The teachers suggested that creating a standardized rubric would greatly facilitate the rating process."
88,"teacher, mediation, resolving, meeting, disagreement","We arranged a mediation meeting between two of the three teachers to discuss 60 stories on which they showed the highest disagreement . In about 20% of cases, one of the teachers disagreed with their own previous rating due to honest lapses of judgment ."
89,"teacher, upwork, platform, freelance, replicating","Replicating the study on Upwork: We recognize that replicating our study is difficult without access to a network of English teachers . We performed the same experiment using three certified teachers recruited on a freelance platform, Upwork . It took approximately one week to collect the data ."
94,"domains, text, generation, evaluation, quality","Most previous studies on human evaluation concentrate on constrained generation domains, such as machine translation or summarization . Other studies evaluate very short, often one sentence long, outputs ."
95,"texts, machine, creative, translated","Even professional translators struggle when evaluating longer machine translated texts . Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference text . Previous studies have asked workers to choose from or distinguish between human-written and machine-generated texts "
96,"control, amt, quality, workers","Data collection using AMT: Many previous works raise concerns about the reliability of data collected on AMT . Reluctance of requesters to reject HITs leads to positive bias in workers' qualifications . Furthermore, a large number of responses are provided by small number of productive workers ."
98,"custom, qualifications, amt, control, cognitively-demanding, worker, task, quality","Our experiments show that evaluating open-ended generated text is an incredibly challenging task even for expert raters . We recommend future AMT evaluations implement additional quality control mechanisms such as filtering workers by observed time spent per HIT rather than WorkTimeInSeconds, specifying a maximum number of items per"
100,"texts, open-ended","Specifically, texts from social media sites like Reddit may contain racist, sexist, and other forms of vulgar content . Additionally, neural language models like GPT2 have been trained on open domain text crawled from the web . As such, we advocate adequately warning any humans who take part in open-ended text"
104,"nlp, group",We thank the reviewers for their insightful comments . We would also like to thank the UMass NLP group for the great advice on the draft of this paper .
118,"analysis, text, generated, evaluation, natural, language",All that's 'human' is not gold: Evaluating human evaluation of generated text . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.
126,"nat-ural, emnlp-ijcnlp, neural, language, processing","Cristina Garbacea, Samuel Carton, Shiyan Yan, and Qiaozhu Mei. 2019. A largescale evaluation study of neural language models for online review generation."
139,"evaluation, human, nlg","Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions . In Proceedings of the 13th International Conference on Natural Language Generation, pages 169-182, Dublin, Ireland. Association for Computational Linguistics."
149,"loss, complexity-weighted","Complexity-weighted loss and diverse reranking for sentence simplification . Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 ."
155,"story, grounding, generation, neural, sense, common, emnlp",Improving neural story generation by targeted common sense grounding . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing .
171,"commonsense, backprop-based, reasoning, emnlp, decoding",Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing .
173,"transfer, formality, gyafc, style, dataset","GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics ."
185,"language, natural, processing","Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz ."
190,"avoid, technology, boring, human, spelling, sentences, language","How to avoid sentences spelling boring? towards a neural approach to unsupervised metaphor generation . Association for Computational Linguistics: Human Language Technologies, Volume 1 ."
202,"human-written, amt, platform, upwork, stories, freelancing",The teachers were paid $175 to evaluate the same 200 human-written stories and 200 GPT-2 generated stories . They were asked to perform the ratings on the AMT platform in order to use the same interface as workers on AMT .
211,"mediation, knowledge, availability, world, meeting, disagreement","The teachers were asked to reevaluate Zoom  60 stories on which they showed disagreement . In about 20% of the cases, the teachers agreed with each other, suggesting that the previous disagreement was due to honest lapses of judgment . Common reasons for disagreement which could be resolved during the mediation meeting included: world knowledge, difference"
213,"herence/grammarlilikabilty/relevance, co, co-",How long did it take you to calibrate your ratings? Explain in more detail how you rated coherencelgrammarlilikabilty/relevance? What was the process. Did you have to reread the text? 4. How often did you take breaks? 5. Which attribute was the most difficult?
223,"child, shocking, shock, oren","Oren was always a conscientious child . It was a necessary skill of a new master, an inherent capability to make the world a better place . Today, the day he brought his sister to his cooking school was the first time ."
239,"pairwise, adjustment, test, post-hoc, bonferroni",The numbers provided in the table are p-values for the given pairwise comparison . Grammar ratings provided by the workers from non-English speaking countries are significantly different from those provided on Day 1 and Day 3 5 .
245,"hoc, comparison, pairwise, adjustment, test, post, bonferroni",Table A9: Pairwise post hoc test with Bonferroni adjustment . Ratings of coherence provided by raters from non-English speaking countries differ significantly from ratings of workers from English-speaking countries .
