element_idx,keywords,summarized_text
3,"imagenet, computational, budget, network, architecture","Inception was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 . To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing . One particular incarnation used in our submission for IL"
5,"deep, of, 2014, recognition, network, architecture, ilsvrc, quality, image","In the last three years, the quality of image recognition and object detection has been progressing at a dramatic pace . Most of this progress is not just the result of more powerful hardware, larger datasets and bigger models . No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition"
6,"adds, deep, architecture, multiply","a notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms - especially their power and memory use - gains importance . For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, SO that the they do"
9,"deep, internet, vision, network, architecture","We will focus on an efficient deep neural network architecture for computer vision, codenamed Inception . Inception derives its name from the Network in network paper by Lin et al ."
11,"networks, overfitting, neural, classification, convolutional, image","Variants of this basic design are prevalent in image classification literature . For larger datasets such as Imagenet, the recent trend has been to increase the number of layers and layer size ."
12,"of, neuro, neuroscience, model, the, primate, visual, cortex, inception","Serre et al. use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similar to the Inception model . Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model."
13,"activation, networks, linear","Network-in-Network is an approach proposed by Lin et al. in order to increase representational power of neural networks . When applied to convolutional layers, the method could be viewed as additional 1 x 1 layers followed typically by the rectified linear activation . This enables it to be easily integrated"
14,"bounding, cnn, box, state-of-the-art, cues, low-level",R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion . Such a two stage approach leverages the accuracy of bounding box segmentation with low
16,"deep, networks, neural, quality, training, higher, models",the most straightforward way of improving the performance of deep neural networks is by increasing their size . This includes increasing the depth - the number of levels - of the network and its width . It comes with two major drawbacks .
22,"computational, size, vision, network, resources","if two convolutional layers are chained, any uniform increase in the number of filters results in a quadratic increase of computation . Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size ."
23,"principle, space, hebbian, neuron, topology, network","The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions . This would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al. Their main result states that if the probability distribution"
24,"modelling, infrastructures, numerical, computing, libraries","non-uniform sparse models require more sophisticated engineering and computing infrastructure . The gap is widened even further by the use of steadily improving, highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication ."
27,"sparse, dense, matrix, matrices",sparse matrix computations tend to give state of the art practical performance . it does not seem far-fetched to think that similar methods would be used for the automated construction of non-uniform deep-learning architecture .
28,"networks, vision, localization, architecture, inception, structure","The Inception architecture started out as a case study of the first author for assessing the hypothetical output of a sophisticated network topology construction algorithm . Despite being a highly speculative undertaking, only after two iterations on the exact choice of topology, we could already see modest gains against the reference architecture "
29,"networks, vision, network, architecture, topologies, inception",the proposed architecture has become a success for computer vision . The most convincing proof would be if an automated system would create network topologies resulting in similar gains in other domains using the same algorithm but with very differently looking global architecture.
31,"vision, network, architecture, correlation, convolutional, visional",The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components . Arora et al. suggests a layer-by-layer construction in which one should analyze the correlation statistics of the
33,"layer, convolutional, modules",a small number of 5x5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters . This problem becomes even more pronounced once pooling units are added to the mix . The merging of the output of the pooling layer with the outputs of con
38,"rectified, activation, reducation, linear, dimension",This leads to the second idea of the proposed architecture: judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise . We would like to keep our representation sparse at most places and compress the signals only whenever they have to be aggregated en masse . This is based on
39,"infrastructural, conception, inefficiency, network, inception","Inception is a network consisting of modules of the above type stacked upon each other . For technical reasons, it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion ."
40,"information, reduction, visual, architecture, dimension",the ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer . Another practically useful aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated SO that the next stage can abstract features simultaneously .
41,"stage, computational, architecture, inception, resources",the improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties . We have found that all the included knobs and levers allow for a controlled balancing of computational resource that can result in networks that are 2 - 3x faster than similarly performing networks
43,"googlenet, inception, network, architecture","We chose GoogLeNet as our team-name in the ILSVRC14 competition . This name is an homage to Yann LeCuns pioneering LeNet 5 network . We have also used a deeper and wider Inception network, which was slightly inferior ."
48,"rgb, modules, channels, rectified, color, inception, lin","The size of the receptive field in our network is 224x224 . ""#3x3 reduce"" stands for the number of 1x1 filters in the reduction layer used before the 3x3 and 5x5 convolutions ."
49,"average, pooling, low-memory, network, footprint",The network is 22 layers deep when counting only layers with parameters . The overall number of layers used for the construction of the network is about 100 . This number depends on the machine learning infrastructure system used .
50,"auxiliary, gradient, networks, signal, network, convolutional, smaller","the ability to propagate gradients back through all the layers in an effective manner was a concern . By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier . These classes take the form of smaller convolutional networks put on top of the output"
60,"distbelief, distb",Our networks were trained using the DistBelief distributed machine learning system . Our training used asynchronous stochastic gradient descent with 0.9 momentum .
61,"resizing, recovery, sampling, image","our image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, like dropout and learning rate, SO it is hard to give a definitive guidance to the most effective single way to train these networks."
63,"imagenet, hierarchy, classification, validation, image","The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy . There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing . Each image is associated with one ground truth category, and performance is measured "
65,"portrait, approach, model, googlenet, cropping, image",We independently trained 7 versions of the same GoogLeNet model . These models were trained with the same initialization and learning rate policies . They only differ in sampling methodologies and random order in which they see input images .
71,"crops, howard, square, cropping, andrew, mirrored","square resized to 224x224, and their mirrored versions . This results in 4x3x6x2 = 144 crops per image ."
74,"supervision, approach, clarifai, top-performing",Our final submission in the challenge obtains a top-5 error of 6.67% on both the validation and testing data . This is a 56.5% relative reduction compared to the SuperVision approach in 2012 .
84,"bounding, region, proposals, googlenet, regression, box, multi-box","GoogLeNet's approach for detection is similar to the R-CNN by . In addition, the region proposal step is improved by combining the Selective Search approach with multi-box predictions for higher object bounding box recall ."
85,"bounding, networks, data, box, localization, task, convolutional","We first report the top detection results and show the progress since the first edition of the detection task . We report the official scores in Table 4 and common strategies for each team: the use of external data, ensemble models or contextual models . The external data is typically the ILSVRC12 classification data for pre-training a model"
88,"computer, vision, sparse, optimal, structure",Our results seem to yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision . Our detection work was competitive despite of neither utilizing context nor performing bounding box .
90,"inception, regression, strength, architecture",regression and this fact provides further evidence of the strength of the Inception architecture . This suggests promising future work towards creating sparser and more refined structures in automated ways on the basis of .
92,"distbelief, photometric, distortions","We would like to thank Sanjeev Arora and Aditya Bhaskara for fruitful discussions on photometric distortions . Also we are indebted to the DistBelief team for their support especially to Rajat Monga, Jon Shlens, Alex Krizhevsky, Jeff"
94,"deep, co-adaptation, networks, neural, network, classification, backpropagation","Know yourmeme: We need to go deeper . CoRR, abs/1310.6343, 2013. Umit V. Catalyurek, Cevdet Aykanat, and Bora U ar. On two-dimensional sparse matrix partitioning: Models, methods, and a recipe"
96,"detection, deep, object, networks, neural, recognition","In Proceedings of the 28th ACM International Conference on Supercomputing, ICS '14, pages 333-342, New York, NY, USA, 2014. ACM. Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance"
