element_idx,summarized_text,keywords
6,"CycleMLP has two advantages compared to modern approaches . It can cope with various image sizes by using local windows . In contrast, previous MLPs have O computations due to fully spatial connections .","size, cyclemlp, visual, image, recognition"
9,"Vision models in computer vision have been long dominated by convolutional neural networks . Recently, inspired by the successes in Natural Language Processing field, Transformers are adopted into the computer vision community . Built with self-attention layers, multi-layer perceptrons , and skip connections .","vision, models, multi-layer, perceptrons, cnn, computer"
10,"Current models are composed of blocks with non-hierarchical architectures, which make the model infeasible to provide pyramid and high-resolution feature representations . FC-like models can not be used in dense prediction tasks due to the three challenges .","model, tasks, visual, pre-diction, recognition, mlp-like, dense"
19,Channel FC aggregates features in the channel dimension with spatial size '1' It can handle various input scales but cannot learn spatial context . Spatial FC has a global receptive field in the spatial dimension .,"spatial, fc, connected, fully, layer, cycle"
20,structure typically requires the input image with a fixed scale . It contradicts the requirements of dense prediction tasks . The computational and memory costs of the current MLP models are quadratic .,"input, training, image, resolution, benchmark, coco, strategy"
22,Our Cycle FC is inspired by Channel FC layer illustrated in Figure 1a . Channel FC is infeasible to aggregate spatial context information due to limited receptive field .,"channel, fc, weight, image-size, agnostic"
23,"Our Cycle FC is designed to enjoy Channel FC's merit of taking input with arbitrary resolution and linear computational complexity . In this way, Cycle FC has the same complexity as channel FC while increasing the receptive field simultaneously . To this end, we adopt Cycle FC to replace the Spatial FC for spatial context aggregati","spatial, token, mixing, fc, cycle"
24,Cycle FC is computational friendly to cope with flexible input resolutions . Extensive experiments on various tasks demonstrate that CycleMLP outperforms existing MLP-like models and is comparable to and sometimes better than Transformers on dense predictions .,"cyclemlp, task, fc, ta, prediction, cycle, dense"
31,"In Sec. 2.1, we introduce CycleMLP models for vision tasks including recognition and dense predictions . Then we compare Cycle FC with Channel FC and multi-head attention adopted in recent Transformer-based models .","cyclemlp, model, multi-head, attention"
34,Channel FC enlarges receptive field of MLP-like models to cope with downstream dense prediction tasks while maintaining computational efficiency . The basic Cycle FC operator can be formulated as below: SH and Sw .,"fc, receptive, field, mlp-like, models, cycle, of"
36,"Figure 1 illustrates the offsets along two axis when SH = 3, that is dj  0 and di = -1, 0, 1, -1 . Figure 1 shows that when H, , Cycle FC has a global receptive field .","examples, cycle, stepsize, fc"
40,the offsets di and dj enlarge the receptive field of Cycle FC as compared to Channel FC . The larger field in return brings improvements on dense prediction tasks like semantic segmentation and object detection . Both the FLOPs and the number of parameters are linear to the spatial scale .,"fc, dj(c), d, di(c), cycle"
44,"equation 4 shows that only the weights of Wmhsa on spatial shift +1,8j + 1) are taken into account in Wmlp This indicates that Cycle FC introduce an inductive bias that the weighting matrix in MHSA should be sparse . Thus Cycle FC inherits the large recept","inductive, bias, fc, receptive, matrix, field, cycle, weighting"
46,"Each patch is then treated as a ""token""  Specifically, we follow to adopt an overlapping patch embedding module with the window size 7 and stride 4 .","input, image, patch, raw, embedding"
47,Cycle FC block consists of three parallel Cycle FCs . Then there is a channel-MLP with two linear layers and a GELU non-linearity in between . A LayerNorm layer is applied before both Parallel Cycle FC layers .,"fc, convolution, attention, cycle, block"
54,"The number of tokens is maintained within each stage . At each stage transition, the channel capacity is expanded while the number is reduced . CycleMLP can conveniently serve as a general-purpose visual backbone .","cyclemlp, backbone, visual"
55,"Model Variants is mainly inspired by the philosophy of hierarchical Transformer models . These models reduce the number of tokens at the transition layers as the network goes deeper and increase the channel dimension . In this way, we can build a model ZOOS that is critical for dense prediction tasks .","zoos, transformer, architecture, hierarchical"
66,We first compare CycleMLP with existing MLP-like models . The results are summarized in Table 2 and Figure 2 . We attribute the accuracy-FLOPs tradeoff to the effectiveness of Cycle FC .,"mlp-like, cyclemlp, model, cyclemlp-b2"
68,"Table 3 further compares CycleMLP with previous state-ofthe-art CNN, Transformer and Hybrid architectures . GFNet uses the fast Fourier transform to learn spatial information .","swin, cyclemlp, transformer"
71,"Cycle FC is capable of serving as a general-purpose, plug-and-play operator for spatial information communication and context aggregation . CycleMLP-B2 outperforms the counterparts built on both Channel FC and Spatial FC .","spatial, fc, cyclemlp-b2, spati"
85,"CycleMLP can take arbitraryresolution images as input without any modification . On the contrary, GFNet needs to interpolate the learnable parameters on the fly when the input scale is different from the one for training . Figure 3 shows that the absolute Top-1 accuracy on ImagNet and Figure 3 show the accuracy differences","cyclemlp, adaptability, resolution"
87,"We conduct object detection and instance segmentation experiments on COCO dataset . We first follow the experimental settings of PVT , introduced in Appendix . E.2 The results are presented in Table 6.","swin, coco, transformer"
94,"CycleMLP-based RetinaNet consistently surpasses the CNN-based ResNet , ResNeXt and Transformer-based PVT under similar parameters constraints . Furthermore, using Mask R-CNN for instance segmentation also demonstrates similar comparison results .","swin, cyclemlp, transformer"
101,CycleMLP-B2 surpasses Swin-T by 0.9 mIoU with slightly less parameters . The state-of-the-art Transformer-based backbone can obtain comparable or even better performance .,"swin, cyclemlp, transformer"
114,"CycleMLP is built upon the Cycle Fully-Connected Layer . It is capable of dealing with variable input scales and can serve as a generic, plug-and-play replacement of vanilla FC layers . Experimental results demonstrate that CyclemLP outperforms existing MLP models on ImageNet classification .","cyclemlp, input, modeling, imagenet, variable, scales, classification"
124,"Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Tianhe","wang, mmdetection, jiaqi"
168,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Aly","library, deep, learning"
209,VGG demonstrated a state-of-the-art performance on ImageNet via deploying small convolution kernels to all layers . DenseNet connected each layer to every other layer in a feed-forward fashion .,"convolutional, performance, features, neural, image, hand-crafted, state-of-the-art, networks, cnn"
210,"Transformers were first proposed by Vaswani et al. for machine translation and have since become the dominant choice in many NLP tasks . Recently, transformer have led to a series of breakthroughs in computer vision community since the invention of ViT .","nlp, cnn, transformer, cnns"
211,"MLP-based Models differ from the above discussed CNN- and Transformer-based models . Instead, they use MLP layers over feature patches on spatial dimensions to aggregate the spatial context . These MLP models share similar macro structures but differ from each other in the detailed design of the micro block .","cnn-based, models, s2-mlp"
225,"positional embedding matrix containing positional information for every input token can be replaced by the output of any function fp that encodes the position of tokens . And Ai,j is the attention score between the ith and jth token in X .","matrix, attention, positional, embedding"
227,"Wkey is introduced to only pertain to the positional encoding rq,k . u and v are learnable parameter vectors that replace the original Pq,: Wqry term, which implies that the attention bias remains the same regardless of the absolution positions .","token, bias, k, encoding, attention, positional, q"
230,"In order to conduct fair and convenient comparison, we build two model zoos: the one is in PVTStyle and the other in Swin-Style . These models are scaled up by adapting several architecture-related hyper-parameters .","swin, to, cyclemlp-b1, cyclemlp-t, swin-style, -b5"
233,Our code is implemented based on PyTorch framework and heavily relies on the timm repository . The optimizer is AdamW with the momentum of 0.9 and weight decay of 5x10-2 by default . We train our models on the ImageNet-1K dataset .,"validation, imagenet-1k, v100, setting, gpu, tesla, dataset"
240,"We use the AdamW optimizer with the initial learning rate of 1x10-4  All models are trained on 8 Tesla V100 GPUs with a total batch size of 16 for 12 epochs . In the testing stage, the shorter side of input images is resized to 800 pixels while no constraint","segmentation, cyclemlp, instance"
242,"ADE20K contains 20K training, 2K validation and 3K testing images . We adopt the mmsegmenati on toolbox as our codebase in this subsection .","segmentation, method, semantic, ade20k, pvt"
249,"the averaged Top-1 accuracy on ImageNet-1K drops by 1.3% . We hypothesize that the decreased performance is caused by the fact that random sampling will totally disturb the semantic information of objects, which is essential to image recognition.","drops, image, random, sampling, recognition, imagenet-1k"
250,"compared with random sampling, dilated solutions lose the fine-grained information for recognition . It may hurt the accuracy performance to some extent .","dilated, local, stepsize, information, aggregation"
251,"dense sampling strategies incredibly increase the models' parameters and FLOPs . Therefore, for fair comparisons, we conducted extra ablation studies on training models for 100 epochs with strictly same learning configuration .","mod-, model, els, training, sampling, dense"
265,"We further conduct experiments on CycleMLPs with stepsize of 1x7 and 7x1 respectively . The results are summarized Table 15. To analyze the impact of stepsize on the performance, we take Figure 7 for better illustration .","cyclemlp, 3x1, 1x7, stepsize, of"
