element_idx,keywords,summarized_text
4,"carlo, computer, networks, value, monte, go",The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space . Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions . These deep neural networks are trained by a novel combination of supervised learning from human expert
5,"carlo, value, monte, superhuman, performance, approximate, function","In large games, such as chess 1 and especially Go 1, exhaustive search is infeasible2,3, but the effective search space can be reduced by two general principles . First, the depth of the search may be reduced through position evaluation: truncating the search tree at state s and replacing the sub"
6,"search, tree, carlo, rollouts, children, monte","Monte Carlo tree search 11,12 is used to estimate the value of each state . As more simulations are executed, the search tree grows larger and the relevant values become more accurate . The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves13 ."
8,"networks, neural, domain, visual, convolutional, games, atari","neural networks have achieved unprecedented performance in visual domains . For example, image classification17, face recognition 18 playing Atari games19 . We use convolutional layers to construct increasingly abstract, localized representations of an image20 ."
9,"learning, application, value, network, machine","We train the neural networks using a pipeline consisting of several stages of machine learning . This provides fast, efficient learning updates with immediate feedback and high-quality gradients . We train a reinforcement learning policy network Pp that improves the SL policy network by optimizing the final outcome of games of selfplay ."
11,"expert, learning, moves, policy, supervised, network, go",The SL policy network alternates between convolutional layers with weights . A final softmax layer outputs a probability distribution over all legal moves . The input s to the policy network is a simple representation of the board state .
18,"policy, rollout, kgs, go, server",We trained a 13-layer policy network from 30 million positions from the KGS Go Server . The network predicted expert moves on a held out test set with an accuracy of 57.0% using all input features and 55.7% using only raw board position and move history as inputs .
20,"winning, reward, current, policy, function, r(s)",Randomizing from a pool of opponents in this way stabilizes training by preventing overfitting to the current policy network . The outcome Zt=  r is the terminal reward at the end of the game from the perspective of the current player at time step t: +1 for winning and - 1 for losing
25,"search, rl, carlo, program, monte, policy, network, gs","the RL policy network won more than 80% of games against the SL policy network . We also tested against the strongest open-source Go program, Pachi14 ."
38,"perfect, state-outcome, rl, pairs, policy, network, red, play","Ideally, we would like to know the optimal value function under perfect play * . In practice, we instead estimate the value function VPP for our strongest policy ."
41,"rl, carlo, rollouts, overfitting, monte, policy, network","To mitigate this problem, we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game . Each game was played between the RL policy network and itself until the game terminated ."
45,"node, prior, leaf, policy, rollout, probability","the leaf position SL is processed just once by the SL policy network Po . The output probabilities are stored as prior probabilities P for each legal action a, P = Po. The leaf node is evaluated in two very different ways ."
54,"alphago, computation, value, networks","AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs and computes policy and value networks in parallel on GPUs . The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs."
58,"alphago, search, blue, asynchronous, bl, light",Vertical lines show KGS ranks achieved online by that program . Games against the human champion Fan Hui were also included . 95% confidence intervals are shown .
65,"alphago, rollout, network, evaluations","Figure 5 I How AlphaGo selected its move in an informal game against Fan Hui . For each of the following statistics, the location of the maximum value is indicated by an orange circle . a, Evaluation of all successors s' of the root position s, using the value network vo ."
66,"alphago, fan, move, hui, probabilities","e, Percentage frequency with which actions were selected from the root during simulations . The moves are presented in a numbered sequence ."
86,"alphago, stones, handicap","singlemachine AlphaGo is many dan ranks stronger than any previous Go program, winning 494 out of 495 games against other Go programs . AlphaGo won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi respectively."
87,"alphago, carlo, rollouts, evaluation, value, monte, network","We also assessed variants of AlphaGo that evaluated positions using just the value network or just rollouts . However, the mixed evaluation performed best, winning 95% of games against other variants."
90,"fan, hui","AlphaGo and Fan Hui competed in a formal five-game match . This is the first time a computer Go program has defeated a human professional player, without handicap, in the full game of Go-a feat ."
92,"search, deep, tree, program, networks, neural, go","We have developed a Go program, based on a combination of deep neural networks and tree search . The program plays at the level of the strongest human players, thereby achieving one of artificial intelligence's ""grand challenges"""
96,"alphago, value, kasparov4, network",AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov4 . The neural networks of AlphaGo are trained directly from gameplay purely through general-purpose supervised and reinforcement learning methods.
97,"alphago, networks, intelligence, value, artificial, functions","Go is exemplary in many ways of the difficulties faced by artificial intelligence33,34 . The previous major breakthrough in computer Go led to corresponding advances in many other domains ."
100,"source, engine, computing, world, open, championship, programming, monte-carlo","Artif. Intell. 134, 277-311 . Schaeffer, J. The games computers play . Advances in Computers 52, 189-266 . Buro, M. From simple features to sophisticated evaluation functions ."
101,"deep, learning, reinforcement, networks, soft, neural, convolutional, segmentation","Krizhevsky, A., Sutskever, 1. & Hinton, G. ImageNet classification with deep convolutional neural networks . In Advances in Neural Information Processing Systems, 1097-1105 . Lawrence, S., Giles, C. L., Tsoi, A"
103,"alphago, team","Fan Hui agrees to play against AlphaGo; T. Manning and T. Schaul for refereeing the match; A. Cain and M. Cant for work on the visuals; P. Dayan, G. Wayne, D. Kumaran, D Purves, H. van Hasselt"
104,"alphago, g., g.v.d.d.","A.H., G.v.d.D., J.S., I.A., M.La., A.G., T.G, and D.S . designed and trained the neural networks in AlphaGo . J. S. and J.N., a.H"
111,"markov, state, transition, games, function","We restrict our attention to two-player zero-sum games, r1 = -12 =r, with deterministic state transitions,f=f, and zero rewards except at a terminal time step T . A policy p is a probability distribution over legal actions a E A ."
112,"Â·, search, tree, pruning, min-, alpha-beta, value, optimal, imax, function","Min imax tree search has achieved superhuman performance in chess4, checkers5 and othello6, but it has not been effective in Go7 ."
113,"self-play, backgammon, learning","Reinforcement learning can learn to approximate the optimal value function directly from games of self-play39 . The majority of prior work has focused on a linear combination vo =  0 of features with weights 0. Weights were trained using temporal-difference learning41 in chess42,43, check"
114,"search, simulation, tree, carlo, minimax, monte, policy","Monte Carlo tree search 11,12 estimates the optimal value of interior nodes by a double approximation, Vn U vPn 2 v* . In the limit, MCTS converges12 to the maximum value function limn00 Vn = limN VP = "
115,"search, algorithms, carlo, truncated, rollouts, value, monte, functions","MCTS has previously been combined with a policy that is used to narrow the beam of the search tree to high-probability moves13 . By contrast, AlphaGo's use ofvalue functions is based on truncated Monte Carlo search algorithms8,9, which terminate rollouts before the end of"
116,"patterns, handcrafted, position, rollout-based, mcts, evaluation, policy, rollout","MCTS performance is determined by the quality of the rollout policy . Prior work has focused on handcrafted patterns50 or learning rollout policies by supervised learning13, reinforcement learning16, simulation balancing51,52 or online adaptation30,53 ."
119,"analysis, position, value, rollout, rewards","Multiple simulations are executed in parallel on separate search threads . The APV-MCTS algorithm proceeds in the four stages outlined in Fig. 3. Selection . At each in-tree phase, t> L, actions are selected according to the statistics in the search tree, at = argmaxa +"
120,"apv-mcts, value, rollout, network, architecture","The entire search tree is stored on the master, which only executes the in-tree phase of each simulation . The prior probabilities of the policy network are returned to the master ."
121,"alphago, count, maximum, visit",At the end of search AlphaGo selects the action with maximum visit count . This is less sensitive to outliers than maximizing action value15 . The match version continues searching during the opponent's move .
125,"alphago, dynamic, komi, policy, rollout","AlphaGo does not use progressive widening13, dynamic komi59 or an opening book60 . The parameters used by AlphaGo in the  Fan Hui match are listed in Extended Data Table 5. Rollout policy. The rollout policy P is a linear softmax policy based on fast, incrementally computed"
126,"selection, action, policy, rollout, network","In previous work, the symmetries of Go have been exploited by using rotationally and reflectionally invariant filters in the convolutional layers24,28,29 . Instead, we exploit the higher-quality action selection within MCTS, which is informed both by the search tree and the policy network"
127,"set, board, data, policy, raw, network, kgs, description","This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players . 35.4% of the games are handicap games . The data set was split into a test set and a training set ."
128,"reinforcement, network, policy, learning","Every 500 iterations, we added current parameters P to the opponent pool . Each game iin the mini-batch was played out until termination at step T1, and then scored to determine the outcome z't =  r from each player's perspective . The games were then replayed to determine"
129,"value, rl, policy, network","Data set consisted of over 30 million positions, each drawn from a unique game of self-play . Each game was generated in three phases by randomly sampling a time step U  unif1, 450, and sampling the first t= 1,... U - 1 moves from the SL policy network,"
130,"board, value, features, network, go","Each position s was pre-processed into a set of 19 x 19 feature planes . The features come directly from the raw representation of the game rules . All features were computed relative to the current colour to play . For example, the stone colour at each intersection was represented as either player or opponent rather"
131,"alphago, softmax, network, architecture, function","The input to the policy network is a 19x 19 x 48 image stack . The first hidden layer zero pads the input into a 23 x 23 image, then convolves k filters of kernel size 5 x 5 with stride 1 with the input image and applies a rectifier nonlinearity ."
132,"convolution, layer, hidden, value, network, stack, additional, image","Hidden layers 2 to 11 are identical to the policy network . Hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter ofkernel size 1 x 1 with stride 1 and hidden layer 14 is a fully connected linear layer with 256 rectifier units ."
133,"rating, program, computer, regression, bayeselo, go, logistic","We evaluated the relative strength of computer Go programs by running an internal tournament and measuring the Elo rating of each program . We estimate the probability that program a will beat program 6 by a logistic function 1 by Bayesian p = and estimate the ratings e 1 + exp - e) logistic regression, computed"
136,"alphago, system, handicap",All Go programs received a maximum of 5s computation time per move . All games were scored using Chinese rules with a komi of7.5 points .
137,"alphago, program, computer, software, configuration, go, hardware","In Fig. 4, approximate ranks of computer programs are based on the highest KGS rank achieved by that program . the KGS version may differ from the publicly available version ."
138,"fan, hui, computer, informal, go, games, time, controls","Five formal games and five informal games were played with 7.5 komi, no handicap, and Chinese rules . AlphaGo won these games 5-0 and 3-2 respectively . Time controls for formal games were 1h main time plus three periods of 30 s byoyomi ."
139,"search, tree, game, learning, reinforcement, alpha-beta, multi-agent, pruning","In 11th International Conference on Machine Learning, 157-163 . 40. Knuth, D. E. & Moore, R. W. An analysis of alpha-beta pruning . Mach. Learn. 3, 9-44 . 42. Baxter, J., Tridgell, A"
140,"carlo, computer, monte, management, backgammon, last-good-reply, go, time","In 17th International Joint Conference on Artificial Intelligence, 529-534 . 46. Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves master-level play . Neural Comput. 6, 215-219 . 47. Dahl,"
155,"alphago, computation, policy, network, architecture, time","Policy network architecture consists of 128, 192 or 256 filters in convolutional layers; an explicit symmetry ensemble over 2, 4 or 8 symmetries; using only the first 4, 12 or 20 input feature planes listed in Extended Data Table 1 . The results consist of the test and train accuracy on the K"
174,"nets, rollouts, only, value, bayeselo",Each program used 5 s per move on a single machine with 48 CPUs and 8 GPUs . Elo ratings were computed by BayesElo .
