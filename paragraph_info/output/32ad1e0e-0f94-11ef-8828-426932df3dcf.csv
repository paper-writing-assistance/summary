element_idx,keywords,summarized_text
6,"q-learning, deep, dexterous, manipulation","We present an actor-critic, model-free algorithm based on the deterministic policy gradient . Our algorithm robustly solves more than 20 simulated physics tasks . We further demonstrate that for many of the tasks the algorithm can learn policies ""end-to-end"""
8,"deep, sensory, neural, network, processing","Recently, significant progress has been made by combining advances in deep learning for sensory processing with reinforcement learning . To do so, deep neural network function approximators were used to estimate the action-value function ."
9,"high-dimensional, action, low-dimensional, dqn, observation, spaces","DQN can only handle discrete and low-dimensional action spaces . Many tasks of interest, most notably physical control tasks, have continuous and high dimensional action space . However, it cannot be straightforwardly applied to continuous domains ."
10,"of, dimensionality, dqn, actions, discretization","a 7 degree of freedom system with the coarsest discretization ai E -k, 0, k for each joint leads to an action space with dimensionality: 37 = 2187 . The situation is even worse for tasks that require fine control of actions as they require a finer grained"
17,"normalization, approximators, batch, value, dqn, functions, function",We combine the actor-critic approach with insights from the recent success of Deep Q Network . DQN is able to learn value functions using such function approximators in a stable and robust way .
18,"physical, problem, control, sensory, cartpole, raw, input, swing-up","In order to evaluate our method we constructed a variety of challenging physical control problems that involve complex multi-joint movements, unstable and rich contact dynamics, and gait behavior . A long-standing challenge of robotic control is to learn an action policy directly from raw sensory input such as video ."
20,"space, state, ddpg, underlying, low-dimensional",DDPG can sometimes find policies that exceed the performance of the planner . For the physical control problems we compare our results to a baseline computed by a planner that has full access to the underlying simulated dynamics and its derivatives .
22,"learning, reinforcement, setup, environment, agent","We consider a standard reinforcement learning setup consisting of an agent interacting with an environment E in discrete timesteps . At each timestep t the agent receives an observation Xt, takes an action at and receives a scalar reward rt . In general, the environment may be"
24,"future, state, reward, visitation, discounted, distribution","The return from a state is defined as the sum of discounted future reward Rt = Ti=t yr with a discounting factor 2 E . Note that the return depends on the actions chosen, and therefore on the policy . The goal in reinforcement learning is to learn a policy which ."
35,"approximators, networks, neural, large, q-learning, function","Recently, adapted the Q-learning algorithm in order to make effective use of large neural networks as function approximators . They introduced two major changes: the use of a replay buffer, and a separate target network for calculating Yt We employ these in the context of DDPG and explain their implementation in"
37,"q-learning, q-, algorithm, dpg","it is impossible to apply Q-learning to continuous action spaces . This optimization is too slow to be practical with large, unconstrained function approximators . Here we used an actor-critic approach based on the DPG algorithm ."
40,"approximators, learning, state, batch, neural, dpg, dqn, network, large, spaces, function","NFQCA , which uses the same update rules as DPG but with neural network function approximators, uses batch learning for stability, which is intractable for large networks . We refer to our algorithm as Deep DPG ."
44,"optimizations, reinforcement, learning, hardware","most optimization algorithms assume samples are independently and identically distributed . to make efficient use of hardware optimizations, it is essential to learn in minibatches, rather than online."
45,"dqn, ddpg, transitions",The replay buffer is a finite sized cache R. Transitions were sampled from the environment according to the exploration policy . At each timestep the actor and critic are updated by sampling a minibatch uniformly from the buffer .
46,"target, update, networks, soft, actor-critic","Directly implementing Q learning with neural networks proved unstable in many environments . Our solution is similar to the target network used in but modified for actor-critic and using ""soft"" target updates . The weights of these target networks are then updated by having them slowly track the learned networks: 0'  TU +"
47,"feature, observations, low, vector, dimensional",the different components of the observation may have different physical units and the ranges may vary across environments . This can make it difficult for the network to learn effectively and makes it difficult to find hyperparameters which generalise across environments with different scales of state values.
48,"normalization, deep, learning, state, batch, input","batch normalization normalizes each dimension across samples in a minibatch to have unit mean and variance . In deep networks, itis used to minimize covariance shift during training, by ensuring that each layer receives whitened input ."
59,"exploration, state, st+1, policy, actor, noise",Select action at = + Nt according to the current policy and exploration noise Execute action at and observe reward rt and observe new state St+1 Store transition in R Sample a random minibatch of N transitions from R Set Yi = ri + YQ'102') Update critic by minimizing the loss
63,"tasks, communication, locomotion",In all domains but cheetah the actions were torques applied to the actuated joints . These environments were simulated using MuJoCo . Figure 1 shows renderings of some of the environments used .
64,"repeats, state, action, low-dimensional, renderings, high-dimensional, description","In all tasks, we ran experiments using both a low-dimensional state description and high-dimensional renderings of the environment . For each timestep of the agent, we step the simulation 3 timesteps, repeating the agent's action and rendering each time . Thus the observation reported to the agent contains 9 feature maps which"
65,"exploration, dpg, training, noise","Figure 2 shows the performance curve for a selection of environments . We also report results with components of our algorithm removed . In order to perform well across all tasks, both of these additions are necessary ."
71,"q-learning, ddpg",Q-learning is prone to overestimating values . Figure 3 shows that in simple tasks DDPG estimates returns accurately without systematic biases .
72,"tasks, physics, learning, torcs",Torcs has previously been used as a testbed in other policy learning approaches . We used an identical network architecture and learning algorithm hyper-parameters to the physics tasks but altered the noise process for exploration because of the very different time scales involved .
74,"inputs, feature, ddpg, pixel, low-dimensional, vector",We tackle all tasks using both low-dimensional feature vector and high-dimensional pixel inputs . Detailed descriptions of the environments are provided in the supplementary .
78,"dpg, data, efficiency, tile-coding",original DPG paper evaluated algorithm with toy problems using tile-coding and linear function approximators . It demonstrated data efficiency advantages for off-policy DPG over both on- and off policy stochastic actor critic .
84,"ddpg, score, algorithm, reward, raw, training","Table 1: Performance after training across all environments for at least 2.5 million steps . All scores, except Torcs, are normalized so that a random agent receives 0 and a planning algorithm 1 . For comparision we also include results from the original DPG algorithm with a replay buffer and batch normalization"
86,"optimization, policy, complex",is thought to be difficult because it deals simultaneously with complex environmental dynamics and a complex policy . Most past work with actor-critic and policy optimization approaches have had difficulty scaling up to more challenging problems .
90,"algorithm, trick, dpg, reparametrization, svg(0)","Balduzzi & Ghifary extended the DPG algorithm with a replay buffer . They explicitly learn dQ /da . However, they only train on two low-dimensional domains ."
91,"optimization, region, trust, trpo, policy, network","trust region policy optimization aims to construct stochastic neural network policies without decomposing problems into optimal control and supervised phases . This method produces near monotonic improvements in return by making carefully chosen updates to the policy parameters, constraining updates to prevent the new policy diverging too far from the existing policy."
92,"mapping, approach, policy, gps, actor-critic, state-to-action, optimal","to combat the challenges of the actor-critic approach, recent work with guided policy search algorithms decomposes the problem into three phases that are relatively easy to solve . first, it uses full-state observations to create locally-linear approximations of the dynamics around one or more nominal trajectories "
93,"features, data, gps, efficiency, visual",GPS uses a similar convolutional policy network to ours in real-world robotic manipulation tasks using vision . The policy also receives direct low-dimensional state information about the configuration of the robot at the first fully connected layer in the network .
94,"processes, gaussian","PILCO uses Gaussian processes to learn a non-parametric, probabilistic model of the dynamics . It calculates analytic policy gradients and achieves impressive data efficiency ."
95,"deep, model, dynamical, pendulum, task, swing-up",Wahlstrom et al. trained a differentiable forward model and encoded the goal state into the learned latent space . This approach is only applicable to domains with goal states that can be demonstrated to the algorithm .
98,"deep, approximators, learning, reinforcement, non-linear, function",the work combines insights from recent advances in deep learning and reinforcement learning . This algorithm robustly solves challenging problems across a variety of domains .
141,"networks, critic, neural, low-dimensional, adam",We used Adam for learning the neural network parameters with a learning rate of 10-4 and 10-3 for the actor and critic respectively . For Q we included L2 weight decay of 10-2 and used a discount factor of 2 = 0.99 . The neural networks used the rectified non-linearity for all hidden layers 
144,"control, trajectory, optimization, model-predictive",Our planner is implemented as a model-predictive controller . Every single trajectory optimization is planned over a horizon between 250ms and 600ms . This planning horizon recedes as the simulation of the world unfolds.
145,"ilqg, iteration, value, forward, dynamics, function",The iLQG iteration begins with an initial rollout of the previous policy . We use repeated samples of simulated dynamics to approximate a linear expansion of the dynamics around every step of the trajectory . This back-pass results in a putative modification to the action sequence that will decrease the total cost .
152,"physical, than, control, rather, reward, smooth, hopping, gaits",For physical control tasks we used reward functions which provide feedback at every step . For grasping and manipulation tasks we provided a smoothly varying reward based on distance to a goal state . In locomotion tasks we reward forward action and penalize hard impacts .
159,"chain, cha, hard, cheetah",canada2da Agent is required to use a 7-DOF arm with hockey-stick like appendage to hit a ball to a target . cart Agent must move a simple mass to rest at 0. The mass begins each trial in random positions and with random velocities. cartpoleThe classic cart-
162,"movinggripper, movinggripperrandom, agent",reacher3daFixedTarget Agent is required to move a 7-DOF human-like arm from random starting locations to random target positions . reacherObstacle Agent is a 5-DOF arm around an obstacle to a randomized target position . walker2d Agent should
