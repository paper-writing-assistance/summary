element_idx,keywords,summarized_text
5,"2600, learning, reinforcement, q-learning, games, atari","We present the first deep learning model to learn control policies directly from high-dimensional sensory input using reinforcement learning . The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and a value function estimating future rewards . We apply our method to seven Atari"
7,"and, features, vision, speech, hand-crafted",Learning to control agents directly from high-dimensional sensory inputs is one of the long-standing challenges of reinforcement learning . Most successful RL applications have relied on hand-crafted features combined with linear value functions or policy representations .
8,"deep, learning, sensory, data, architecture","Recent advances in deep learning have made it possible to extract high-level features from raw sensory data . These methods utilise convolutional networks, multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks ."
9,"deep, learning, underlying, data, training, hand-labelled, distribution","most successful deep learning applications have required large amounts of handlabelled training data . RL algorithms must learn from a scalar reward signal that is frequently sparse, noisy and delayed . The delay between actions and rewards seems particularly daunting compared to the direct association between inputs and targets found in supervised learning ."
16,"2600, network, input, ale, visual, atari, agent",Atari 2600 is a challenging RL testbed that presents agents with a high dimensional visual input and a diverse and interesting set of tasks that were designed to be difficult for humans players . Our goal is to create a single neural network agent that is able to successfully learn to play as many of the games as
18,"actions, emulator, time-steps, atari","We consider tasks in which an agent interacts with an environment 3 in a sequence of actions, observations and rewards . At each time-step the agent selects an action at from the set of legal game actions, A = 1 . The action is passed to the emulator and modifies its internal state and the game score"
19,"decision, states, state, markov, representation, emulator, process","We consider sequences of actions and observations, St = X1, a1, X2,  , at-1, Xt . All sequences in the emulator are assumed to terminate in a finite number of time-steps . This formalism gives rise to a large but finite Mar"
20,"future, action-value, optimal, rewards, function","The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards . We make the standard assumption that future rewards are discounted by a factor of 2 per time-step, and define the future discounted return at time t as Rt = Et'=t yt' "
24,"(s, equation, qi+1(s, q*, bellman","The basic idea behind many reinforcement learning algorithms is to estimate the actionvalue function, by using the Bellman equation as an iterative update, Qi+1 = E . In practice, this basic approach is totally impractical, as because the action-value function is estimated separately for each sequence, without any generalisation . Instead"
25,"weights, iteration, network",where Yi = Es'E is the target for iteration i and p is a probability distribution over sequences s and actions a that we refer to as the behaviour distribution . Note that the targets depend on the network weights . This is in contrast with the targets used for supervised learning .
27,"strategy, learning, reinforcement, 3, emulator, e-greedy","this algorithm is model-free: it solves reinforcement learning task directly using samples from the emulator 3 without explicitly constructing an estimate of E . In practice, the behaviour distribution is often selected by an E-greedy strategy that follows the greedy strategy with probability 1 - E and selects a random action with probability"
34,"deep, reinforcement, learning",Divergence issues with Q-learning have been partially addressed by gradient temporal-difference methods . These methods are proven to converge when evaluating a fixed policy with an nonlinear function approximator .
35,"representation, low-dimensional, input, visual, nfq","NFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to update the parameters of the Q-network . However, it uses a batch update that has a computational cost per iteration that is proportional to the size of the data set, whereas stochastic gradient updates"
36,"hyperneat, 2600, learning, reinforcement, emulator, atari","Atari 2600 emulator used reinforcement learning algorithms with linear function approximation and generic visual features . Subsequently, results were improved by using a larger number of features, and using tug-of-war hashing to randomly project the features into a lower-dimensional space . HyperNEAT evolutionary architecture"
38,"deep, gradient, networks, stochastic, neural, updates",The most successful approaches are trained directly from raw inputs . Our goal is to connect a reinforcement learning algorithm to a deep neural network which operates directly on RGB images and efficiently process training data by using stochastic gradient descent .
39,"td-gammon, backgammon, network, architecture","Tesauro's TD-Gammon architecture provides a starting point for such an approach . This architecture updates the parameters of a network that estimates the value function, directly from on-policy samples of experience, St+1 ."
40,"q-learning, upda, updates, replay","TD-Gammon uses a technique known as experience replay . We store the agent's experiences at each time-step, et = in a data-set D = e1 . After performing experience replay, the agent selects and executes an action according to an e-greed"
49,"learning, directly, off-policy, experience, replay","Learning directly from consecutive samples is inefficient due to the strong correlations between the samples; randomizing the samples breaks these correlations . Third, when learning on-policy the current parameters determine the next data sample that the parameters are trained on . For example, if the maximizing action is to move left then the training distribution"
50,"application, transitions, replay, memory","In practice, our algorithm only stores the last N experience tuples in the replay memory, and samples uniformly at random from D when performing updates . This approach is in some respects limited since the memory buffer does not differentiate important transitions and always overwrites with recent transitions due to the finite memory size N "
52,"rgb, frames, raw, atari","Raw Atari frames are 210 x 160 pixel images with a 128 color palette . The final input representation is obtained by cropping an 84 x 84 region of the image that roughly captures the playing area . For the experiments in this paper, the function O from algorithm 1 applies this preprocessing"
53,"state, representation, neural, network, architecture","Q maps historyaction pairs to scalar estimates of their Q-value . We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network . The main drawback of this type of architecture is that there is only a single forward"
55,"layer, nonlinearity, hidden, rectifier, games, atari","The input to the neural network consists is an 84 x 8 image produced by  . The second hidden layer convolves 32 4 x 4 filters with stride 2, again followed by a rectifier nonlinearity ."
57,"network, architecture, games, rewards, atari","So far, we have performed experiments on seven popular ATARI games . We use the same network architecture, learning algorithm and hyperparameters settings across all seven games - showing that our approach is robust enough to work on a variety of games without incorporating game-specific information . Since the scale of scores varies greatly from"
59,"frame-skipping, games, technique, atari, skipping",k = 4 makes the lasers invisible because of the period at which they blink . The agent sees and selects actions on every kth frame instead of every frame . This technique allows the agent to play roughly k times more games without increasing the runtime .
61,"learning, action-value, reinforcement, average, reward, q, total, training, function","In reinforcement learning, one can easily track the performance of an agent during training by evaluating it on the training and validation sets . The average total reward metric tends to be very noisy because small changes to the weights of a policy can lead to large changes in the distribution of states the policy visits. The leftmost two plot"
69,"value, function","Figure 3 shows a visualization of the learned value function on Seaquest . The figure shows that the predicted value jumps after an enemy appears on the left of the screen . Finally, the value falls to roughly its original value after the enemy disappears ."
71,"background, rgb, atari, sub-traction",We compare our results with the best performing methods from the RL literature . Contingency used the same basic approach as Sarsa but augmented the feature sets with a learned representation .
72,"al., bellemare, average, score, et, per-game, policy, e-greedy",We report scores for an expert human game player and a policy that selects actions uniformly at random . The human performance is the median reward achieved after around two hours of playing each game . Note that our reported human scores are much higher than the ones in Bellemare et al.
77,"software, algorithm, color, screen, channel, atari",The HNeat Pixel score is obtained by using the special 8 color channel representation of the Atari emulator that represents an object label map at each channel . This method relies heavily on finding a deterministic sequence of states that represents a successful exploit . It is unlikely that strategies learnt in this way will generalize to
80,"deep, atari, learning, reinforcement, 2600",This paper introduced a new deep learning model for reinforcement learning . It demonstrated its ability to master difficult control policies for Atari 2600 computer games using only raw pixels as input . We also presented a variant of online Q-learning that combines stochastic minibatch updates with experience replay memory to ease the training of deep
82,"atari, games, 2600","In Proceedings of the 12th International Conference on Machine Learning , pages 30-37 . Morgan Kaufmann, 1995. Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function approximation. In Advance in Neural Information Processing Systems 25, pages 2222-2230, 2012. Marc G Bellemar"
84,"game, recognition, aerial, approximation, ap, function","Audio, Speech, and Language Processing, IEEE Transactions on, 20:30 -42, January 2012. Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. A neuro-evolution approach to general atari game playing. 2013. Nicolas Heess, David Silver, and Ye"
