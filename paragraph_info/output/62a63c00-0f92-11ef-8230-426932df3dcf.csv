element_idx,summarized_text,keywords
5,Local-window self-attention performs notably in vision tasks . It suffers from limited receptive field and weak modeling capability issues . MixFormer provides competitive results on image classification with EfficientNet .,"tasks, vision, self-attention, local-window, efficientnet"
7,"Vision Transformer in image classification validates the potential to apply Transformer to vision tasks . Challenges remain for downstream tasks, especially the inefficiency in high-resolution vision tasks and the ineffectiveness in capturing local relations . One possible solution is to use local-window selfattention .","transformer, vision, tasks"
12,"Windows are connected by shifting , expanding , or shuffling operations . Convolution layers are also employed as they capture natural local relations .","convolution, shuffling, self-attention, local-window, shifting"
15,"Mixing Block enlarges the receptive fields by modeling intra-window and cross window relations simultaneously . Second, we introduce bi-directional interactions across branches .","modeling, convolution, depth-wise, self-attention, local-window, ability"
16,MixFormer-B4 delivers a 2.2 mIoU gain over Swin-T on ADE20k . We achieve competitive results with EfficientNet by a large margin .,"computational, tasks, vi-sion, multiple, transformer, complexity"
18,ViT demonstrates great potentials to apply transformer to the computer vision community . ViT works like PVT and CvT insert spatial reduction or convolution before global self-attention .,"self-attention, convolution, vision, transformer"
20,"Global Vision Transformer shows its success on image classification . For high resolution vision tasks, the computation cost of the Vision Transformer is quadratic to image size, making it unaffordable for real-world applications .","mechanism, vision, window, attention, convnets, window-based, transformer"
21,Window-based Vision Transformer computes self-attention within nonoverlapping local windows . Convolution layers are used to create connections because they capture local relations in nature .,"windows, receptive, convolution, local, fields"
22,"Dynamic networks here refer to networks whose parts of weights or paths are data-dependent . The dynamic network achieves higher performance than its static alternative as it is more flexible in modeling relations . In ConvNets, the dynamic mechanism is widely used to better extract customized features given different inputs .","mechanism, network, networks, dynamic"
32,The Parallel Design brings computational efficiency2 . Itresults in limited receptive field due to no crosswindow connections being extracted .,"efficiency, windows, brings, computational"
34,"In this paper, we propose a parallel design that enlarges the receptive fields by simultaneously modeling intrawindow and cross-window relations . In detail, they use different window sizes .","convolution, parallel, design, depth-wise, self-attention, local-window"
38,"parallel design benefits two-folds: first, combining local-window self-attention with depth-wise convolution across branches models connections across windows simultaneously . second, parallel design models intra-wow and cross-werve relations simultaneously, providing opportunities for feature interweaving across branches and achieving better feature representation learning ","convolution, parallel, design, depth-wise, local-window, self-attention"
39,Localwindow self-attention computes weights on the fly while sharing weights across channels . We focus on this issue in this subsection .,"interactions, modeling, ability, sharing, weights, bi-directional"
40,depth-wise convolution shares weights on the spatial dimension while focusing on the channel . This can provide complementary clues for local-window self-attention and vice versa .,"spatial, dynamic, self-attention, channel-wise, local-window, weights, dimension"
44,"the bi-directional interactions consist of the channel and spatial interaction among the parallel branches . The information flows to the other branch through the channel interaction, which strengthens the modeling ability in the channel dimension .","spatial, channel, wise, interaction, convolution"
45,"channel interaction contains one global average pooling layer followed by two successive 1 x 1 convolution layers . At last, we use sigmoid to generate attention in the channel dimension . The input of our channel interaction comes from another branch, while the SE layer is performed in the same branch .","channel, interaction, window, self-attention, attention, module, local"
46,a simple design consists of two 1 x 1 convolution layers with followed BN and GELU . These two layers reduce the number of channels to one . At last a sigmoid layer is used to generate the spatial attention map .,"spatial, branch, convolution, depth-wise, attention, map"
49,MIX is a function that achieves feature mixing between the W-MSA branch and CONV branch . The MIX function first projects the input feature to parallel branches by two linear projection layers and two norm layers . Next it mixes the features by following the steps shown in Figure 1 and Figure 2 .,"mix, branch, convolution, projection, parallel, depth-wise, layer"
51,"MixFormer is a hybrid vision transformer, which uses convolution layers in both stem layers and downsampling layers . The projection layer increases the feature's channels to 1280 with a linear layer followed by an activation layer . It gives a higher performance in classification, especially works well with smaller models .","network, transformer, vision, mixformer"
55,"The number of blocks in different stages is set by following a recipe: putting more blocks in the last two stages, which is roughly verified in Table 2 . As shown in Table 2, we present the detailed settings of the models.","variants, table, architecture, 2"
59,We first verify our method by classification on ImageNet-1K . We train all models for 300 epochs with an image size of 224 x 224.,"optimizer, size, model, smaller, image, adamw"
60,Results. Table 3 compares our MixFormer with efficient ConvNets and various Vision Transformers . MixForMER performs on par with EfficientNet and outperforms RegNet by significant margins under various computational budgets. Previous works such as DeiT and PVT show dramatic performance drops when reducing,"swin, transformer, mixformer"
65,MixFormer train Mask R-CNN on the COCO2017 train split . Two training schedules are adopted to show a consistent comparison with previous methods .,"tasks, training, downstream, multiscale, mixformer"
74,MixFormer-B1 performs strongly with Mask R-CNN . The results suggest that implications for designing high performance small models on detection are highlighted .,"segmentation, mask, detection, r-cnn, mixformer"
85,MixFormer-B3 obtains 45.5 mIoU . HRNet is carefully designed to aggregate features in different stages . Note that HRNet simply constructs pyramid feature maps .,"task, swin-, prediction, swin-t, mixformer, dense"
87,"MixFormer-B1 provides ablations with respect to our designs on ImageNet-1K classification, COCO detection and segmentation, and ADE20K semantic segmentation . To make quick evaluations, the pre-train models provide slightly different results with the ones in Table 4 and Table 6.","imagenet-1k, ade20k, mixformer"
94,Table 7 shows the results of the proposed interactions . We see that both channel and spatial interactions outperform the model without interactions across all different vision tasks . Combining two interactions promotes better performance .,"interactions, light-weighted, bi-directional, designs"
95,Table 8 shows that the performance will drop significantly on various vision tasks if we reduce the window size of the depth-wise convolution from 3 x 3 to 1 x 1 . This phenomenon means that it's necessary to use a window size with the ability to connect across-window .,"depth-wise, window, size, convolution"
100,"shifted window fails to provide gains over MixFormer . We hypothesize that the depth-wise convolution builds connections between windows, removing the need for shift operation .","convolution, depth-wise, shift, operation, ffn"
101,"Ablation: Number of Blocks in Stages. Previous works usually put more blocks in the third stage . In Table 10, we achieve slightly higher performance on various vision tasks under less computational complexities .","blocks, tasks, vision, mixformer"
110,"Apply Mixing Block to ConvNets, ResNet50 and MobileNetV2 . To make a fair comparison, we adjust the number of blocks to maintain the overall computational cost . Mixing block brings 1.9% and 1.6% Top-1 accuracy on ImageNet1K .","mobilenetv2, convnets, [41, [41]"
112,Our MixFormer is proposed to mitigate issues in local-window self-attention . It may be limited to window-based vision transformers in this paper . More efforts are needed to apply our mixing block to global attention .,"mixing, self-attention, deit-tiny, block"
118,Our MixFormer enlarges receptive fields efficiently without shifting or shuffling windows . The bi-directional interactions boost modeling ability in the channel and spatial dimension .,"modeling, convolution, depth-wise, window, weak, local"
123,"In this section, we provide two instantiated models . Their detailed settings are provided in Table 13 along with previous methods . More variants can be obtained with further attempts following the design of MixFormer .","imagenet-1k, mixformer, mixformer-b0, mixformer-b5"
124,"MixFormer-B0 achieves competitive result even with 0.4G FLOPs . On the other side, it can achieve on par results with Swin-B . We believe that further efforts can be made to give higher performance .","mobile, level, state-of-the-art, result, mixformer"
137,Apply Mixing Block to DeiT. Although our mixing block is proposed to solve the window connection problem in local-window self-attention . It can also be applied to global attentions . We simply apply our mixing blocks to Deit-Tiny . But the result is slightly lower than baseline on ImageNet-1,"problem, mixing, deit-tiny, connection, window, block"
151,"We train all models for 160K on ADE20K . For testing, we report the results with single-scale testing . In multi-scale tests, the resolutions used are the x of that .","optimizer, ablation, studies, multi-scale, testing, adamw"
156,Conformer aims to couple local and global features across convolution and transformer branches . MixFormer uses channel and spatial interactions to address the weak modeling ability issues caused by weight sharing .,"spatial, conformer, interactions, modeling, ability, weak"
157,Twins and Shuffle Transformer + random spatial shuffle construct local and global connections to achieve information exchanges . Our MixFormer concatenates the parallel features: the non-overlapped window feature and the local-connected feature .,"connection, global, attention, sub-sampled, transformer, msg"
160,"An image is worth 16x16 words: Transformers for image recognition at scale . arXiv preprint arxiv:2107.00652, 2021 . 2 Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection ","image, transformer, vision, computer"
163,"A dataset for large vocabulary instance segmentation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5356-5364, 2019. 2, 11 Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yu","mobile, transformer, vision"
164,"Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2, 5, 9 Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and","visual, vision, recognition, design, network, computer"
167,"Efficientnet: Rethinking model scaling for convolutional neural networks . In International Conference on Machine Learning, pages 10347-10357 . PMLR, 2021. 1, 2, 4, 5, 6, 8, 10 Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, N","backbone, attention, visual, recognition"
169,"2, 6 Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, and Chuang Zhang . Contnet: Why not use convolution and transformer at same time? arXiv preprint arxiv:2107.00641, 2021 . 1, 2, 3, 5,","image, vision, recognition, transformer, computer"
