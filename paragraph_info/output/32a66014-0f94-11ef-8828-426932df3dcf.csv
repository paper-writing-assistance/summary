element_idx,keywords,summarized_text
4,"deep, analysis, learning, reinforcement, networks, neural","reinforcement learning agents must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations . The theory ofreinforcement learning provides a normative account1 deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour . This work bridges"
5,"neural, artificial, network, visual, cortex, architecture","We set out to create a single algorithm that would be able to develop a wide range of competencies on a varied range of challenging tasks . This is a central goal of general artificial intelligence13 that has eluded previous efforts8,14,15 To achieve this, we developed a novel agent, a deep"
8,"action-value, fun, function, approximator","Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator is used to represent the action-value function20 . This instability has several causes: the correlations present in the sequence ofobservations, the fact that small updates to Q may significantly change the policy and therefore change"
9,"q-network, update, function, q-learning",these methods involve the repeated training ofnetworks de novo on hundreds of iterations . This method is too inefficient to be used successfully with large neural networks . To perform experience replay we store the agent's experiences et = at each time-step t in a data set Dt = e
19,"network, architecture","We used the same network architecture, hyperparameter values and learningprocedure throughout-taking high-dimensional data as input-to demonstrate that our approach robustly learns successful policies over a variety of games based solely on sensory inputs with only very minimal prior knowledge ."
21,"learning, reinforcement, literature, dqn, agent","Our DQN method outperforms the best existing reinforcement learning methods on 43 of the 49 games where results were available12.15 . In addition to the learned agents, we also report scores for a professional human games tester playing under controlled conditions . a policy that selects actions uniformly at random and 0% on "
29,"dqn, agent","We demonstrate the importance of the individual core components of the DQN agent-the replay memory, separate target Q-network and deep convolutional network architecture-by disabling them ."
30,"similar, of, ization, embeddings, dqn, data, visual-, high-dimensional",W e next examined the representations learned by DQN that underpinned the successful performance ofthe agentin the context of Space Invaders . We also found instances in which the t-SNE algorithm generated similar embeddings for DQn representations of states that are close in terms of expected reward 
31,"competing, 2, table, extended, data, methods, games, outperforms",Audio output was disabled for both human players and agents . Error bars indicate s.d. across the 30 evaluation episodes .
32,"bottom, dqn, right",the network is able to learn representations that support adaptive behaviour from high-dimensional sensory inputs . We also show that the representations learned by DQN can generalize to data generated from policies other than its own-in simulations where we presented as input to the network game states experienced during human and agent play .
39,"dqn, invaders, space, agent",Figure 4 I Two-dimensional t-SNE embedding of the representations in the last hidden layer assigned by DQN to game states experienced while playing Space Invaders . The plot was generated by letting the agent play for 2h ofreal game time and running the tSNE algorithm25 on the
40,"dqn, planning, ball, strategy, strat","In certain games DQN is able to discover a relatively long-term strategy . Nevertheless, games demanding more temporally extended planning strategies still constitute a major challenge for all existing agents including ."
41,"learning, reinforcement, network, visual, cortex, architecture","In this work, we demonstrate that a single architecture can successfully learn control policies in a range of different environments with only very minimal prior knowledge . In contrast to previous work24,26, our approach incorporates 'end-to-end' reinforcement learning that uses reward to continuously shape representations within the convolutional network towards"
42,"bottom, right, left, screenshots",Partially completed screens are assigned lower state values because less reward is available . The screens shown on the bottom right and top left and middle are less perceptually similar than the other examples but are still mapped to nearby representations and similar values .
43,"learning, reinforcement, hippocampal, replay, machine","reactivation of recently experienced trajectories during offline periods21,22 provides a putative mechanism by which value functions may be efficiently updated through interactions with the basal ganglia22 . In the future, it will be important to explore the potential use ofbiasing the content of experience replay towards sal"
46,"animal, recognition, network, visual, cortex","Sutton, R. & Barto, A. Reinforcement Learning: An Introduction . Thorndike, E. L. Animal Intelligence: Experimental studies ."
52,"2600, learning, air, appro, games, function, approximation, atari","ACM 38, 58-68 . Riedmiller, M., Gabel, T., Hafner, R. & Lange, S. Reinforcement learning for robot soccer . Proc. Int. Conf. Mach. Learn. 240-247 . Krizhevsky"
53,"decision, reinforcement, learning, visual","Lin, L.-J. Reinforcement learning for robots using neural networks. Technical Report, DTIC Document . 24. Riedmiller, M. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method . Mach. Learn. Res. 9, 2579-2605"
56,"author, algorithms, testing, contributions, platform","Author Contributions V.M., K.K., D.S., J.V., M.G.B., A.A.R. and D.H. conceptualized the problem and the technical framework ."
62,"frames, emu-lator, 2600, preprocessing, raw, atari","w e apply a basic preprocessing step aimed at reducing the input dimensionality and dealing with some artefacts of the Atari 2600 emulator . First, to encode a single frame we take the maximum value for each pixel colour value over the frame being encoded and the previous frame "
64,"state, representation, neural, network, architecture","Model architecture allows us to compute Q-values for all possible actions in a given state with only a single forward pass through the network . The main drawback of this type of architecture is that there is a separate output unit for each possible action, and only the state representation is an input to the neural network. The output"
65,"rectifier, layer, nonlinearity, hidden",input to the neural network consists of an 84 x 84x4 image produced by the preprocessing map  . The first hidden layer convolves 32 filters of 8 x 8 with stride 4 with the input image and applies a rectifier nonlinearity31.32 .
66,"2600, emulator, training, network, atari, architecture","We performed experiments on 49 Atari 2600 games whereresults were available for all other comparable methods12.15 . The same network architecture, learning algorithm and hyperparameter settings were used across all games . We made one change to the reward structure of the games during training only ."
67,"rmsprop, lecture, structure","In these experiments, we used the RMSProp algorithm with minibatches ofsize 32 . The behaviour policy during training was E-greedy with 3 annealed linearly from 1.0 to 0.1 over the first million frames ."
68,"frame-skipping, atari2600, technique, skipping",the agent sees and selects actions on every kth frame instead of every frame . This technique allows the agent to play roughly k times more games without significantly increasing the runtime . We use k = 4 for all games .
69,"invaders, optimization, space, table, extended, data",We did not perform a systematic grid search owing to the high computational cost . The values and descriptions ofallhyperparameters are provided in Extended Data Table 1.
71,"evaluation, random, dqn, agent","The trained agents were evaluated by playing each game 30 times for up to 5 min each time with different initial random conditions and an E-greedy policy with 3 = 0.05 . The random agent served as a baseline comparison and chose a random action at 10 Hz which is every sixth frame, repeating its last action"
72,"2600, engine, human, emulator, tester, atari","professional human tester played under controlled conditions . As in the original Atari 2600 environment, the emulator was run at 60 Hz and the audio output was disabled ."
73,"actions, emulator, time-steps, atari","We consider tasks in which an agent interacts with an environment, in this case the Atari emulator, in a sequence of actions, observations and rewards . At each time-step the agent selects an action at from the set oflegal game actions, A= 1 . The action is passed to the emulator and mod"
74,"decision, states, state, markov, representation, emulator, (mdp), process","St = x1,a1,x2,...,at-1,xt, is input to the algorithm . All sequences in the emulator are assumed to terminate in a finite number of timesteps . This formalism gives rise to a large but finite Markov decision process ."
75,"future, action-value, optimal, rewards, function",The goal ofthe agentis to interact with the emulator by selecting actions in a way that maximizes future rewards . We make the standard assumption that future rewards are discounted by a factor of Y per time-step . T define the future discounted return at time t as Rt = - t rt
77,"equation, reinforcement, bellman, learning","The basic idea behind many reinforcement learning algorithms is to estimate the action-value function by using the Bellman equation as an iterative update, Qi+1 =Es ."
80,"q-network, equation, neural, network, bellman, function, approximator",a neural network function approximator can be trained by adjusting the parameters 0i at iteration i to reduce the mean-squared error in the Bellman equation . The optimal target values rty max ' Q* are substituted with approximate target values of y=r+y max
81,"weights, network","the targets depend on the network weights; this is in contrast with the targets used for supervised learning . At each stage of optimization, we hold the parameters from the previous iteration 0i fixed when optimizing the ith loss function Li . The final term is the variance of the targets, which does not depend on"
83,"deep, learning, reinforcement, algorithm, q-networks","This algorithm is model-free: it solves reinforcement learning task directly using samples from the emulator, without explicitly estimating the reward and transition dynamics P . In practice, the behaviour distribution is often selected by an E-greedy policy that follows the greedy policy with probability 1 - 8 and selects a random action"
84,"q-learning, time-step, replay, experience","During the inner loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience,  U, drawn at random from the pool of stored samples . This approach has several advantages over standard online Q-Learning . First, each step of experience is potentially used in many weight updates,"
86,"sophisticated, memory, application, sampling, replay",our algorithm only stores the last N experience tuples in the replay memory . This approach is limited because the memory buffer does not differentiate important transitions and always overwrites with recent transitions owing to the finite memory size N .
87,"target, q, online, network, q-learning",The second modification to online Q-learning is to use a separate network for generating the targets yj for the following C updates to Q . This modification makes the algorithm more stable compared to standard online Q learning . Generating the targets using an older set ofparameters adds a delay between the time
88,"absolute, value, error, term, loss, function",clipping the error term from the update r +y max ' l - Q to be between - 1 and 1 corresponds to using an absolute value loss function for errors outside of the interval .
96,"architecture, boltzmann, machines, multi-stage","IEEE. Int. Conf. Comput. Vis. 2146-2153 . 32. Nair, V. & Hinton, G. E. Rectified linear units improve restricted Boltzmann machines ."
101,"blue, representation, points, dqn, network, play, agent",the representations learned by DQN do indeed generalize to data generated from policies other than its own . The presence in the t-SNE embedding of overlapping clusters of points corresponding to the network representation of states experienced during human and agent play shows that the agent also follows sequences of states similar to those
106,"pressing'down', ball, interactive, atari, purely","All actions are around 0.7, reflecting the expected value of this state based on previous experience . At time point 2, the agent starts moving the paddle towards the ball and the value of the 'up' action stays high while the value falls to -0.9 . This reflects the fact that pressing 'down' would lead"
116,"sarsa, agent, learner, linear, function, approximator","Best Linear Learner is the best result obtained by a linear function approximator on different types of hand designed features12. Contingency agent figures are the results obtained in ref. 15. Note the figures in the last column indicate the performance of DQN relative to the human games tester, expressed as a percentage, thatis"
121,"frames, 2, table, extended, dqn, data, agents, training","DQN agents were trained for 10 million frames using standard hyperparameters for all possible combinations ofturning replay on oroff, using or notusing a separate target Q-network, and three differentlearning rates . Each agent was evaluated every 250,000 training frames for 135,000 validation frames and the highest average episode score is"
126,"dqn, agent, linearfunction, approximator","Agents were trained for 10 million frames using standard hyperparameters . Each agent was evaluated every 250,000 training frames for 135,000 validation frames and the highest average episode score is reported . Note that these evaluation episodes were not truncated at 5 min ."
