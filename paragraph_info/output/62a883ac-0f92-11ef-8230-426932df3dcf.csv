element_idx,summarized_text,keywords
3,We introduce adaptive input representations for neural language modeling . There are several choices on how to factorize the input and output layers . We perform a systematic comparison of popular choices for a self-attentional architecture .,"modeling, input, neural, language, adaptive, representations"
5,"Language modeling is a basic task in natural language processing . Recent progress has been made by neural methods based on LSTMs , gated convolutional networks and self-attentional networks .","modeling, lstms, language, recognition, speech"
6,Word-based models are particularly challenging since computing probabilities for all 800K words of the BILLION WORD benchmark is still a substantial part of the overall computation .,"probabilities, model, word-based, word, computation, benchmark, billion"
7,"the hierarchical softmax introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words .","computational, words, word, output, burden, embeddings"
8,"In this paper, we introduce adaptive input embeddings which extend the adaptive softmax to input word representations . This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words .","softmax, input, word, adaptive, embeddings, representations"
9,"We compare models based on word inputs, character inputs and sub-word units using a self-attention architecture . We also improve adaptive softmax by introducing additional dropout regularization in the tail projection .","word, representations, adaptive, inputs"
18,Merity et al. use a modified version of adaptive softmax which does not reduce the dimensionality of less frequent words . This setup is akin to a hierarchical softmax with tied weights .,"softmax, variable-sized, adaptive, output, embeddings"
19,"Merity et al. evaluates both character-based and word-based factorizations but does not directly compare them to each other . Recently, Al-Rfou demonstrated that self-attentional models can perform very well on language modeling tasks where the input and output is both characters .","model, word-based, character-based, factorization, attentional"
22,We define a number of clusters that partition the frequency ordered vocabulary V = V1 U V2 . We will refer to V1 as the head and to any subsequent clusters loosely as tail. We reduce the capacity for each cluster by a factor of k.,"cluster, frequency, vocabulary"
26,Weight sharing is simple except for the head where the adaptive softmax has n - 1 additional embeddings for the remaining clusters which are not shared with the input layer .,"softmax, weight, adaptive, sharing"
27,We share all projections except for the head projection which is not available in the adaptive softmax . Performance decreased when we added a head projection in the output . Sharing both the word embeddings and the projections performed very well on WIKITEXT-103 .,"softmax, pro, adaptive, projection, head"
30,We add sinusoidal position embeddings to the input layer and stack N = 16 blocks for both BILLION WORD and WIKITEXT-103 . Each block contains two sub-blocks: the first is a multihead self-attention module with H = 16 heads . The second is an feed-,"self-attention, module, architecture, ffn"
31,"We increase regularization for WIKITEXT-103 by using dropout 0.3, 0.1 ReLU dropout and 0.1 attention dropout. We use the same hyperparameters for all models trained on the same dataset .","regularization, word, rate, billion, models, dropout"
39,We limit the number of tokens per GPU to a maximum threshold B per GPU . For BILLION WORD models we use B = 2048 and train on 32 GPUs . This gives an effective batch size of 65K tokens for WIKITEXT-103 .,"word, billion, model, w"
41,"For fixed size word input layers and softmax output layers we generally use embeddings of size 512 for WIKITEXT- 103 . When we use an adaptive softmax in the output, then we use dimension 256 .","size, softmax, bpe, fixed, word, output, embeddings, layer"
42,"Character embeddings have size 128 and we apply seven filters of size 1x128, 2x256, 3x384, 4x512, 5x512 and 6x512 . We do not add start of word markers as they did not improve validation accuracy .","input, word, character, embedding, cnn"
43,adaptive softmax output layer trains models with large word-based vocabularies . We use embeddings of size d = 1024 for the head and reduce size of subsequent clusters by a factor of k = 4 .,"softmax, inputs, input, word-based, word, adaptive, output, vocabularies"
44,"We obtain a vocabulary of 33,337 tokens for WIKITEXT-103 . BPE input/output embeddings have size 1024 . The probability of a word is the product of the sub-word units .","bpe, encoding, byte-pair, models, sub-word"
46,Different to Vaswani et al. we use Nesterov's accelerated gradient method with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 . The learning rate is linearly warmed up from 10-7 to 1 for 16K steps and then annea,"method, rate, accelerated, learning, schedule, gradient"
47,BILLION WORD models train for a total of 975K updates over C = 3 cycles . The first cycle takes 137K steps and we set M = 0.75 . We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs .,"word, nccl2, library, billion, v100, gpu, nvidia"
56,"For the main results on BILLION WORD, we doubled the batch size by training on 64 GPUs instead of 32 GPUs . We consider two larger setups, one where we added four more blocks and increased the FFN dimension to eff = 6144 .","size, word, batch, billion, dimension, ffn"
57,Table 1 compares our models to previous work on BILLION WORD . The adaptive input model outperforms the best previously reported result at an order of magnitude fewer parameters . Our very large model performs as well as an ensemble of over ten models .,"fewer, input, model, adaptive, magnitude, parameters"
58,"Table 2 shows our result on WIKITEXT-103 where adaptive inputs achieve 18.7 perplexity . For this result only, we partition the training data into blocks of 3072 contiguous tokens instead of 512 tokens .","tokens, inputs, adaptive, perplexity"
66,"word units, both in the input and output, with embeddings of size 1024 and shared weights . Next, we consider replacing the fixed size output representations by an adaptive softmax and characters as input .","softmax, word, adaptive, self-attention, architecture, units"
67,Table 3 shows results when training all configurations for the same number of updates . Adaptive input representations with tied input and output layers achieve the highest accuracy at the same speed as the BPE models which have a very small vocabulary . ASM improves over SM and greatly speeds up training .,"softmax, fixed, input, word, adaptive, embeddings, representations, models"
68,Table 4 shows that adaptive input representations perform equally well on BILLION WORD compared to other factorizations . ADP-T is 34% faster than ADP because there are fewer parameters to update .,"input, word, adaptive, embeddings, representations"
77,Figure 2 shows results on WIKITEXT-103 . Fixed size word embeddings with a word softmax do not perform well on rare words . ADP-T performs best across all frequency ranges .,"bpe, rgb-t, rgb-, bpe-t, rgb"
85,Table 5 shows the importance of context size for WIKITEXT-103. Training block size is the number of consecutive tokens that are considered at once during training . Simply increasing the training block size from 512 to 3072 results in an improvement of nearly 1.2 perplexity .,"size, context, block, training"
87,"adaptive softmax can benefit from regularization of rare words . We add dropout to the output of the first projection for all clusters, except for the head .","softmax, word, adaptive, embeddings, rare"
93,"Adaptive input embeddings vary the size of input word embedderings . When sharing parameters with adaptive softmax, the number of parameters can be further reduced which improve training speed .","softmax, input, word, adaptive, embeddings"
138,Table 7 shows that reducing the capacity of fixed size word input embddings is beneficial on WIKITEXT-103 . We also experimented with sharing the head projection but found this perform less well than not sharing it .,"size, fixed, input, word, adaptive, embddings"
