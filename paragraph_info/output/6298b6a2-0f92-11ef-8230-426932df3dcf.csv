element_idx,keywords,summarized_text
4,"cvt, convolutional, transformer, vision",Convolutional vision Transformer improves Vision Transformer performance and efficiency by introducing convolutions into ViT to yield the best of both designs . This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding . These changes introduce desirable properties to the ViT architecture while
11,"nlp, architectures, architecture","architectures from language understanding with minimal modifications . First, images are split into discrete nonoverlapping patches . Then, these patches are treated as tokens ."
12,"cnn, transformers, tasks, vision","ViT lacks certain desirable properties inherently built into the CNN architecture that make CNNs uniquely suited to solve vision tasks . For example, images have a strong 2D local structure: spatially neighboring pixels are usually highly correlated."
17,"spatial, receptive, structure, fields, subsampling, local, hierarchical","tecture forces the capture of this local structure by using local receptive fields, shared weights, and spatial subsampling . This also achieves some degree of shift, scale, and distortion invariance ."
18,"convolutional, point, vision, floating, (flops), operations, transformer","In this paper, we hypothesize that convolutions can be strategically introduced to the ViT structure to improve performance and robustness . To verify our hypothesises, we present a new architecture, called the Convolutional vision Transformer ."
19,"spatial, convolution, self-attention, cvt, block, grid","The CvT design introduces convolutions to two core sections of the ViT architecture . First, we partition the Transformers into multiple stages that form a hierarchical structure of Transformers . The beginning of each stage consists of a convolutional token embedding that performs an overlapping convolution"
21,"performance, dynamic, attention, cvt, state-of-the-art","former employs all the benefits of CNNs: local receptive fields, shared weights, and spatial subsampling . Our results demonstrate that this approach attains state-of-the-art performance when CvT is pre-trained with ImageNet1k, while being lightweight and efficient ."
23,"convolutional, neural, cnns, transformer, networks","Transformers that exclusively rely on the self-attention mechanism to capture global dependencies have dominated natural language modelling . Recently, the Transformer based architecture has been viewed as a viable alternative to the convolutional neural networks in visual recognition tasks ."
24,"mod, multi-head, vision, self-attention, module, transformer","Vision Transformers is the first to prove that a pure Transformer architecture can attain state-of-the-art performance on image classification when the data is large enough . ViT decomposes each image into a sequence of tokens with fixed length, and then applies multiple standard Transformer layers ."
29,"position, multi-scale, encodings, conditional, tokenization",Transformer-iN- Transformer uses an outer Transformer block that processes the patch embeddings and an inner Transformer block to model both patch-level and pixel-level representation . Tokens-to-Token mainly improves tokenization in ViT by concatenating multiple tokens within a sliding window into
30,"transformer, architecture, structure","Table 1 shows the key differences in necessity of positional encodings, type of token embedding and type of projection . This work aims to achieve the best of both worlds by introducing convolutions into the Transformer architecture ."
32,"relation, global, weight, self-attention, local, architecture, backbone, networks, aggregation","the non-local networks are designed for capturing long range dependencies via global attention . The local relation networks adapts its weight aggregation based on the compositional relations between pixels/features within a local window, in contrast to convolution layers which employ fixed weights over spatially neighboring input feature ."
33,"convolutional, convolutions, projection, recognition, transformer","In NLP and speech recognition, convolutions have been used to modify the Transformer block . Other prior work proposes to propagate attention maps to succeeding layers via a residual connection . Convolutions are used to replace multihead attentions with convolution layers ."
37,"convolutional, vision, architecture, transformer, block","We introduce two convolution-based operations into the Vision Transformer architecture . Each stage has two parts: First, the input image are subjected to the Convolutional Token Embedding layer, which is implemented as a convolution with overlapping patches with tokens reshaped to the 2D spatial grid as the"
44,"token, convolutional, feature, layer, embedding, cnn, dimension","The Convolutional Token Embedding Layer allows us to adjust the token feature dimension and the number of tokens at each stage by varying parameters of the convolution operation . In this manner, in each stage we decrease the token sequence length, while increasing the token features dimension . This gives tokens the ability to represent"
47,"convolutional, self-, multi-head, projection, (mhsa), self-attention, attention, transformer, block","the proposed Transformer block with Convolutional Projection is a generalization of the original Transformer block . previous works try to add additional convolution modules to the Transformer Block . Instead, we propose to replace the original position-wise linear projection for Multi-Head Self-Attention with depth-wise separable con"
49,"token, convolutional, projection, linear, map","Figure 3 shows the original position-wise linear projection used in ViT . The tokens are first reshaped into a 2D token map . Next, a Convolutional Projection is implemented using a depth-wise separable convolution layer ."
58,"convolutional, s, convolution, projection, standard, x","Directly using standard s x s convolutions for the Convolutional Projection would require s2C2 parameters and O FLOPs, where C is the token channel dimension, and T is the number of tokens for processing . In this way, each of the proposed convolutional projection would only"
59,"value, information, convolutional, projection","The s x s Convolutional Projection permits reducing the number of tokens by using a stride larger than 1 . In this way, the numbers for key and value projection are reduced 4 times . This comes with a minimal performance penalty, as neighboring pixels/patches in images tend to have"
63,"token, convolutional, tokens-to-token, embedding, transformer, cnn","Tokensto-Token ViT implements a progressive tokenization, and then uses a Transformer-based backbone in which the length of tokens is fixed . By contrast, our CvT uses both convolutional token embeddings and Convolutional Transformer blocks in each stage . As the length"
64,"vision, pyramid, pvt, transformer, cnn","Pyramid Vision Transformer overcomes the difficulties of porting ViT to various dense prediction tasks . In ViT, the output feature map has only a single scale with low resolution ."
73,"convolutional, vision, cvt-x, transformer, block",Model Variants We instantiate models with different parameters and FLOPs by varying the number of Transformer blocks of each stage and the hidden feature dimension used . CvT-13 stands for Convolutional vision Transformer with X Transformer Blocks in total .
76,"vit, fine-tuning, optimizor","Fine-tuning We fine-tune each model with a total batch size of 512 . As in ViT, we pre-train our models at resolution 224 x 224, and fine tune at resolution 384 x 384."
79,"fewer, flops, flop, cvt, parameters, parameter",CvT-21 obtains a 82.5% ImageNet Top-1 accuracy . It is 0.5% higher than DeiT-B with the reduction of 63% parameters and 60% FLOPs .
89,"imagenet-22k, imagenet",We further investigate the ability of our models to transfer by fine-tuning models on various tasks . Our CvT-W24 model is able to obtain the best performance across all the downstream tasks considered .
98,"position, image, variable, computation, embedding, cvt","removing position embedding of DeiT-S would lead to 1.8% drop of ImageNet Top-1 accuracy . Position Embedding is often realized by fixed-length learn-able vectors . However, a wide range of vision applications take variable image resolution . Recent work CPVT tries to replace"
105,"token, convolutional, 6d, table, embedding","Table 6d is the CvT-13 model . When we replace the Convolutional Token Embedding with non-overlapping Patch Embendding, the performance drops 0.8% ."
109,"convolutional, performance, projection, position-wise, linear",Results are shown in Table 8. We study how the proposed Convolutional Projection affects the performance by choosing whether to use the regular Position-wise Linear Projection for each stage . We observe that replacing the original position-wise linear projection with the proposed convolutional projection improves the Top-1 Accuracy on
111,"cvt, convolution, vision, transformer","In this work, we have presented a detailed study of introducing convolutions into the Vision Transformer architecture . Extensive experiments demonstrate that the introduced convolutional token embedding makes our CvT architecture achieve superior performance while maintaining computational efficiency . Furthermore, due to the built-in local context structure introduced by con"
113,"imagenet, image, vision, processing, transformer, computer","Layer normalization, 2016. 4 Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xiaohua Zhai, and A ron van den Oord. Are we done with imagenet? arXiv preprint arxiv:2006.07159"
114,"mobile, image, vision, recognition, computer","In European Conference on Computer Vision, 2020 . 6 Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen . Up-detr: Unsupervised pre-training for object detection with transformers ."
116,"image, analysis, visual, recognition","Automated flower classification over a large number of classes . In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008. 6 Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar . Improving language understanding by generative pre-"
117,"image, vision, super-resolution, transformer, computer","Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks . In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794-7803, 2018. 3 Yuqing Wang, Zhaoliang Xu,"
