element_idx,keywords,summarized_text
3,"progressive, probabilistic, thermodynamics, nonequilibrium, models, lossy, decompression, scheme, diffusion","Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics . On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art"
5,"digital, matching, networks, score, generative, gan, adversarial, image","Generative adversarial networks , autoregressive models, flows, and variational autoencoders have synthesized striking image and audio samples . there have been remarkable advances in energy-based modeling and score matching ."
12,"probabilistic, markov, parameterization, model, network, chain, diffusion","A diffusion probabilistic model is a parameterized Markov chain . Transitions of this chain are learned to reverse a diffusion process . When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too ."
13,"diffusion, levels, dynamics, models, noise",Diffusion models are straightforward to define and efficient to train . a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training .
14,"log, likelihood, matching, score, model, diffusion","Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models . We find that the majority of our models' lossless codelengths are consumed to describe imperceptible image details ."
19,"gaussian, reparameterization, forward, process, reverse, conditionals","forward process variances Bt can be learned by reparameterization or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in po ."
25,"gaussian, parameterization, model, variable, distribution, diffusion","Diffusion models allow a large number of degrees of freedom in implementation . We establish a new explicit connection between diffusion models and denoising score matching that leads to a simplified, weighted variational bound objective ."
29,"constants, dependent, coordinatewise, unit, variance, time","The first choice is optimal for X0  N, and the second is optimal deterministically set to one point . These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance ."
37,"function, process, dynamics, approximator, reverse, langevin",we can train the reverse process mean function approximator mu to predict t or by modifying its parameterization . We have shown that the e-prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model's variational bound to an objective .
39,"gaussian, data, network, n, reverse, image","We assume that image data consists of integers in 0, 1,  , 255 scaled linearly to . This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior p ."
40,"variational, vae, bound, discretized, distributions, continuous, decoder","the variational bound is a lossless codelength of discrete data . At the end of sampling, we display noiselessly ."
47,"ncsn, edge, effects, bin, width","The t = 1 case corresponds to Lo with the integral in the discrete decoder definition approximated by the Gaussian probability density function times the bin width, ignoring 02 and edge effects . Algorithm 1 displays the complete training procedure with this simplified objective ."
48,"comparison, reweighting, reconstruction",Since our simplified objective discards the weighting in Eq. It is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational binding . This reweighting leads to better sample quality .
50,"evaluation, process, network, forward",We set the forward process variances to constants increasing linearly from B1 = 10-4 to BT = 0.02 . These constants were chosen to be small relative to data scaled to . We keep the signal-to-noise ratio at XT as small as possible .
53,"negative, likelihoods, cifar10, log","Table 1 shows Inception scores, FID scores, and negative log likelihoods on CIFAR10. Our unconditional model achieves better sample quality than most models in the literature . Our FID score is computed with respect to the training set ."
62,"objectives, variances, fixed, training, process, reverse","In Table 2, we show the sample quality effects of reverse process parameterizations and training objectives . The baseline option of predicting m works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq . We also see that learning reverse process"
64,"generative, model, likelihood-based, cifar10, diffusion",Table 1 also shows the codelengths of our CIFAR10 models . The gap between train and test is at most 0.03 bits per dimension .
65,"lossless, model, compressors, cifar10, diffusion",Our CIFAR10 model with the highest quality samples has a rate of 1.78 bits/dim . More than half of the lossless codelength describes imperceptible distortions .
66,"lossy, progressive, compression, eq.5","Progressive lossy compression We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq . Algorithms 3 and 4 assume access to a procedure, such as minimal random coding . When applied to X0 "
69,"distortion, reconstruction, test, stochastic, cifar10, set","Figure 5 shows the resulting ratedistortion plot on the CIFAR10 test set . At each time t, the distortion is calculated as the root mean squared error V ||xo - xo||2/D . The distortion decreases steeply in the low-rate region of the"
72,"progressive, 2, algorithm, generation, unconditional, decompression","Progressive generation We run a progressive unconditional generation process given by progressive decompression from random bits . In other words, we predict the result of the reverse process, x0 . Figures 6 and 10 show the resulting sample quality of xo . Large scale image features appear first and details appear last ."
83,"gaussian, coordinates, data, reordering, quality, diffusion",Gaussian diffusion model can be made shorter for fast sampling or longer for model expressiveness . Prior work has shown that such reorderings introduce inductive biases .
85,"celeba-hq, reconstruction, source, interpolation, image","In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images . We fixed the noise for different values of  SO Xt and x't remain the same . Larger t results in coarser and more varied interpolations "
87,"variational, inference, diffusion, dynamics, models, langevin","diffusion models are designed SO that q has no parameters and the top-level latent XT has nearly zero mutual information with the data X0 . Diffusion models admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference ."
88,"energy, curves, rate-distortion, modeling, based",Our rate-distortion curves are computed over time in one evaluation of the variational bound . Our progressive decoding argument can be seen in convolutional DRAW and related models and may lead to more general designs for subscale orderings .
91,"diffusion, data, quality, models, image",diffusion models seem to have excellent inductive biases for image data . We look forward to investigating their utility in other data modalities and as components in other types of machine learning systems .
93,"gan, generative, models, diffusion",Our work on diffusion models takes on a similar scope as existing work on other types of deep generative models . Our paper represents progress in making diffusion models a generally useful tool in this family of techniques .
94,"unlabeled, generative, cnn-generative, models, image","generative models can be employed to produce fake images and videos for political purposes . Fortunately, CNN-generated images have subtle flaws that allow detection . Generative models also reflect biases in datasets on which they are trained ."
95,"digital, compression, data, models, diffusion",diffusion models may be useful for data compression . Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks .
99,gsns,"In International Conference on Learning Representations, 2019. Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use"
101,"energy, compression, visual, based, dynamics, models","In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3603-3613, 2019. Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Flow contrastive estimation of energy-based models . In International Conference on"
104,"of, inference, approx-imate, waveglow, generative, modeling, quality, image","Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
106,"analysis, dynamic, patterns, audio, raw, image","Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. A-NICE-MC: Ad"
120,"resnet, model, wide, cifar10, pixelcnn++",Our neural network architecture follows the backbone of PixelCNN++ . Our 32 x 32 models use four feature map resolutions . All models have two convolutional residual blocks per resolution level .
121,"v3-8, cat, tpu, lsun, cifar",Our CelebA-HQ/LSUN models train at 21 steps per second at batch size 128 . Sample a batch of 256 images takes 17 seconds .
123,"rmsprop, schedule, linear, quality, cifar10","We chose the Bt schedule from a set of constant, linear, and quadratic schedules, all constrained SO that LT 21 0. We set the dropout rate on CIFAR10 to 0.1 by sweeping over the values 0.1, 0.2,0.3,0.4 . Without dropout on C"
126,"celeba-hq, t, ttur, cifar10","Sample quality scores and log likelihood are reported on the minimum FID value over the course of training . On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI and TTUR repositories ."
129,"ncsn, metric, convolutions, quality, dilated",NCSN uses a RefineNet with dilated convolutions . We condition all layers on t by adding in the Transformer sinusoidal position embedding . NCSN omits this scaling factor .
132,"x750, intermediate, stochasticity, latent, x0","During sampling, both the prior XT  N and Langevin dynamics are stochastic . To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA 256 x 256 dataset . Figure 7 shows multiple draws from the"
