element_idx,keywords,summarized_text
3,"self-discovery, critical, ing, task-intrinsic, self-discover, reasoning, think-",Core to the framework is a selfdiscovery process where LLMs select multiple atomic reasoning modules . SELF-DISCOVER significantly improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks .
5,"texts, llm, coherent, transformers",Large Language Models powered by transformers have produced impressive breakthroughs in generating coherent texts . various prompting methods have been proposed to enhance LLMs' ability to reason and solve complex problems .
8,"generalization, prompting, decomposition-based, compositional, step-back","step-back prompting is inspired by how humans solve complex problems . a fundamental limitation is that each technique serves as an atomic reasoning module . Instead, we argue that each task has a unique intrinsic structure ."
9,"tasks, into, breakdown, self-discover, pro-gram, sub, reasoning","This paper aims at self-discovering the underlying reasoning structure unique to each task . Our approach, SELF-DISCOVER, is inspired by how humans internally devise a reasoning program for problem-solving . Stage 1 operates at the tasklevel and uses three actions to guide the LLM to generate"
15,"math, modules, self-discover, bbh, guides",SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L . BBH results are in Appendix C Table 3.
17,self-discover,"We test SELF-DISCOVER on 25 challenging reasoning tasks including Big Bench-Hard , Thinking for Doing and MATH . The self-discovered reasoning structure outperforms CoT on 21/25 task with performance gains up to 42% ."
21,"knowledge, self-discover, skills, and",We design SELF-DISCOVER to enact these steps into two stages . We take inspiration from how humans use prior knowledge and skills to devise a reasoning program .
22,"json, meta-reasoning, module, reasoning",Stage 1 of SELF-DISCOVER aims to uncover the intrinsic reasoning structure for solving this task via meta-reasoning . We format the structure in key-value pairs similar to JSON due to interpretability and findings on following JSON boosts reasoning and generation quality .
29,"reasoning, task-solving, module, adapted","The first stage consists of three actions: 1) SELECT, where relevant reasoning modules for task-solving are chosen from the set of reasoning module descriptions; 2) ADAPT, where descriptions of selected reasoning modules are rephrased to be more specific to the task at hand; and 3) IMPLEMENT, where the"
30,"reflective, self-discover, reasoning, module, thinking","SELECT First, not every reasoning module is helpful for every task . For example, ""reflective thinking"" might help search for first-principle theories on science problems . ""creative thinking"" helps on generating a novel continuation to a story ."
32,"reasoning, arithmetic, module, problems",ADAPT rephrases each module to be more specific to the task . SELECT uses a meta-prompt PA and a generative model M to generate the adapted reasoning module descriptions DA:
33,"modules, reasoning, implement, structure",SELF-DISCOVER operationalizes the reasoning modules into an implemented reasoning structure DI with specified instruction on what to generate for each step . IMPLEMENT also provides a demonstration of a human-written reasoning structure Shuman on another task to better convert the natural language descriptions into a reasoning structure.
44,"task, math, big-bench, t4d","BBH tasks cover a diverse range of reasoning problems . We test on a grounded social agent reasoning task called Thinking for Doing . For evaluations, we use accuracy to measure the model performance ."
51,"atomic, cot, answer, reasoning","Direct Prompting, where model directly generates the answer without intermediate reasoning steps . Plan-and-Solve, where models are prompted to first generate a plan and then solve the problem ."
63,"discovering, structures, capabilities, reasoning, llm","We will show qualitative examples of self-discovered structures, LLM output following the structures, and compare with LLM input following other prompting methods ."
70,self-discover,"SELF-DISCOVER achieves 69% and 85% accuracy on PaLM 2-L and GPT-4 . In contrast, the reasoning structure is generated automatically from atomic reasoning modules without human intervention ."
71,"structures, analysis, 2-l, palm, reasoning, error","MATH observes a moderate gain of 1%-7% on PaLM 2-L from SELF-DISCOVER compared to baselines . The majority of the failures comes from errors in executing the computations, consistent with prior findings ."
73,"diverse, tasks, knowledge, world, self-discover, reasoning",Figure 4 presents the average improvement in terms of delta in accuracy of SELFDISCOVER over direct answer and CoT . We adopt the categorization from Suzgun et al.
81,"method, inference, accuracy, wise, boosting, calls, self-consistency","Figure 5 shows average accuracy and number of inference calls required per instance for each method using GPT-4 . Accuracy wise , we find that SELFDISCOVER outperforms other baselines even those that require repeated inference call calls such as CoT-self-consistency and majority voting of applying"
86,"self-discover, reasoning, multiple","CoT, Plan-and-Solve, and SELFDISCOVER make incorrect assertions early . This leads the model to generate logical conclusions and arrive at the correct answer ."
93,"appendix, e, select","In Sec. 5.2, we demonstrate the universality of the self-discovered reasoning structures by applying the structures discovered by PaLM 2-L to GPT-4 . We further show the commonalities between the reasoning structures and human reasoning patterns in Appendix E ."
95,self-discover,"We conduct ablation study on the three actions: SELECT, ADAPT, and IMPLEMENT to analyze the effects of SELFDISCOVER actions . Figure 8 shows results using GPT-4 on 4 reasoning tasks . We find that with each stage, model's zero-shot reasoning capability improve consistently across"
100,"gpt-4, optimization","We first use a PaLM 2-L model to discover the reasoning structures of 4 reasoning tasks . Then, we apply the resulting reasoning structures to the decoding of GPT-4 as grounding . We compare our approach to OPRO which discovered zero-shot-prompts through optimizations ."
105,"gpt-4, llam, llama2-70b",Applying GPT-4 Discovered Structures to Llama2 and ChatGPT Motivated by transferrability performance across LLMs . We find that using self-discovered structures outperforms CoT on disambiguation QA zero-shot and on GPT-3.5-turbo on geometry with
108,"light-shot, prompting, few-shot, shot, few, stepback, llms","Recent advancements in the area of LLMs have given rise to a plethora of few-shot and instruction prompting techniques . Some of the techniques include Chain-of-Thought prompting , Leastto-most prompting, Decomposed prompting and Stepback Prompting ."
110,"prompting, self-discover, methods, self-discovery","SELF-DISCOVER presents the missing piece in the prompting literature . Composing over prompting methods is analogous to programming literature where a program is written using various basic building blocks such as loop, if/else condition etc."
112,"summarization, decomposition, ques, self-discover, question","SELF-DISCOVER allows models to combine multiple reasoning approaches by selfcomposing into a structure without the need to access task labels . The work combines skills in-context such as SkiC , devising a strategy , and planning with iterative quering ."
114,"self-discover, human-ai, collaboration","SELF-DISCOVER is an efficient and performant framework for models to self-discover a reasoning structure for any task from a seed set of general problem-solving skills . We observe drastic improvements on challenging reasoning benchmarks from multiple LLMs up to 30% . Forward looking, we are excited to explore more"
139,"content, co, customized, aclanthology","Findings of the Association for Computational Linguistics: ACL 2023, pp. 11834-11890, Toronto, Canada, July 2023 ."
183,"math, matching, extracted, bbh, answers, t4d, exact","We use accuracy and exact matching as with other methods tested on BBH, T4D and MATH . We manually examine each task's outputs and design heuristics to extract the final answers ."
192,"analysis, information","Try creative thinking, generate innovative and out-of-the-box ideas to solve the problem . Focuses on logical reasoning, evidence-based decision-making, and identifying potential biases or flaws in thinking . Explore unconventional solutions, thinking beyond traditional boundaries, and encouraging imagination and originality ."
198,"self-discover, errors, step-wise, calculations","out of 99 examples where the model prediction is wrong, wrong reasoning structures account for only 25.3% of the errors . The remaining 74.7% errors are due to errors in the intermediate calculations such as math computations . Table 5 shows 3 examples of such errors. This insight indicates that future improvements should aim at improving the step-"
200,"patterns, bbh-navigation, human, reasoning, task",Model-Discovered Reasoning Structures vs. Human Reasoning Patterns We investigate whether LLM-discovered reasoning structures share some commonalities with human reasoning patterns . We give humans 3 task instances without labels and an example reasoning structure and ask them to write a reasoning structure for a task before solving it .
