element_idx,keywords,summarized_text
4,"text-to-music, music-specific, inspired, music-domain-knowledge-, conditions, models, diffusion","Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion . Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key ."
8,"al., text-to-music, mustango, agostinelli, generation, model, et","Generating music directly from a diffusion model poses some unique challenges . First, music adheres to specific rules related to, for example, tempo, key, and chord progressions . For instance, MusicLM , a text-to-music model, ensures that the generated music matches the text prompts in"
11,"dataset, musicbench","We use state-of-the-art music information retrieval methods to extract such control information from our training data . To diversify the music samples in the training set, we augment this dataset with variants of the existing music, altered along three aspects-tempo, pitch, and volume . The text descriptions are also altered accordingly "
13,"musiccaps, text-to-music, mustango, model, munet, diffusion","Mustango is a text-to-music diffusion model with our novel MuNet module . We release the Musi cBench dataset with 53K pairs of music audio and description . This is achieved by altering the music samples of MusicCaps along the harmony, tempo, and volume dimensions ."
18,"downbeat, beat, tempo","BeatNet extracts beats and downbeats, chords, keys, and tempo . We use BeatNet to extract beat features, 6 E R Lbeats x2 . The second feature tempo is estimated by averaging the reciprocal of the time interval between beats ."
19,"appendix, i",We refer to these as control sentences and they will be appended to the original text prompt to form the enhanced prompts . A full list of the different control sentence templates can be found in Appendix I .
21,"prompts, augmentation, dataset, text","our dataset augmentation for both music audio and text prompts increases the total amount of training data 11-fold . Standard text-to-audio augmentations may not work for music due to overlapping rhythms, dissonance in harmony, and overall musical concept mismatch ."
23,"shift, au-, pitch, dio, music, pyrubberband2","We shift the pitch of the music audio within a range of 3 semitones . We change the speed of the audio by %, drawn from a uniform distribution . Finally, we alter the volume by introducing a gradual volume change with the minimum volume drawn from an uniform distribution from 0.1 to "
24,"appendix, chatgpt, audio, text","The text descriptions are enhanced and modified in tandem with the alterations to the music audio . To enhance the robustness of the model, we randomly discard one to four sentences from the prompt that describe the aforementioned music features ."
26,"dataset, audio-, audioset, audio-dioset, caps","MusicCaps dataset comprises 5,521 audio clips featuring music . Each clip is 10 seconds long and is sourced from the train and evaluation splits of the AudioSet dataset ."
32,"sampl, pipeline, augmentation, sample, pitch-shifted","The higher quality samples are altered to form a set of 37k augmentation samples . In the case of pitch-shifted samples, instead of randomly sampling from a uniform distribution, we used all 6 unique semitone shifts . We pick 0/1/2/3/4 prompts with a probability of 25 /30/20"
38,"prior, audio, co, zo, computational, audioldm, extracted, language, tango","Inspired by Tango and AudioLDM, we leverage the latent diffusion model to reduce computational complexity . More specifically, we aim to construct a latent audio prior zo extracted using an extra variational autoencoder with condition C . In our case refers to a joint music and text condition."
40,"noise, gaussian, reverse, diffusion","reverse process reconstructs 20 from Gaussian noise ZN  N . Intuitively, backward diffusion aims to reconstruct the latent audio prior Zn-1 from the previous step Zn until zo ."
44,"cross-attention, mha, block, attention, multi-headed","MHA is the multi-headed attention block for the cross attentions . Q, K, and V are query, key, and value respectively . FLAN-T5 is the text encoder model adopted from Tango ."
45,"blocks, cross-attention, fme, multiple, downsampling, munet, upsampling","MuNet consists of a UNet -consisting of multiple downsampling blocks . We use two encoders, Encb and Encc, to encode beat and chord features . These leverage both the state-ofthe-art Fundamental Music Embedding and an onset-and-"
46,"embedding, timing, encb, beat, encoding",Encb encodes beat types 6 using One-Hot Encoding and beat timings 6 with Music Positional Embedding . By concatenating these beat types and timing encodings and passing them through a trainable linear layer .
50,"of, embedding, ing, able, fundamental, layer, the, chord, linear, train-, music","ing of the chords . Subsequently, this concatenated representation is passed through a trainable linear layer . Notably, we incorporate a music-domain-knowledge informed music embedding through the use of the Fundamental Music Embedding from Guo et al."
51,"layer, cross-attention, denosion",We use two additional cross-attention layers to integrate beat and chord embeddings . Tango used one to incorporate only text conditions . MuNet leverages both music and text features .
55,"count, value, timestamps, float, beat",the beat count takes an integer value between 1 and 4 for the instances in our training data . The interval durations are predicted as a float value from the second token onwards .
56,"flan-t5, verbalized, model, timestamps, large, beats",We use the sequence to sequence FLAN-T5 Large model as the chords predictor . The model takes the concatenation of the text caption and the verbalized beats as input .
61,"of, text-to-music, mustango, text-to-audio, baselines, model, musicgen, audiocaps, musicbench","Mustango has a similar architecture with Mustango, except for the extra conditioning module: MuNet . We train the following three models from scratch: Tango trained on MusicCaps TrainA . Mustango fine-tuned from pre-trained Tango checkpoints ."
64,"fmacaps, electronic, models","We curated 1,000 pseudo-captioned evaluation samples from Free Music Archive . The details of creating FMACaps are reported in Appendix F."
71,"fmacaps, control, ability, controlability, sentences, testb","Controllability Evaluation We evaluate each model's controllability using TestB and FMACaps . We first generate music based on the text prompts and then extract the musical features mentioned in 2.1 . Subsequently, we define nine metrics to evaluate whether the music properties in the generated music match the"
84,"musicgen, audioldm2, fmacaps, musicbench","TestA, TestB and FMACaps are presented in Table 1 . Both Tango variants trained on MusicCap are inferior to the other four models . Mustango trained from scratch is comparable in FD and KL to both pre-trained versions ."
85,"musicgen, audioldm2","MusicGen and AudioLDM2 differ from what was reported in their original papers . This is due to MusicBench representing a different, more challenging, split of data than the MusicCaps evaluation set . The results show more improvement than Mustango when evaluated on FMACaps as compared to TestA and TestB"
86,"metrics, controllable, symmetry, tems, tempo, sys-","In Beat metrics, all the models perform comparably, except for MusicGen, which performs better . Similarity in performance could be caused by the MusicCaps dataset already containing enough information about tempo ."
87,"musiccaps, fmacaps, musicbench, mustango",Mustango outperforms all the other models on TestB and placed second on FMACaps . Mustango performs fine in Beat and Tempo metrics and excels in Key and Chord controllability .
93,"appendix, g, musiccaps","In the first round of the general listening test, subjects listened to ten generated music samples for each of the four models . The ten text prompts were custom-made by music experts in the style of MusicCaps, and are shown in Table 7 in the Appendix ."
94,"appendix, audio, j, training, music",experts with at least five years of formal musical training can identify music attributes from music audio . Samples consisted of ten contrasting pairs that aimed to target musical controllability.
95,"fmacaps, metric, audio, bias, quality","In the second round of the general listening test, we used the same 10 captions as in the first run . For both of these tests, we downsampled the MusicGen samples to 16 kHz to eliminate audio quality bias in listeners' responses ."
97,"mustango, rhythm, training, presence, musicbench, musical","A total of 48 participants participated in the first round of the general listening test, of which 26 had more than five years of formal musical training . The results in Table 3 show the average ratings for each of the metrics defined above . We can clearly see that the Tango baseline model is outperformed in all metrics by the models trained"
98,"mustango, listening, 3, table, controllability",A total of four experts participated in the controllability listening study . The results of the expert listening study in Table 3 confirm that both Mustango models outperform Tango baselines in all metrics .
99,"expert, audioldm2, test, mustango","In the second run, a total of 17 general audience listeners and 4 experts participated . We performed a series of paired t-tests on the obtained results . Mustango outperforms MusicGen and AudioLDM2 in terms of REL ."
104,"soundscape, mustango, audiocaps, pre-training, dataset",We initialized Mustango with a pre-trained Tango checkpoint and fine-tuned it using the AudioCaps dataset . This Tango model was pre-trained using 1.2 million text-audio paired samples .
105,"performance, controlability, mustango",We prove the effectiveness of MuNet in 4.5 and 4.7 . We show that MuNet significantly enhances the performance of Mustango in terms of controllability .
107,"musicgen, music",Mustango gives state-of-the-art performance in music quality and drastic improvement in music controllability . Mustango is trained on a publicly available dataset of relatively small size compared to other available text-to-music systems such as MusicGen .
110,"controllable, text-to-music, generation, music-theory, musicbench",Mustango is a controllable diffusionbased text-to-music system inspired by musicdomain knowledge which is able to generate music that follows certain music properties embedded within user-specified text prompts . The integration of the MuNet module within Mustango enables greater music controllability over stateof-the-art text
112,"mustango, controlability, generation, music",Mustango is currently limited to generating music of up to ten seconds . Adapting Mustango for generating long-form music is left for future work .
118,"nlp, space, knowledge, commonsense, for, ski, huggingface, 2021-04_06",This project has received funding from SUTD MOE grant SKI 2021_04_06 and AcRF MoE Tier2 grant . This work is also supported by the Microsoft Research Accelerate Foundation Models Academic Research program .
121,ism,"Dmitry Bogdanov, Nicolas Wack, Emilia Gomez Gutierrez, Sankalp Gulati, Herrera Boyer, Oscar Mayor, Gerard Roma Trepat, Justin Salamon, Jose Ricardo Zapata Gonzalez, Xavier Serra ."
162,"separately, input, feature","We determine the probability of masking a prompt as min%, where N represents the number of sentences in the current prompt, and M is the average number of sentence per prompt . Once the prompt is chosen for masking, we randomly draw an integer X from a uniform distribution in the range and proceed to remove X%"
163,"tango, robustness, input, text","The idea behind the first two dropouts is to enable the model to work with incomplete, faulty, or missing input information . The third dropout is aimed at improving robustness for short text inputs ."
166,"control, predictions, sentences, noise","During the inference phase, we utilize pre-trained predictors for chord and beat predictions based on textual prompts . This predictor exhibit exceptional performance when the prompts explicitly contain chord information, achieving accuracy of 94.5 % on the TestB dataset."
167,"control, testa, sentences, predictors",TestA serves as a scenario where control sentences are not included in textual prompts . The latter outperforms the former across most metrics . This observation indicates that the control predictors do not compromise the performance of Mustango .
169,"cmot, cmoot, cmo","the metrics on TestB are PCM - 16.15, ECM - 33.95, CMO - 39.81, and CMOT - 47.82 . Mustango tends to follow the chords predicted by the chord predictor quite often ."
170,"folk, song","""This folk song features a female voice singing the main melody. This is accompanied by a tabla playing the percussion. A guitar strums chords. This song has minimal instruments . The chord sequence is Bbm, Ab."
173,"bluesy, medium, melody, tempo","""A female singer sings this bluesy melody. The song is medium tempo with minimal guitar accompaniment and no other instrumentation. The key of this song is G minor. The time signature is 3/4."
176,"predicted, chords, actual",We can observe some substitutions in the actual chords detected from the audio compared to the predicted chords . These substitutions are very close musically and could even be a consequence of the feature extraction system not being 100% accurate . The substitution of Bbm for F#maj7 is more of a change at first
177,"predictor, chord, behavior",the generated samples follow a pattern of two chords that alternate . A more elaborate study on the Chord predictor behavior should be a topic for future work .
182,"timestamps, cbench, beat, music",In Figure 3 we can see the mel-spectrogram generated by pre-trained Tango finetuned on Musi cBench . The music appears abruptly in contrast to the sample generated by Mustango depicted in Figure 4 where the rhythm is very consistent . These predicted beat timestamps
183,"starts, chord, controllability, perceived, musicbench",Mustango seems to follow the predicted chords as well as their starting time . This is also clear from listening to the sample and seeing the spectrogram with perceived chord starts . The substitution of the C7 chord for C can be a minor mistake .
189,"audio, prompt, text-to-music, generation",AudioLM uses the state-of-the-art semantic model w2v-Bert to generate the semantic tokens from audio prompt . These tokens condition the generation of acoustic tokens that are decoded using SoundStream to generate audio.
190,"ldm, representation, audioldm, model, language","AudioLDM is a text-to-audio framework that leverages CLAP , a joint audio-text representation model, and a latent diffusion model . Tango leverages the pre-trained VAE and replaces the CLAP model with an instruction fine-tuned large language model: FLAN"
191,"digital, audio, audioworkstations, generation, midi, music","In the field of music generation, there is a long history of generated MIDI music . The focus ofconditional music generation within the audio domain has centered around musical conditions, such as note intensity or tempo . More recently, however, models that directly generate audio music from text captions have emerged ."
192,"to, autoregressively, tokens, acoustic",MusicLM outperforms two existing commercially available text-to-music software . These acoustic tokens are then decoded to become the final audio output .
193,"candidate, candidat, generic, multiple, noise2music","LaMDALF , a large language model, is used to generate multiple generic candidate text descriptions . The obtained music and text pairs are then used to train a two-stage diffusion model, where the first diffusion model generates an intermediate representation ."
195,"dmae, autoencoder, jen-1, generation, magnitude, text-guided, music, diffusion","Schneider et al. proposes a 2-stage diffusion model in which the first diffusion magnitude autoencoder learns . in the second diffusion model, text condition along with latent acquired at the first stage is included to guide the final music generation . MusicGen uses a single-stage transformer LM with efficient token"
197,"fmacaps, audio, audio-tagging, model, fma","We took 1,000 random samples from FMA-large and clipped out a random 10-second fragment from each of them . Then, we used Essentia's tagging models to assign tags to audio . Next, we instructed ChatGPT to perform an in-context learning task to get pseudo-pro"
199,"evaluation, listening, participation, tests",human evaluation participants recruited through email contacts of various music research and machine learning communities . Their age ranged from 15 to 60+ years . Participation was free of reward and voluntary .
207,"caption, musical, terms, text","""I have a song for which the caption is the following:"" You should generate only the caption and nothing else . The length of the new caption should be no more than eight sentences ."
210,"voice, accordion, singing, male, main, melody",This folk song features a male voice singing the main melody in an emotional mood . There is no percussion in this song . This song can be played at a Central Asian classical concert .
212,"voice, sings, male, main, central, asia, melody","The accordion fills the background while a violin provides a droning melody . There is no percussion in this piece, which can be played at a classical concert ."
221,"rock, ambient, smooth, instrumental, piece","This instrumental blues song goes very slow at a bpm of 50 . It includes reggae electric guitar, horn, and percussion like bongos . The keyboard provides lush chords. The time signature is 4/4. The harmonica plays a solo over the harmonious guitar and bass ."
223,"pairs, acoustic, guitar, table, 7, strumming, contrasting","Table 7 presents the 20 text prompts used for the expert listening studies . They consist of 10 contrasting pairs written by music experts . For example, caption 1 in Table 7 contrasts with caption 2 ."
224,"guitar, acoustic","The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute . 2 A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Adagio."
235,"drums, solid-bodied, song, metal, guitar, bass","bassist wielding a solid-bodied bass guitar adds depth and power to the sonic landscape . As the song begins, the guitar roars to life, delivering distorted chords ."
