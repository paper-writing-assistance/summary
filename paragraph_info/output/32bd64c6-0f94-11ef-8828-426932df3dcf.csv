element_idx,keywords,summarized_text
4,"few-shot, natural, state-of-the-art, language, pal","Large language models have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks . ProgramAided Language models : a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python"
7,"guage, lan-, reasoning, llm, large, models","Until as recently as two years ago, reasoning was considered to be one of the most significant challenges that large language models had not yet overcome . Recent LLMs have shown impressive success on a wide range of tasks, including commonsense , mathematical , and symbolic reasoning ."
8,"steps, intermediate, llm, reasoning, explicit","LLMs can decompose natural language problems into steps and perform simple arithmetic operations . In fact, even when fine-tuning a PaLM-based model on 164B tokens of explicit mathematical content, its two most common failures are ""incorrect reasoning"""
9,"language, natural, pal, programming","Program-Aided Language model is a novel method that uses an LLM to read natural language problems and generate programs as reasoning steps . This offloads the solution step to a Python interpreter, as illustrated in Figure 1 . The LLM can decompose a natural language problem into programmatic steps "
17,"symbolic, prompting, reasoning, pal, chain-of-thought, ai","We demonstrate the effectiveness of PAL across 13 arithmetic and symbolic reasoning tasks . In all these tasks, PAL outperforms much larger models such as PaLM-540B using chain-of-thought prompting . For example, on the popular GSM8K benchmark PAL achieves state-"
19,"test-time, prompting, few-shot, llm, large-language, input, models",Fewshot prompting leverages the strength of large-language models to solve a task with a set of k examples that are provided as part of the test-time input . These input-output examples ki=1 are concatenated in a prompt p  x1 
22,"of, steps, intermediate, in-context, chain, example, thought","Wei et al. augment each in-context example with chain of thought intermediate steps . Xi, ti, Yi are input-output pair as before . Yi is a natural language description of the steps that are needed to arrive at the output Yi ."
23,"ytest, answer, llm","the model is tasked with generating both the thought ttest and the final answer Ytest . This approach of prompting the model to first generate a reasoning process . During inference, the new question Xtest the prompt ."
25,"program-, model, ming, aided, natural, language, pal","In a Program-aided Language model, we propose to generate the thoughts t for a given natural language problem x . Every in-context example in PAL is a pair xi, ti, where tj = with each Si E NL U PL, a"
32,"prompts, pal, leveraging, leverage","In our experiments, we leveraged the prompts of existing work whenever available . In all cases, we augmented the free-form text prompts into PAL-styled prompts ."
33,"basket, variable, llm, names","In Section 6 we show that such meaningful variable names are critical . Notably, it is also possible to incrementally run the PL segments and feed the execution results back to the LLM to generate the following blocks ."
36,"hard, control, in-context, big-bench, examples","Data and in-context examples We experiment with three broad classes of reasoning tasks: mathematical problems from a wide range of datasets including GSM8K , SVAMP , ASDIV , and MAWPS . For all of the experiments for which CoT prompts were available, we use the same in"
49,"code, code-davinci-002, prompting, di-rect, pal","Baselines We consider three prompting strategies: DIRECT prompting . We performed greedy decoding from the language model using a temperature of 0. Unless stated otherwise, we used CODEX as our backend LLM for both PAL, DIRECT, and CoT."
53,"gsm8k, gsm8, gsm-hard, larger, number, non-integer","LLMs can generalize to larger and non-integer numbers? We constructed a harder version of GSM8K, which we call GSM-HARD . The numbers in a question were replaced with a random integer of up to 7 digits ."
55,"descriptions, hard, natural, big-bench, language, pal","COLORED OBJECTS requires answering questions about colored objects on a surface . This task requires keeping track of relative positions, absolute positions, and the color of each object . Figure 4 shows an example for a question and example PAL prompt ."
63,pal,Table 1: Problem solve rate on mathematical reasoning datasets. The highest number on each task is in bold . The results for DIRECT and PaLM-540B are from Wei et al.
66,"algorithmic, copy, word, repeat, reasoning",OBJECT COUNTING involves answering questions about the number of objects belonging to a certain type . REPEAT COPY requires generating a sequence of words according to instructions .
71,"gsm-hard, direct, gsm-h","the accuracy of DIRECT drops dramatically from 19.7% to 5.0% . The accuracy of CoT drops from 65.6% to 20.1% . PAL remains stable at 61.5%, dropping by only 14.3% ."
76,"thoughts, gsm-hard, numbers, natural, llm, large, language","In 16 out of 25 cases we analyzed, CoT generates nearly identical natural language ""thoughts"" the primary failure mode is the inability to perform arithmetic accurately ."
79,"multi-sample, methods, chain-of-thought-style, generation",We then repeated the greedydecoding experiments using nucleus sampling with p = 0.95 and k = 40 . This further increases the accuracy of PAL from 72.0% to 80.4% on GSM8K .
85,"complexity, colored, question, input, objects, pal","We examined how the performance of PAL and CoT change as the complexity of the input question grows . As the number of objects in the question increases, COT's accuracy is unstable and drops ."
91,"code-cushman-001, code-davinci-001, lm, base, weaker","We compared PAL with CoT when both prompting approaches use the same weaker base LMs code-cushman-001 and code-davinci-001 . This shows that PAL can work with weaker models, while its benefit scales elegantly to stronger models ."
92,"code, ability, modeling, text-davinci-003, text-davinci-002","PAL is not limited to LMs of natural language . PAL outperforms CoT, and PAL text-davinci-003 performs almost as PAL code . Figure 8 shows that PAL can work with LM mainly trained for natural language."
93,"pal, interpreter, prompt","We created prompts that are similar to PAL's, except that they do include the final answer . This resulted in a 23.2 solve rate on GSM8K, much lower than PAL . The main benefit of PAL comes from the synergy with the interpreter ."
100,"code, intermediate, quality, comments, nl","In COLORED OBJECTED and DATE, removing intermediate NL comments but keeping meaningful variable names slightly reduces the results compared to the full PAL prompt . Removing variable names further decreases accuracy, and performs worse than CoT ."
107,"prompting, code-generation, chain-of-thought","Methods such as chain-of-thought prompting have further unlocked a variety of reasoning tasks . PAL avoids these problems by offloading the calculation and some of the reasoning to a Python interpreter, given the right program ."
108,"code, generates, pal",PAL generates code for a Python interpreter without specialized modules and ad-hoc fixes . Chowdhery et al. have also experimented with external calculators . PAL improves Codex by 6.4% on the same benchmark .
110,"benchmark-specific, prompt, examples",a preprint of our work was submitted to arXiv . PoT only demonstrates efficacy on mathematical problems . We chose benchmark-specific prompt examples .
111,"python, context-free, sql, sql-like, parsing, semantic, grammar, pal","Semantic parsing Our work generates free-form Python code . Some works constrain the decoder using a Context-Free Grammar to generate a domainspecific meaning representation or a canonical utterance . In contrast, PAL does not require any constraining or domain-specific representations other"
115,"python, curacy, ac-, natural, reasoning, state-of-the-art, language, pal","We introduce PAL, a new method for natural language reasoning, using programs as intermediate reasoning steps . The main idea is to offload solving and calculating to an external Python interpreter . This results in a final answer that is guaranteed to be accurate ."
117,"grounding, robotic, affordance","arXiv preprints arxiv:2204.01691, 2022 . Irpan, K., Jesmonth, S., Joshi, N. J., Kalashnikov, D., Kuang, Y."
120,"language, models","In NeurIPS, 2020, Language Models are Few-Shot Learners . In a new study, language models are few-shot learners ."
122,"analysis, modelling","arXiv preprints arxiv:2107.03374, 2021a . evaluating Large Language Models Trained on Code ."
126,"language, modeling, scal-ing","arXiv preprints arxiv:2204.02311, 2022 . . A scaling Language Modeling with Pathways ."
132,"benchmark, ben, gem","GEM Benchmark: Natural Language Generation, its Evaluation and Metrics . arXiv preprint arxiv:2102.01672, 2021 ."
155,"zero-shot, task, generalization","Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, D., Scao, T. L. . Multitask Prompted Training Enable"
171,"pal, reasoning",A Alternative Prompts without Meaningful Variable Names 13 B Additional analysis on Arithmetic Reasoning 13 C Effect of Using Language Models of Code 14 D Analyzing the Effect Of Increasing Number of Samples on PAL 14 E Standard Deviations Across Multiple Order of Promptes 17 F PAL
184,"explanation, gains, text, communication, performance","Table 4 shows a significant performance drop: from an average of 71.8% to 59% . Note that the ablation where structured outputs are completely removed is precisely the CoT setting, which achieves a solve rate of 63% . combining both text and procedural statements leads to higher performance gains ."
192,"multi-step, prompting, breakdown, direct, pal",Succinct Code The programs used in few-shot examples by PAL are multi-step . Can we return a single line expression to calculate the result? Results in Table 6 shows that is not the case with single-line expressions .
193,"intermediate, pal, chain, reasoning","To investigate this, we experiment with a variant that forces the LLM to generate the answer after generating the reasoning chain . The results show that the solve rate drops to near DIRECT levels . This reinforces our hypothesis that current LLMs can be excellent at specifying a high-level plan to solve a task"
197,"code, text-davinci-001, language, model","In our experiments, we focused on evaluating the performance of a language model for code . We aimed to investigate whether the additional performance boost observed in our results was due to the use of models like Codex . Our findings indicate that the PAL approach is not restricted to working solely with Codex, but can also be"
226,"symbolic, in-context-learning, pal-style, methods, reasoning, chatgpt","In this section, we demonstrate examples of tasks that may not initially appear to require using programs as intermediate reasoning steps, but can be improved through the use of PAL-style reasoning ."
228,"answer, chatgpt","ChatGPT does not change the answer after this explicit reasoning . Explicitly instructing the model to perform step-by-step reasoning before answering the question still yields the wrong answer . In this case, PAL-style reasoning only takes few lines of code ."
254,"in, first, the, syllable, third, letters, counting","Count the letters in the first syllable, ""in"" - there are 2 letters . Then, count them in the third . 4. Add up all the letters counted in each ."
267,"colored, log-likelihood, reasoning, llm, objects, chains, dataset",We randomly selected 20 questions from the COLORED OBJECTS dataset . We then manually compared the two mechanisms by focusing on tokens with a low log-likelihood .
268,"of, control, number, objects, indexing","CoT often has lower confidence in tokens related to numbers and quantitative information . We found that this occurred in seven, SIX, two, and six examples out of the 20 we examined . In contrast, PAL uses list manipulations, such as len , accesses objects and their associated properties through list and index"
286,"correctness, of, correct, gsm8k, program, answer, the, sampling, nucleus","For 71% of the examples where PAL is correct on GSM8K, we use the generated program and replace the initial value with the larger values . Running the program could automatically produce the correct answer of the harder question . For the incorrect 29% of the cases, we run PAL again and perform nucleus sampling with"
295,muhammad,"Q : How old was Mohamed four years ago? A : Four years ago, Kody was half as old as Mohamed, SO Kody must have been 56 / 2 =  28 years old . The answer is 32 ."
302,"reducing, problem, least-to-most, solving","LEAST-TO-MOST solves problems in two stages, problem-reducing and problem-solving . Problem reducing stage turns the problem into sub-problems, and the solving stage solves them sequentially . It keeps two prompts, each for an individual stage ."
308,"letters, pal, last",Zhou et al. experiment with 500 examples and record results in Table 12 . PAL can take advantage of the problem decomposition offered by the LEAST-TO-MOST reducing .
323,"james, under, old, penguins, years, 8","The age of Louis is 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the height of Bernard is 80 cm . We now add a penguin to the table: James, 12, 90, 12 How many penguins are less than 8 years old"
375,"boolean, data","result = for i in range : result.append if i == 2: result-append print )  # Q: Print boolean eleven times, but after the 3rd and 8th also say correct result . java twice and data once, and repeat all of this three"
387,"non-gold, items, objects, pal, counting","PAL often performs better on questions that involve counting objects that satisfy one or several conditions . For example, CoT fails in the following example: ""On the desk, you see a bunch of items arranged in a row"" With pure NL reasoning, a LLM can easily lose track of the objects and output"
388,"syntax, pl, pink, objects, filtering",PAL is able to accurately construct the object lists with correct order and attributes . Figure 4 lists the last a few lines of the solution generated by PAL . PAL can further compose such operations across multiple reasoning steps .
390,"vincent, bernard","Louis is 7 years old, Bernard is 5 years old and Vincent is 9 years old . Bernard  is 12 years old. Bernard is 12 . There are 2 penguins less than 8 years old so the answer is 2 ."
394,"removal, penguin, penguins, bernard, pal","Figure 24 lists the generations of CoT and PAL to the question that contains the removal of a penguin . CoT picks up the critical information that ""penguins that are less than 8 years old are Louis and Bernard"" it still fails to aggregate the information properly and infer that there is one penguin less than"
400,"jane, birthday",Jane was born on the last day of Feburary in 2001 . Today is her 16-year-old birthday . What is the date 24 hours later in MM/DD/YYY?
