element_idx,keywords,summarized_text
3,"physical, of, approximators, bayesian, model, methods, vaes, scenes, function",Variational Autoencoders have emerged as one of the most popular approaches to unsupervised learning of complicated distributions . VAEs are appealing because they are built on top of standard function approximators . They can be trained with stochastic gradient descent .
6,"modeling, generative, image","""Generative modeling"" deals with models of distributions P, defined over datapoints X . Each ""datapoint"" has thousands or millions of dimensions . The generative model's job is to somehow capture the dependencies between pixels ."
10,"of, something, images, graphic, raw, designers, models, 3d","We could start with a database of raw images and synthesize new, unseen images . We might take in a databases of 3D models of something like plants and produce more of them to fill a forest . Tools like this might actually be useful for graphic designers ."
11,"backpropagation, approximators, function, training","training this type of model has been a long-standing problem in machine learning community . Most approaches have had one of three serious drawbacks . First, they might require strong assumptions about the structure in the data . Third, they may rely on computationally expensive inference procedures like Markov Chain Monte Carlo ."
13,"audience, bayesian, vision, methods, vaes",This tutorial is intended to be an informal introduction to VAEs . It is aimed at people who might have uses for generative models . This tutorial began its life as a presentation for computer vision reading groups .
16,"character, handwritten","the more complicated the dependencies between the dimensions, the more difficult the models are to train . Say for simplicity that we only care about modeling the digits 0-9 . If the left half of the character contains the right half of a 5 then the right . half cannot contain the left . 50% of the "
17,"probability, density, dataset, function","Formally, say we have a vector of latent variables Z in a high-dimensional space Z which we can easily sample according to some probability density function P defined over Z . f is deterministic, but if z is random and 0 is fixed, then f will be a random variable in"
19,"x, x-, x-z, f(z), distribution","In VAEs, the choice of this output distribution is often Gaussian, i.e., P = N, 2 * I . That is, it has mean f and covariance equal to the identity matrix I times some scalar 0 . This replacement is necessary to formalize the intuition"
23,"dirac, output, gradient, gaussian, delta, descent, distribution, function","In general, and particularly early in training, our model will not produce outputs that are identical to any particular X . By having a Gaussian distribution, we can increase P by making f approach X for some z, i.e. gradually making the training data more likely under the generative model "
25,"sparse, decoder, autoencoder","VAEs approximately maximize Equation 1 according to the model shown in Figure 1 . They are called ""autoencoders"" only because the final training objective that derives from this setup does have an encoder and a decoder ."
31,"of, information, dimensions, z, latent, structure","Ideally, we want to avoid deciding by hand what information each dimension of Z encodes . VAEs assume that there is no simple interpretation of the dimensions of Z, and instead assert that samples of Z can be drawn from a simple distribution, i.e., N ."
39,"d, dimensions, values","If Z is 2D and normally distributed, g = z/10 + z/|||z|| is roughly ring-shaped, as shown in Figure 2 . If f is a multi-layer neural network, then we can imagine the network using its first few layers to map the normally distributed z"
41,"cumulative, inverse, sampling, distribution, function, transform","In one dimension, you can use the inverse cumulative distribution function of the desired distribution composed with the CDF of a Gaussian . For multiple dimensions, do the stated process starting with the marginal distribution for a single dimension ."
43,"gaussian, handwritten, vision, digits, distribution","mize the model using stochastic gradient ascent . We first sample a large number of Z values Z1, ..., Zn, and compute P 22 1nEi P . The problem here is that in high dimensional spaces, n might need to be extremely large before we have an"
45,"q(z), equation, 1, qp(x)","The key idea behind the variational autoencoder is to try to sample values of Z that are likely to have produced X, and compute P just from those . This means that we need a new function Q which can take a value of X and give us a distribution over Z values likely to produce X"
52,"autoencoder, right, side, hand","the right hand side of Equation 5-has suddenly taken a form which looks like an autoencoder . Log P is ""decoding"" it to reconstruct X . We'll explore this connection later ."
53,"helmholtz, vaes, machines",Helmholtz Machines assume a discrete distribution for latent variables . This choice prevents transformations that make gradient descent tractable in VAEs .
55,"5, log, equatinon, p(z), model","Equatinon 5 starts with the left hand side, we are maximizing log P while simultaneously minimizing D . Q will hopefully actually match P, in which case this KLdivergence term will be zero, and we will be optimizing log P ."
57,"equation, of, right, 5, side, hand","Q = N, where 14 and are arbitrary deterministic functions with parameters 0 that can be learned from data . The last term-D -is now a KL-divergence between two multivariate Gaussian distributions, which can be computed in closed form as:"
59,"equation, p(x), 5, log","Equation 5 uses sampling to estimate EzQ . Hence, as is standard in stochastic gradient descent, we take one sample of Z and treat log P for that Z as an approximation ."
66,"q, equation, 9","EzQ depends not just on the parameters of P, but also on Q . However, in Equation 9, this dependency has disappeared! In order to make VAEs work, its essential to drive Q to produce codes for X that P can reliably decode ."
70,"backpropagation, gradient, stochastic",Stochastic gradient descent via backpropagation can handle stochastic inputs . Given and -the mean and covariance of Q -we can sample from N . The equation we actually take the gradient of is:
71,"backpropagation, gradient, trick, symbol","This function is deterministic and continuous in the parameters of P and Q . For a fixed 7, either h needs to ignore X or there needs to be some point at which h ""jumps"" from one possible value in Q's sample space to another, i.e., a discontinuity "
76,"5, testing, model, equa-tion, example","D is positive, meaning that the right hand side of Equation 5 is a lower bound to P . This lower bound can't quite be computed in closed form due to the expectation over Z, which requires sampling ."
78,"equation, 5-in, of, optimization, information, penalty, the-, ory, sparsity, terms","this section aims to take a deeper look at what the objective function is actually doing . First, we ask how much error is introduced by optimizing D in addition to log P ."
80,"parameterization, gaussian, distribution, function","The tractability of this model relies on our assumption that Q can be modeled as a Gaussian with some mean and variance . For fixed P, this might mean that DQ|P] never goes to zero ."
82,"appendix, a, algorithms, learning, proof, error, approximation, technique, machine","in practice such a small 0 might cause problems for existing machine learning algorithms, since the gradients would become badly scaled . however, itis comforting to know that VAEs have zero approximation error in at least this one scenario . This fact suggests that future theoretical work may show us how much approx"
84,"interpretation, equation, gain, 5, information",Log P can be seen as the total number of bits required to construct a given X . The right hand side of Equation 5 views this as a two-step process . It measures the amount of extra information that we get about X when Z comes from Q instead of from P .
86,"equation, 5, sparsity, regularization","Equation 5 is a regularization term in sparse autoencoders . It's interesting to ask if the variational autoencoding has any ""regularization parameter"" the sparsity regularization parameter looks something like this ."
89,"parameter, variational, regularization, autoencoder","a variational autoencoder does not have such a regularization parameter, which is good because thats one less parameter that the programmer needs to adjust . This parameter can come from changing Z  N to something like z' N, but it turns out that this doesn't change the model "
90,"output, bernoulli, parameter, model, data, continuous, regularization","a good choice for the output distribution for continuous data is P  N, o2 * I) for some 0 we supply . Note that the existence of this parameter relies on our choice of the distribution of X given z . If X is binary, we can actually count the number of bits that"
92,"filling, digits, hole, handwritten","Let's return to our running example of generating handwritten digits . Say that we don't just want to generate new digit, but instead we want to add digit to an existing string . This is similar to a truly practical problem in computer graphics called hole filling ."
96,"variational, autoencoder, conditional, cvae, multimodal, distribution","a standard regression model will fail in this situation, because the training objective generally penalizes the distance between a single prediction and the ground truth . In the case of digits, this will most likely look like a meaningless blur . What we need is an algorithm that takes in a string or an image,"
97,"autoencoder, regression, denoising, model","denoising autoencoder can be seen as a slight generalization of the regression model . We would say that the ""noise distribution"" simply deletes pixels . Note that this still doesn't solve the problem ."
108,"capabilities, variational, learning, autoencoder, mnist, distribution","to demonstrate the distribution learning capabilities of the VAE framework, lets train a variational autoencoder on MNIST . To show that the framework isn't heavily dependent on initialization or network structure, we dont use existing published VAE networks ."
112,"x', x'i, mnist, xi-xi, x-xi, xi","Cross entropy measures the expected probability of X' . Thus were actually modeling X, the randomly binarized version of MNIST . The training was run to completion exactly once ."
118,"d, z, dynamics, dimension","Results with 1,000 z's were good, but with 10,000 they were also degraded . in theory, a model with m >> n should not be worse, since the model can simply learn to ignore the extra dimensions ."
121,"regressor, value, mnist, network, cvae","CVAE models are most likely to outperform simple regression when the output is ambiguous given a training example . In MNIST, each pixel has a value between 0 and 1, meaning there is still enough information even in this single column of pixels for the network to identify a specific training example. The second"
122,"object, information, and, cs294, group, ucb, recognition, visual, activity, misc-read, cmu","Thank you to everyone in the UCB CS294 Visual Object And Activity Recognition group and CMU Misc-Read group . I would like to thank Philipp Kr henbuhl, Jacob Walker and Deepak Pathak for helping me formulate and refine my description of the method ."
127,"p(z)dz, qo(z)/pr(z), pgt","We make the dependence on 0 explicit here since we will send it to 0 to prove convergence . The theoretical best possible solution is where Po == Pgt and D = 0. By ""arbitrarily powerful"" learners, we mean that if there exist f, h and which achieve this bestposs"
128,"pg(z), d[qg(z)]","Note that D is invariant to affine transformations of the sample space . Let Qo = N,g'2) and Po = Po + ) * o. When I write P, I am using the PDF of Z as a function ."
137,"deep, generative, backpropagation, model","Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Auto-encoding variational Bayes . ICLR, 2014. 2. Tim Salimans, Derderik Kingm . Markov chain monte carlo and variational inference"
139,"deep, static, generative, natural, conditional, models, image","In NIPS, 2015. 8. Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting from static images using variational autoencoders . In ECCV, 2016. 10. Bruno A Olshausen and David J Field. Emergence of simple"
141,"opti-, energy, dynamic, stochastic, free, autoencoders, mization","In NIPS, 1994. 22. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embeddding ."
