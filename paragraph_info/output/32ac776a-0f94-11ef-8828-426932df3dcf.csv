element_idx,keywords,summarized_text
6,"representation, networks, long-standing, neural, challenges, recurrent, hierarchical, temporal",recurrent neural networks have been considered as a promising approach to resolve this issue . There has been a lack of empirical evidence showing that this type of models can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales .
8,"deep, spatial, representation, networks, neural, data, hierarchical","One of the key principles of learning in deep neural networks as well as in the human brain is to obtain a hierarchical representation with increasing levels of abstraction . A stack of representation layers, learned from the data in a way to optimize the target task . The recent resurgence of recurrent neural networks has led"
9,"multiscale, long-term, architecture, rnns, dependencies",multiscale RNNs group hidden units into multiple modules of different timescales . based on the observation that high-level abstraction changes slowly with temporal coherency . low level abstraction has quickly changing features sensitive to the precise local timing .
15,"dynamic, variables, timescale, dynamics, timescales","the most popular approach is to set the timescales as hyperparameters instead of treating them as dynamic variables that can be learned from the data . however, considering the fact that non-stationarity is prevalent in temporal data, and that many entities of abstraction such as words and sentences are in variable length, we claim that"
16,"short-term, multiscale, memory, novel, long, (lstm), neural, (hm-rnn), recurrent, network, hierarchical, (","In this paper, we propose a novel multiscale RNN model . This model can learn the hierarchical multiscale structure from temporal data without explicit boundary information . We find that this model tends to learn fine timescales for low-level layers . To do this, we introduce a binary boundary detector at"
17,"handwriting, sequence, generation, state-of-the-art, hm-rnn",HM-RNN achieves state-of-the-art results on Text8 and handwriting sequence generation . We compare results to the standard RNN on the handwriting sequencing sequence generation using the IAM-OnDB dataset .
18,"estimator, rnn, straight-through, training",We propose for the first time an RNN model that can learn a latent hierarchical structure of a sequence without using explicit boundary information . We show that it is beneficial to utilize the above structure through empirical evaluation .
20,"multiscale, bengio, schmidhuber, hihi, &, rnn, hierarchical, el, structure","Schmidhuber and El Hihi & Bengio propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN ."
23,"multiscale, update, lstm, concept","LSTMs employ the multiscale update concept, where the hidden units have different forget and update rates . However, unlike our model, these timescales are not organized hierarchically . We hypothesize that this property mitigates the vanishing gradient problem more efficiently ."
24,"lstm, hard, cw-rnn, soft, timescales","The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates . In the biscale RNNs, the authors proposed to model layer-wise timescales adaptively by having additional gating units ."
25,"hrnn, utterances, utter, dialogue, architecture","Hierarchical RNN architectures have been proposed in the cases where the explicit hierarchical boundary structure is provided . In Ling et al., after obtaining the word boundary via tokenization, the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers "
26,"1-d, spatial, networks, neural, kernels, convolutional",Kim and Kim et al. proposed convolutional neural networks with 1-D kernels for language modelling and sentence classification . also proposed to obtain high-level representation of sequences of reduced length .
28,"generalization, bernoulli, stochastic, depth, zoneout, distribution, regularization","In Zoneout, an identity transformation is randomly applied to each hidden unit at each time step according to a Bernoulli distribution . This results in occasional copy operations of previous hidden states . Our model learns to dynamically determine when to copy from the context inputs ."
35,"modelling, language, rnn, data, hierarchical, temporal",In Figure 1 we depict a hierarchical RNN for language modelling with two layers . the first layer receives characters as inputs and generates word-level representations . The second layer takes the word level representations as input .
36,"word-level, representation, end-of-word, label, c2w-rnn",the C2W-RNN obtains word-level representation after processing the last character of each word . The W2P-RNn performs a update of the phrase level representation . Note that the hidden states remain unchanged while all the characters of a word are processed .
37,"hierarchical, multiscale, structure","the W2P-RNN is updated at a much slower update rate than the C2W RNN . a considerable amount of computation can be saved, gradients are backpropagated . layer-wise capacity control becomes possible ."
38,"boundaries, higher-level, natural, rnn, concepts, units",can an RNN discover hierarchical multiscale structure without explicit boundary information? This is a legitimate problem when we consider higher-level concepts which we would like the RNN to discover autonomously .
44,"layer, below, representation, state","Using the boundary states, each layer selects one of the following operations . The selection is determined by the boundary state of the current time step in the layer below zt-1 ."
49,"layer, lower, representation, summary, flush, operation","The COPY operation is performed to update the summary representation of the layer l if the boundary zt-1 is detected from the layer below . The UPDATE operation is executed sparsely unlike the standard RNNs where it is executed at every time step . If a boundary is detected, the FL"
58,"hm-lstm, layer, 1, l-, l, gate, binary","the HM-LSTM has a top-down connection from to l, which is allowed to be activated only if a boundary is detected at the previous time step of the layer l . This allows the layer to be initialized with more long-term information after the boundary was detected and execute the FLUS"
63,"backpropagation, straight-through, training, estimator","the straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass is replaced by a differentiable functions during the backward pass . The straight through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estim"
64,"annealing, hard, trick, slope, sigmoid","In our experiments, starting from slope a = 1 we slowly increase the slope until it reaches a threshold with an appropriate scheduling . The idea is to reduce the discrepancy between the two functions used during the forward pass and the backward pass . Note that starting with a high slope value from the beginning can make"
70,"prize, states, wikipedia, hutter, hm-lstm, test, bpc, boundary, signals, error, set, detector",This model is a variant of the HM-LSTM that does not discretize the boundary detector states . This method uses test error signals for predicting the next characters .
74,"corpora, text, parameter, bpc, benchmark","0 is the model parameter, N is the number of training sequences, and Tn is the length of the n-th sequence . We evaluate our model on three benchmark text corpora: Penn Treebank, Text8 and Hutter Prize Wikipedia."
75,"output, embedding, layer, rnn, module, input","Model We use a model consisting of an input embedding layer, an RNN module and an output module . The output module is the HM-LSTM, described in Section 3 with three layers . In order to adaptively control the importance of each layer at each time step, we introduce three scalar"
82,"validation, backpropagation, log-likelihood, negative",Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation . We train the model using Adam with an initial learning rate of 0.002 . The norm of the gradient is clipped with a threshold
83,"decision, hard, hm-lstm, soft, boundary","In Table 1 , we compare the test BPCs of four variants of our model to other baseline models . Note that the HM-LSTM using the step function for the hard boundary decision outperforms the others using either sampling or soft boundary decision ."
84,"layer, text8, dataset, hm-lstm","Text8 The Text8 dataset consists of 100M characters extracted from Wikipedia corpus . Text8 contains only alphabets and spaces, and thus we have total 27 symbols . In order to compare with other previous works, we follow the data splits used in Mikolov et al."
85,"prize, wikipedia, hutter, markups, mar, xml","Hutter Prize Wikipedia The dataset contains 205 symbols including XML markups and special characters . We follow the data splits used in Graves where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set . In Table 1 we show"
86,"multiscale, boundary, hm-lstm, learned, detectors, hierarchical, structure","We visualize the boundaries detected by the boundary detectors of the HM-LSTM while reading a character sequence of total length 270 taken from the validation set of either the Penn Treebank or Hutter Prize Wikipedia dataset . Due to the page width limit, the figure contains the sequence partitioned into three segments of length 90 "
94,"boundary, telephone, information, flushing","In Figure 3, we observe that the 21 tends to detect the boundaries of the words . In Figure 4, we also see flushing in the middle of a word . Note that ""tele"" is a prefix after which a number of postfixes can follow ."
95,"detection, of, frequency, flush, boundary, de",The model learns that it is more beneficial to delay the information ejection to some extent . This is somewhat counterintuitive because it might look better to feed the fresh update to the upper layers at every time step without any delay .
96,"states, copy, hidden, detectors, boundary","In Figure 4, we depict the heatmap of the l2-norm of the hidden states along with the states of the boundary detectors . The color of ||h1 II changes quickly because there is no COPY operation in the first layer ."
97,"of, update, the, rnn, frequencies, architecture","A notable advantage of the proposed architecture is that the internal process of the RNN becomes more interpretable . For example, we can substitute the states of 21 and z2-1 into Eq. 2 and infer which operation among the UPDATE, COPY and FLUSH was applied to the second layer at time step "
106,"handwriting, iam-ondb, pen-tip, location, examples","IAM-OnDB dataset consists of 12, 179 handwriting examples, each of which is a sequence of coordinate and a binary indicator p for pen-tip location . At each time step, the model receives , and the goal is to predict . The pen-up indicates an end of a stroke"
107,"iam-ondb, output, embedding, hm-lstm, layer, log-likelihood, dataset","In Table 3, we compare the log-likelihood averaged over the test sequences of the IAM-OnDB dataset . In Figure 5, we let the HM-LSTM to read a random picked validation sequence and present the visualization of handwriting examples by segments based on either the states of 2 or the states"
109,"handwriting, results, state-of-the-art, hm-rnn, sequences","In this paper, we proposed the HM-RNN that can capture the latent hierarchical structure of the sequences . In order to implement these operations, we introduced a set of binary variables and a novel update rule that is dependent on the states of these binary variables . Each binary variable is learned to find segments"
114,theano,"The authors would like to thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussion . We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, Samsung, IBM, Facebook, Google, Microsoft, NSERC, Calcul Quebec, Compute"
