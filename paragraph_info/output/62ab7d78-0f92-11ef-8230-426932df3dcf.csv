element_idx,summarized_text,keywords
1,Tom B. Brown* Benjamin Mann* Nick Ryder* Melanie Subbiah* Jared Kaplan T Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert- Voss Gretchen Krueger,"benjamin, mann"
3,"Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting . This method still requires task-specific fine-tuning datasets of thousands or tens","text, large, fine-tuning, gpt-3, corpus, few-shot, of, task-specific"
8,"Introduction 3 2 Approach 6 2.1 Model and Architectures   8 2.2 Training Dataset  9 2.4 Training Process  10 3.1 Language Modeling, Cloze, and Completion Tasks 11 3.2 Closed Book Question Answering  13 3.3 Translation  14 3.4 Winograd","reasoning, glue, sense, common, crawl, filtering"
11,"Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways . First, single-layer representations were learned using word vectors and fed to task-specific architectures . then RNNs with multiple layers of representations and contextual","systems, nlp, architectures, multiple, representations, layer, task-specific"
12,"this last paradigm has led to substantial progress on many challenging NLP tasks . a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets . Removing this limitation would be desirable for several reasons .","task-specific, fine-tuning, nlp, architecture, datasets"
13,There exists a very wide range of possible useful language tasks . For many of these tasks it is difficult to collect a large supervised training dataset .,"examples, labeled"
14,Models are designed to be large to absorb information during pre-training . but are then fine-tuned on very narrow task distributions . For example observe that larger models do not necessarily generalize better out-of-distribution .,"model, fine-tuning, narrowness, of, paradigm, the"
21,"this adaptability has practical advantages - it allows humans to seamlessly mix together or switch between many tasks and skills . To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.","task, new, competence, quality, adaptability, of"
22,"meta-learning means model develops broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task .","task, model, language, specification, meta-learning"
24,In recent years the capacity of transformer language models has increased substantially . Each increase has brought improvements in text synthesis and/or downstream NLP tasks . There is evidence suggesting that log loss follows a smooth trend of improvement with scale .,"language, modeling, loss, log"
25,"the method is ""zero-shot"" in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples . To avoid this confusion, we use the term ""meta-learning"" to capture the inner-loop / outer","inference, demonstrations, training, time, inference-time"
29,"In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model . For each task, we evaluate GPT-3 under 3 conditions: ""few-shot learning"" and ""zero-shot"" learning, where no demonstrations are allowed and only an instruction in natural language is given","training, in-context, gpt-3, learning, setting, abilities"
30,Figure 1.2 illustrates the conditions we study . Model performance improves with the addition of a natural language task description . Few-shot learning also improves dramatically with model size .,"task, natural, language, learning, few-shot, description"
31,"GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 in CoQA and 85.0 F1 . GPT3 achieves 64.3% accuracy on TriviaQA . 68.0% in the one-shot set, and 71.2% in the few-shot","achieves, state-of-the-art, gpt-3"
32,"GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning . We also show that in the short-shot setting, GPT3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles .","reasoning, gpt-3, proficiency, rapid, setting, few-shot, adaption"
33,"We also find some tasks on which few-shot performance struggles, even at the scale of GPT-3 . This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.","inference, modelling, language, gpt-3, learning, few-shot"
36,"We develop systematic tools to measure data contamination and quantify its distorting effects . Data contamination has a minimal effect on GPT-3's performance on most datasets, but we do identify a few datasets where it could be inflating results .","contamination, study, common, data, crawl, systematic"
37,"the gap between zero, one-, and few-shot performance often grows with model capacity, perhaps suggesting larger models are more proficient meta-learners .","few-shot, smooth, gpt-3, scaling"
39,"In Section 2, we describe our approach and methods for training GPT-3 and evaluating it . Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 6 discusses broader impacts. Section 7 reviews related work .","contamination, gpt-3, data, train-test, overlap"
41,"Our basic pre-training approach, including model, data, and training, is similar to the process described in . Our use of in-context learning is also similar to , but in this work we systematically explore different settings for learning within the context . We start this section by explicitly defining and contrasting the different settings","in-context, gpt-3, learning"
42,Fine-Tuning has been the most common approach in recent years . It involves updating weights of a pre-trained model by training on a supervised dataset . The main advantage of fine-tuning is strong performance on many benchmarks .,"task-agnostic, performance, gpt-3, fine-tuning"
43,Few-Shot is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning . We typically set K in the range of 10 to 100 as this is how many examples can fit in the model's context window . The main,"specific, task, fs, data, few-shot"
44,"One-Shot is the same as few-shot except that only one demonstration is allowed . It most closely matches the way in which some tasks are communicated to humans . For example, when asking humans to generate a dataset on a human worker service, it is common to give one demonstration of the task .","few-shot, zero-shot"
48,"Zero-Shot is the same as one-shot except that no demonstrations are allowed . This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations . In some cases it may even be difficult for humans to understand the format of the task without prior examples .","instruction, world, natural, language, records, zero-shot"
49,"Figure 2.1 shows the four methods using the example of translating French . In this paper we focus on zero-shot, one-shot and few-shot . We aim to compare them not as competing alternatives .","few-shot, french, translation"
55,"We train 8 different sizes of model, ranging from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3 . Previous work suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size .","gpt-2, loss, validation"
56,Table 2.1 shows the sizes and architectures of our 8 models . We partition the model across GPUs along both the depth and width dimension . Previous work suggests validation loss is not strongly sensitive to these parameters within a reasonably broad range .,"computational, model, loss, validation, efficiency, gpu"
58,"Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 constituting nearly a trillion words . This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice . However, unfiltered or lightly filtered versions tend to have lower quality than more curated dataset","largest, common, models, quality, crawl"
60,"Table 2.2 shows the final mixture of datasets that we used in training . The CommonCrawl data was downloaded from 41 shards of monthly commoncrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering . Note that during training, datasets are not sampled in proportion to their","commoncrawl, training, plaintext, compressed, datasets"
68,"a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model . In Section 4 we characterize the impact of the remaining overlaps .","internet, contamination, tasks, data"
70,We measure the gradient noise scale during training and use it to guide our choice of batch size . All models were trained on V100 GPU's on part of a high-bandwidth cluster provided by Microsoft . Details of the training process and hyperparameter settings are described in Appendix B .,"training, scale, larger, noise, models, gradient"
73,for LAMBADA and Storycloze there is no supervised training set available SO we draw conditioning examples from the development set and evaluate on the test set . For Winograd there is only one dataset .,"few-shot, set, learning, training"
74,"K can be any value from 0 to the maximum amount allowed by the model's context window, which is nctx = 2048 for all models . Larger values of K are usually but not always better, SO when a separate development and test set are available . For some tasks we also use a","appendix, g, test, set"
75,"On tasks that involve choosing one correct completion from several options, we provide K examples of context plus correct completion . On a small number of datasets we gain additional benefit as measured on the development set P by normalizing by the unconditional probability of each completion.","correct, completion, answer"
78,Final results are reported on the test set when publicly available . We submit to the test server on a small number of datasets where we were able to make submission work .,"learning, test, setting, set, work"
80,In Figure 3.1 we display training curves for the 8 models described in Section 2 . Language modeling performance follows a power-law when making efficient use of training compute . One might worry that these improvements come only from modeling spurious details of our training corpus .,"training, cross-entropy, curves, loss"
82,In Section 3.1 we evaluate on traditional language modeling tasks and tasks similar to language modeling . We evaluate on Cloze tasks and sentence/paragraph completion tasks . In Section 3.3.2 we evaluate the model's performance on Winograd Schema-like tasks in Section 3.5 .,"answering, question, tasks"
91,We calculate zero-shot perplexity on the Penn Tree Bank dataset measured in . We also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training data . PTB escapes these issues due to predating the modern internet .,"ptb, zero-shot, dataset"
93,LAMBADA dataset tests the modeling of long-range dependencies in text . the model is asked to predict the last word of sentences which require reading a paragraph of context .,"lambada, long-range, dataset, dependencies"
100,LAMBADA is a demonstration of the flexibility of few-shot learning . It provides a way to address a problem that classically occurs with this dataset . a standard language model has no way of knowing this detail .,"learning, lambada, flexibility, few-shot, of"
102,GPT-3 achieves 86.4% accuracy in the few-shot setting . This is an increase of over 18% from the previous state-of-the-art .,"few-shot, method, gpt-3, setting, fill-in-blank"
108,HellaS wag dataset involves picking the best ending to a story or set of instructions . GPT-3 achieves 78.1% accuracy in one-shot setting and 79.3% accuracy in few shots .,"model, wag, hellas, dynamics, dataset"
112,In this section we measure GPT-3's ability to answer questions about broad factual knowledge . This task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text . They denote this more restrictive evaluation setting,"system, questions, gpt-3, retrieval, information"
113,"TriviaQA achieves 64.3% in the zero-shot setting, 68.0% in the one-shot set, and 71.2% in the few-shot settings . GPT-3's result already outperforms the fine-tuned T5-11B by 14.2% .","gpt-3, t5-11b"
114,GPT-3 achieves 14.4% in the zero-shot setting . This compares to 37.4% for fine-tuned T5-11B+SSM . WebQuestions shows a much larger gain compared to TriviaQA .,"gpt-3, fine-tuned, models, setting, few-shot, state-of-the-art"
119,"On Natural Questions GPT-3 achieves 14.6% in zero-shot setting, 23.0% in one-shot set, and 29.9% in few-shot . Similar to WebQS, the large gain from zero-to-need-shot may suggest a distribution shift .","natural, nqs, questions, gpt-3"
120,GPT-3's one-shot matches the open-domain fine-tuning SOTA . On the other two datasets it approaches the performance of the closed-book .,"sota, fine-tuning, open-domain, gpt-3, closed-book"
122,"GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text . Since we increase the capacity by over two orders of magnitude, we also expand the scope of the training dataset to include more representation of other languages .","capability, gpt-2, multilingual, filtering, collection, dataset"
123,Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation . GPT-3 also uses a single training objective which is not customized or designed for any task in particular .,"gpt-3, pretraining, back-translation, combine, monolingual, datasets"
135,"Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work . For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find .","backtranslation, gpt-3"
145,On Winograd we test GPT-3 on the original set of 273 Winograde schemas . This setting differs slightly from the WSC task in the SuperGLUE benchmark . The results show no clear in-context learning .,"dynamics, state-of-the-art, gpt-3"
148,"GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot and 82.8% accuracy few-shot . This compares favorably to the 79.4% accuracy prior state-of-the-art .","world, physical, reasoning, gpt-3, accuracy, works"
152,PIQA shows relatively shallow scaling with model size . GPT-3's few-shot and even zero-shot result outperform the current state-of-the-art .,"fire-tuned, gpt-3, roberta"
153,"ARC is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams . On the ""Challenge"" version of the dataset, GPT-3 achieves 51.4% accuracy in the zero-shot setting . This is approaching the performance of a fine-tuned Ro","question, arc, questions"
157,"We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings . In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each dataset .","answ, formats, gpt-3"
158,"GPT-3 performs best on CoQA a free-form conversational dataset and performs worst on QuAC . On SQuAD 2.0, GPT3 demonstrates its few-shot learning capabilities . This allows it to slightly outperform the best fine-tuned result in the original paper .","bert, baseline, comprehension, gpt-3"
160,"GPT-3's test-set performance on the SuperGLUE dataset is shown in Table 3.8 . In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set .","tasks, nlp, superglue, gpt-3, benchmark"
170,"On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings . On WSC, performance is reasonable, roughly matching that of a fine-tuned BERT-Large . We see signs of life at 75.6% in the few-","few-shot, gpt-3, setting"
171,WiC is a notable weak spot with few-shot performance at 49.4% . We tried a number of different phrasings and formulations for WiC . This hints at a phenomenon that will become clearer .,"few-shot, weak, spot, gpt-3"
172,GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score .,"superglue, gpt-3, score"
178,"SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task . On RTE only the largest version of GPT-3 performs convincingly better than random in any evaluation setting, but in a few-shot setting GPT3 performs similarly to a single-task fine-t","inference, large, natural, language, bert, rte, adversarially, mined"
180,"We test GPT-3's ability to perform simple on-the-fly computational reasoning . Second, we create several tasks that involve rearranging or unscrambling the letters in a word . Third, we test the ability to solve SAT-style analogy problems few shots .","qualitative, tasks, gpt-3"
183," 2 digit addition - The model is asked to subtract two integers sampled uniformly from [0, 100); the answer may be negative . Example: ""Q: What is 34 minus 53? A: -19""","subtraction, addition, number"
187," 4 digit addition - Same as 3 digit subtraction, except uniformly sampled from [0, 10000).  5 digit multiplication - The model is asked to multiply two integers sampled uniformly from[0, 100), e.g. ""Q: What is 24 times 42","multiplication, operation, composite"
189,"GPT-3 displays strong proficiency when the number of digits is small . Performance decreases as number increases, but GPT 3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on five digit operation . GPT3 also achieves 29.2% accuracy at 2 digit multiplication,","gpt-3, number, digits, setting, few-shot, of"
198,"We took the 3-digit arithmetic problems and searched for them in our training data . Out of 2,000 addition problems we found only 17 matches . In addition, inspection of incorrect answers reveals that the model often makes mistakes .","problems, memorizing, memorization, table, arithmetic"
202,"The model is given a word with its letters cycled, then the ""="" symbol, and is expected to generate the original word . For example, it might be given ""lyinevitab"" and should output ""inevitably""","words, cycle, cycled, letters"
208,performance is significantly weaker in the one-shot setting . model can rarely perform any of the tasks at test time . artificial nature makes them unlikely to appear in pre-training data .,"zero-shot, model, setting"
209,"""in-context learning curves"" show task performance as a function of the number of examples . We can see that larger models are able to make increasingly effective use of in-constext information .","in-context, curves, learning"
210,"LM's perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure . CL, A1 and A2 are not bijective , requiring the model to perform some search to find the correct unscrambling .","unscrambled, bpe, word, unscrambling, correct"
212,"GPT-3 achieves 65.2% in few shots, 59.1% in one-shot setting, and 53.7% in zero-shot . The average score among college applicants was 57% .","analysis, analogy, gpt-3, qua, quality, of, the"
217,"Previous work on generative language models qualitatively tested their ability to generate synthetic ""news articles"" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story . To solve this problem we employed GPT-3's few-shot learning abilities .","gpt-3, sampling, conditional, articles, news"
218,Generative language models are trained to match the distribution of content generated by humans . The ability of humans to distinguish the two is a potentially important measure of quality .,"sample, gpt-3, conditional, quality, articles, generation, news, of"
219,"In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com . For each model, we presented around 80 US-based participants with a quiz consisting of these real titles . Participants were asked to select whether the article was ""very likely","machine, subtitles, model, human, a"
220,Model outputs were formatted and selected programmatically to prevent human cherry-picking . All models used same context to condition outputs on and were pre-trained with same context size and the same article titles and subtitles were used as prompts .,"model, control, cherry-picking, output, randomness"
226,"Mean human accuracy at detecting that the intentionally bad articles were model generated was  86% where 50% is chance level performance . Human abilities to detect model generated text appear to decrease as model size increases, and human detection of GPT-3 is close to chance .","size, model, text, human, gpt-3, generated, accuracy, increases, mean"
227,"Factual inaccuracies can be an indicator that an article is model generated . Other indicators include repetition, non sequiturs, and unusual phrasings .","authentic, human, gpt-3, articles, synthetic, content"
229,"To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words . Ippolito et al. also note that human accuracy in detecting model generated text increases as humans observe","model, control, gpt-3, longer, news"
230,"Mean human accuracy at detecting the intentionally bad longer articles was  88%, while mean human accuracy was barely above chance at  52% . This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles .","gpt-3, written, longer, articles, news"
232,"a task studied in developmental linguistics is the ability to learn and utilize new words . We give GPT-3 the definition of a nonexistent word, such as ""Gigamuru"" we ask it to use it in a sentence .","linguistics, gpt-3, ability, value, developmental"
233,"6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50% .","t-test, student’s, model, control, appendix, f., two-sample"
240,"United Methodists agree to a historic split - one that is expected to end in the creation of a new denomination . The majority of delegates voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will ""discipline"" clergy who officiate at same-sex","christian, church, marriage, methodist, united, gay, the"
244,"Joaquin Phoenix appeared on the red carpet wearing a tuxedo with a paper bag over his head that read, ""I am a shape-shifter. I can 't change the world. I "" It was a promise to not change to fit into the can only change myself .","joaquin, film, phoenix"
248,"To do a ""farduddle"" means to j ump up and down really fast . A ""yalubalu"" is a type of vegetable that looks like a big pumpkin .",africa
255,"Boldface is GPT-3's completions, plain textis human prompts . In the first example both the prompt and the completion are provided by a human . This serves as conditioning for subsequent examples .","gpt-3, prompt, plain, textis, completions"
256,"Table 3.16 shows the 6 examples we generated; all definitions were human-generated . In all cases the generated sentence appears to be a correct or at least plausible use of the word . Overall, GPT-3 seems to be at least proficient at the task of using novel words .","word, screeg, gpt-3, nonexistent"
260,Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices . This issue is becoming increasingly important to attend to .,"contamination, internet-scale, benchmark, sets, test, datasets"
263,Poor English input : I eated the purple berries . Good English output : Thank you for choosing me as your designer . I'd appreciate it . Poor English output: The mentioned changes have done . or I did the alteration that you requested .,"it, purple, berries, appreciate"
277,"GPT-3's completions, plain text is human prompts . This serves as conditioning for subsequent examples . The distinction between ""poor"" and ""good"" English is complex, contextual, and contested .","text, correcting, english, gpt-3, plain"
282,"GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated . We expect that contamination is likely to be frequent, but that its effects may not be as large as feared .","contamination, validation, gpt-3, memorization, set"
283,"a bug resulted in only partial removal of all detected overlaps . Due to the cost of training, it wasn't feasible to retrain the model . To address this, we investigate in detail how the remaining detected overlap impacts results.","training, contamination, data"
284,"For each benchmark, we produce a 'clean' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set .","appendix, clean, c, subset"
285,"We then evaluate GPT-3 on these clean benchmarks and compare to the original score . If the score on the clean subset is lower, this suggests contamination may be inflating the results .","contamination, performance, benchmarks, gpt-3, difference, clean"
292,"We found 25% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU . After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training","analysis, subset, workplace, clean, overlap"
294,"We found the 4 Wikipedia language modeling benchmarks measured in GPT-2 plus the Children's Book Test dataset to be almost entirely contained in our training data . We do not report results on these datasets, even though we intended to when starting this work .","bank, gpl, tree, common"
295,"We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed . One notable exception was LAMBADA, which appeared to have substantial genuine contamination . The clean subset scored within 0.5% of the full dataset .","contamination, scoring, lambada, actual, subset, clean, cle"
296,the clean subset is not drawn from the same distribution as the original dataset . The sheer number of shifts close to zero suggests this is unlikely . We also observed no noticeable difference in the shifts for small models .,"analysis, contamination, memorization, subset, clean"
297,"We have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results . Much work remains to be done to address this important and subtle issue for the field in general .","contamination, appendix, benchmarks, field, c, data"
300,"We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3's limitations and strengths at text synthesis . In the domain of discrete language tasks, we have noticed informally that GPT3 seems to have special difficulty with ""common sense physics"" Specifically, G","nlp, gpt-3, quality, of, the"
301,"GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above . We focused on exploring in-context learning behavior in autoregressive language models . As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising . Thus our design","model, fine-tuning, passage, bidirectional, long"
304,"Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important . Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem . Finally, useful language systems might be better thought of as taking goal-directed actions rather","customizing, objective, world, function, pretraining, of, the"
305,GPT-3 takes a step towards test-time sample efficiency . It still sees much more text during pre-training than a human sees in their lifetime .,"model, language, efficiency, test-time, gpt-3, pre-training, sample"
306,few-shot learning in GPT-3 may vary from task to task . Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo . Even organizing diverse demonstrations during pre-training is an advance for language models .,"task, gpt-3, organization, learning, few-shot"
307,"a limitation associated with models at the scale of GPT-3 is that they are both expensive and inconvenient to perform inference on . Large models contain a very wide range of skills, most of which are not needed for a specific task . Distillation is well-explored in general but has not been","tasks, large, distillation, gpt-3, models, specific"
308,"GPT-3 shares some limitations common to most deep learning systems . its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs . This last issue - biases in the data that may lead the model to generate stereotyped or prejudiced content - is of special","system, gpt-3, biases, learning"
310,Language models have a wide range of beneficial applications for society . GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text .,"text, language, gpt-3, autocompletion, and, code, models, writing, synthetic"
311,"Here we focus on the potential harms of improved language models . We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like this .","language, gpt-3, models, improved, representation"
314,"Malicious uses of language models can be difficult to anticipate because they involve repurposing language models in a very different environment or for a different purpose than researchers intended . To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts","likelihood, applications, misuse, potential"
316,"Language models that produce high quality text generation could lower existing barriers to carrying out these activities . Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting .","models, language, quality, text"
320,"low and mid-skill actors think about language models . We have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed .","language, gpt-2, models"
321,Since the release of GPT-2 there has been no discernible difference in operations . The assessment was that language models may not be worth investing significant resources in .,"text, apt, language, models, for, generating"
323,"phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials . Using language models to augment existing TTPs would likely result in an even lower cost of deployment .","ttps, procedures, phishing, deployment, techniques"
324,"Having stable infrastructure has a large impact on the adoption of TTPs . The outputs of language models are stochastic, but developers can constrain them . If a social media disinformation bot produces outputs that are reliable 99% of the time, but incoherent outputs 1%, this could reduce","ease, truncation, use, top-k, ttp, of"
325,"AI researchers will eventually develop language models that are consistent and steerable that they will be of greater interest to malicious actors . We expect this will introduce challenges for the broader research community and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.","model, landscape, ai, actors, research"
328,"Biases present in training data may lead models to generate stereotyped content . This is concerning, since model bias could harm people in relevant groups in different ways .","stereotypes, bias, model"
330,Our analysis indicates that internet-trained models have internet-scale biases . We probe for bias in the 175 billion parameter model and also in smaller models .,"bias, internet-scale, internet-training, internet-trained, models"
332,"83% of the 388 occupations we tested were more likely to be followed by a male gender identifier by GPT-3 . We measured this by feeding the model a context such as ""The detective was a "" .","bias, gpt-3, identif, gender"
333,"We also tested how these probabilities changed when we shifted the context to be the ""The competent occupation was a"" for each occupation in the dataset . We found that when prompted with ""the competent 'occupationwas a, the majority of occupations had an even higher probability of being followed by ","male, {occupation], identifier, competent, ma"
334,"One method measured models' ability to correctly assign a pronoun as the occupation or the participant . For example, we fed the model a context such as ""The advisor met with the advisee because she wanted to get advice about job applications .","advisor, resolution, advisee, pronoun"
335,GPT-3 175B had the highest accuracy of all the models on this task . It was also the only model where the accuracy for Occupant sentences for females was higher than for males .,"sentences, words, occupation, gpt-3, participant, 17b"
341,"females were more often described using appearance oriented words such as ""beautiful"" compared to men who were more frequently described using adjectives that span a greater spectrum .",adverbs
342,"Table 6.1 shows the top 10 most favored descriptive words for the model . The raw number of times each word co-occurred with a pronoun indicator is ""Most Favored""","words, qualifying, co-occurrences"
344,"To investigate racial bias in GPT-3, we seeded the model with prompts such as ""The race man was very"" We measured word co-occurrences in the generated samples . We measured sentiment using Senti WordNet for the words which co-occurred disproportionately with each","bias, bes10, senti, gpt-3, wordnet, racial"
345,resulting sentiment can reflect socio-historical factors . text relating to a discussion of slavery will often have a negative sentiment . this may lead to demographic being associated with negative sentiment under this testing methodology.,"slavery, sentiment, negative"
346,"'Asian' had a consistently high sentiment - it ranked 1st in 3 out of 7 models . On the other hand, 'Black' had an consistently low sentiment . These differences narrowed marginally on the larger model sizes . This analysis gives a sense of the biases of","analysis, highlights"
354,We generated 800 model outputs of length 50 with a temperature of 1 and a top p of 0.9 for every prompt . We then allowed the model to naturally carry out completions and created a corpus of such completions .,"atheism, buddhism"
357,"models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world . For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions .",islam
360,"We have presented this preliminary analysis to share some of the biases we found in order to motivate further research . We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point .","reporting, model, models, biases"
361,"We offer only a few brief comments on future directions specific to large language models . In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together normative, technical and empirical challenges of bias mitigation .","language, mitigation, systems, bias"
364,Models like GPT-3 consume significant resources during training . generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr . techniques like model distillation can further bring down the cost of such models .,"large-scale, models, pre-training, gpt-3"
366,"An early work scaled LSTM based language models to over a billion parameters . One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion . This approach rely on the conditional computation framework .","lstm, performance, task, count, computation, increasing, parameter"
367,Several efforts have also systematically studied the effect of scale on language models . find a smooth power-law trend in loss as autoregressive language models are scaled up .,"model, power-law, language, smooth, trend"
371,fine-tuned language models have neared human performance on many standard benchmark tasks . in this work we test our models on many of these datasets .,"language, tasks, models, benchmark"
372,"Many previous efforts have focused specifically on question-answering . Recent efforts include , which fine-tuned an 11 billion parameter language model . Our work differs in focusing on in-context learning .","in-context, question-answering, learning"
373,Metalearning in language models has been used in . Language model metalearning has an inner loop-outer-loop structure . Our approach of stuffing the model's context with previous examples is most structurally similar to RL2 .,"estimation, density, models, ml, metalearning"
375,giving multi-task models instructions in natural language was first formalized in a supervised setting with and utilized for some tasks in . the concept of presenting tasks in natural languages was also explored in the text-to-text transformer .,"multi-task, models"
376,"Multi-task learning fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one . Multitask learning has shown some promising initial results . This is limited by the need to manually curate datasets and set up training curricula .","capability, multi-task, transfer-learning, learning"
377,"Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality , prefixLM and encoder-decoder architectures . Many of these techniques provide significant gains on downstream tasks . In this work we continue to focus on pure autoregressive language models","implementation, model, large, fine-tuning, gpt-3, setting"
381,"state-of-the-art fine-tuned systems generate high-quality samples . We also discussed the social impacts of this class of model . These results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.","state-of-the-art, fine-tuning, model"
383,"authors would like to thank Ryan Lowe for giving detailed feedback on draft paper . Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI's infrastructure .","visual, model, openai, design"
415,Classifier is trained using logistic regression classifier with features from Spark's standard tokenizer and HashingTF 10 . We then used this classifier to re-sample Common Crawl by prioritizing documents predicted to be higher quality .,"regression, logistic, raw, common, classifier, crawl"
417,a was chosen to match the distribution of scores from our classifier on WebText . We found this re-weighting increased quality as measured by loss on a range of out-of-distribution text samples .,"model, quality, classifier, webtext, capacity"
420,"To train all versions of GPT-3, we use Adam with B1 = 0.9, B2 = 0.95, and E = 10-8 . There is a linear LR warmup over the first 375 million tokens . All models use weight decay of 0.1 to provide a small amount of regularization ","training, learning, gpt-3, rate"
421,We train on sequences of the full nctx = 2048 token context window . Sequences with multiple documents are delimited with a special end of text token . This allows for efficient training without need for any special sequence-specific masking .,"computational, training, efficiency, documents, multiple"
424,"Initial training set filtering We tried to remove text occurring in benchmarks from training data by searching for 13-gram overlaps . For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation . Documents split into more than 10 pieces were considered contaminated .","workplace, filtering, whitespace, set"
427,"Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives . An example of a false positive might be a test set based on Wikipedia . We ignored 13- grams that matched more than 10 training documents .","repository, release, documents, collision, gpt-3, entirely, 11, long, single"
428,"We used a variable number of words N to check for overlap for each dataset . For performance reasons, we set a maximum value of 13 for all tasks . Values for N and the amount of data marked as dirty are shown in Table C.1.","spark, apache, benchmark, overlap, methodology"
430,"Test and validation splits had similar contamination levels . Due to a bug revealed by this analysis, filtering described above failed on long documents such as books .","validation, children’s, book, test, filtering"
431,"if the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen .","examples, clean, score, clean-only"
432,DROP is a reading comprehension task in which 94% of the examples are dirty . The information required to answer the question is in a passage provided to the model .,"wordscrambling, drop, passage, source, dataset"
437,"""Relative Difference Clean VS All"" shows the percent change in performance between only the clean examples VS all the examples in the benchmark . ""Clean percentage"" is the percent of examples that are clean VS total .","clean, percentage, cle"
443,"T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass . Then we add a multiplier of dparams dacts similar amount of compute as the 3x .","model, training, encoder-decoder, t5, tokens"
446,97 participants excluded for failing an internet check question . Mean participant age was  38 years old . All participants were US-based but there were no other demographic restrictions .,"internet, check, participation, question"
447,We arbitrarily selected 25 news articles that appeared in newser.com in early 2020 . Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically .,"model, each, procedure, early, output, 2020"
454,Each quiz consisted of 25 articles: half were human written and half were model generated . The order of quiz question was shuffled for each participant . Participants were instructed not to look up the articles or their content during the quiz .,"quiz, question"
455,"Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups . This was implemented in Python using the scipy  stats  ttest_ind function .","control, two-sample, python, t-test"
461,We recruited 160 unique US-based participants to take part in 2 experiments through Positly . We then used the article titles and Reuters locations to generate completions from GPT-3 175B .,"model, world, control, word, gpt-3, 175b, news"
463,"In Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens . Completions were truncated when the model began to write an author heading .","heading, sampling, gpt-3"
465,The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY SOME TREES John Ashbery Shadows on the Way Wallace Stevens Generated Poem 1 Generated poem 3 I must have All is changed . The mind must dwell on those Each step,wood
473,The Gold Coast is located one mile west of Las Vegas Strip on West Flamingo Road . It is located across the street from Palms Casino Resort and Rio All Suite Hotel and Casino .,"neither, nei, gold, coast"
497,"There are three levels within MultiRC: the passage, the questions, and the answers . During evaluation, accuracy is determined at the per-question level . For this reason, we use K to refer to the number of questions shown within the context.","multirc, questions, level"
580,"The second PASCAL recognising textual entailment challenge. 2006. Yonatan Bisk, Ari Holtzman, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor . ArXiv preprint ar","question, natural, language, answer, y"
582,"In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177-190. Springer, 2006. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lu","inference, natural, language, transformers, back-translation"
584,"Deep learning scaling is predictable, empirically . arXiv preprint arxiv:1801.06146, 2018. Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International Conference on Artificial Neural Networks, pages 87-","answering, question, modeling, natural, language, bert"
586,"Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao . Multilingual denoising pre-training for neural machine translation .","understanding, natural, language, bert, translation"
588,"Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher . Learned in translation: Contextualized word vectors . A corpus and evaluation framework for deeper understanding of commonsense stories.","natural, language, inference, representation"
590,"In Proceedings of the 2014 conference on empirical methods in natural language processing, 2014. QIANXIN. Sa-net on albert , April 2020. Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with ","bias, model, gender, function, emnlp, language, generation, word-level"
592,"A simple method for commonsense reasoning. In Advances in neural information processing systems, pages 3630-3638, 2016. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Attention is all you need","systems, analysis, analogy, communication, processing, information, technical"
