element_idx,keywords,summarized_text
3,"neural, network, convolutional, regularization, method","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes . On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state"
5,"larger, transformations, mnist, label-preserving, datasets","Current approaches to object recognition make essential use of machine learning methods . To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting . For example, the currentbest error rate on the MNIST digit-recognition task approaches human performance ."
6,"imagenet, networks, recognition, task, image",Convolutional neural networks constitute one such class of models . Their capacity can be controlled by varying their depth and breadth . So our model should also have lots of prior knowledge to compensate for all the data we don't have .
8,"imagenet, sc, cnns, scale, large","current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs . Recent datasets such as ImageNet contain enough labeled examples to train such models ."
9,"networks, overfitting, neural, ilsvrc-2012, ilsvrc, convolutional",we trained one of the largest convolutional neural networks to date on the subsets of ImageNet . Our network contains a number of new and unusual features which improve its performance and reduce its training time . The size of our network made overfitting a significant problem .
12,"imagenet, recogni, challenge, large-scale, recognition, visual","ImageNet is a dataset of over 15 million high-resolution images belonging to roughly 22,000 categories . The images were collected from the web and labeled by human labelers using Amazon's Mechanical Turk crowd-sourcing tool ."
13,"test, label, ilsvrc, dataset, set, image","ILSVRC-2010 is the only version for which the test set labels are available . On ImageNet, itis customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images ."
14,"variable-resolution, rectangular, imagenet, image","ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality . Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256 x 256 patch from the resulting image "
20,"rectified, saturation, linear, network, units","Normal way to model a neuron's output f as a function of its input x is with f = tanh . In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturated nonlinearyity f= max"
21,"cnn, model, large, dataset, models","We are not the first to consider alternatives to traditional neuron models in CNNs . For example, Jarrett et al. claim that the nonlinearity f = |tanh| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset "
24,"cifar-10, neurons, saturating, neural, network, convolutional",A four-layer convolutional neural network with ReLUs reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons . No regularization of any kind was employed .
25,"computation, cross-gpu, network, gpu",1.2 million training examples are enough to train networks which are too big to fit on one GPU . Current GPUs are particularly well-suited to cross-GPU parallelization . This means that the GPUs communicate only in certain layers .
26,"cnn, columnar, top-1, architecture, top-5, error","The resultant architecture is somewhat similar to that of the ""columnar"" CNN employed by Cire an et al. This reduces our top-1 and top-5 error rates by 1.7% and 1.2% . The two-GPU net takes slightly less time to train than the one-gPU net2 ."
27,"net, layer, two-gpu, final, convolutional",2The one-GPU net actually has the same number of kernels . This is because most of the net's parameters are in the first fully-connected layer . We did not halve the size of the final convolutional layer to make the two nets have approximately the same numbers .
30,"normalization, neuron, kernel, relu","ReLUs have the desirable property that they do not require input normalization to prevent them from saturating . however, we still find that the following local normalization scheme aids generalization ."
31,"layer, kernel, number, maps",The order of the kernel maps is of course arbitrary and determined before training begins . This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons . We applied this normalization after applying the ReLU nonlinearity in certain layers .
32,"normalization, critical, control, cifar-10, cnn, top-1, rate, error",Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2% respectively . a four-layer CNN achieved a 13% test error rate without normalization .
34,"pooling, overlapping, cnn, scheme","Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap . If we set s = z, we obtain traditional local pooling as commonly employed in CNNs . This scheme reduces top-1 and top-5 error rates by 0.4% and 0.3% ."
36,"softmax, cnn, network",the net contains eight layers with weights; the first five are convolutional and the remaining three are fully connected . The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels . Our network maximizes the multinomial logistic regression objective 
37,"fully-connected, layer, kernel, response-normalization, maps","kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU . Response-normalization layers follow the first and second convolutionals . Max-pooling layers, of the kind described in Section 3.4, follow both response-normalizing"
43,"map, kernel",third convolutional layer has 384 kernels of size 3 x 3x 256 connected to outputs of second . fifth convolutionally layer has 256 kernels .
47,"overfitting, augmentation, data, dataset, image","the easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations . In our implementation, the transformed images are generated in Python code while the GPU is training on the previous batch of images ."
48,"softmax, reflections, layer, augmentation, data, horizontal","The first form of data augmentation consists of generating image translations and horizontal reflections . We extract random 224 x 224 patches from the 256x256 images . This increases the size of our training set by a factor of 2048 . Without this scheme, our network suffers from substantial overfit"
53,"natural, rgb, image","where Pi and  are ith eigenvector,  and ai are the aforementioned random variable . This scheme approximately captures an important property of natural images . The object identity is invariant to changes in the intensity and color of the illumination ."
55,"dropout, back-propagation","The recently-introduced technique, called ""dropout"" consists of setting to zero the output of each hidden neuron with probability 0.5 . The neurons which are ""dropped out"" in this way do not contribute to the forward pass . This technique reduces complex co-adaptations of neurons ."
62,"gaussian, neuron, biases, weights, distribution",We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01 . This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs .
67,"ilsvrc-2010, fisher, (fvs), vectors",Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features .
68,"test, model, ilsvrc-2012, rate, error",Model Top-1 Top-5 We also entered our model in the ILSVRC-2012 competition and report our results in Table 2 . In italics we cannot report test error rates for all the models that CNN 37.5% 17.0% we tried . Averaging the predictions of five similar CNNs gives an error rate of 16.
69,"imagenet, testing, test, set, image","On this dataset we follow the convention of using half of the images for training and half for testing . Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably ."
74,"convolutional, col-ored, kernels, specialization, blobs","Figure 3 shows the convolutional kernels learned by the network's two data-connected layers . The network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs . Notice the specialization exhibited by the two GPUs, a result of"
78,"cat, leopard, top-left","In Figure 4 we qualitatively assess what network has learned by computing its top-5 predictions on eight test images . Most of the top-5 labels appear reasonable . For example, only other types of cat are considered plausible labels for the leopard ."
79,"feature, knowledge, activation, separation, euclidean, network, visual",Figure 4 shows five images from the test set and the six images from training set that are most similar to each of them according to this measure . We present the results for many more test images in the supplementary material .
80,"euclidean, similarity, retrieval, distance","Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient . but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes . This should produce a much better image retrieval method than applying autoencode"
82,"record-breaking, results, neural, network, convolutional","a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning . removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network . So the depth really is important for achieving our results"
83,"system, network, visual","To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help . Thus far, our results have improved as we have made our network larger and trained it longer but still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system"
86,"ieee, symposium, and, computer, scale, vision, cvpr, recognition, large, classification, visual, on, pattern, international, image","ACM SIGKDD Explorations Newsletter, 9:75-79, 2007. A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. L. Breiman. Random forests. Machine learning, 45:"
