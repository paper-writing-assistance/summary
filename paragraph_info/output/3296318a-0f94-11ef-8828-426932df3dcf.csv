element_idx,keywords,summarized_text
4,"deep, fpgn, networks, network, convolutional","Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks . This paper proposes a Fast Region-based Convolutional Network method for object detection . It trains the very deep VGG16 network 9x faster, is 213x faster at test-time, and achieves"
14,"detection, convnet, object, proposals","Training is a multi-stage pipeline . R-CNN finetunes a ConvNet on object proposals using log loss . In the third training stage, bounding-box regressors are learned . Detection with VGG16 takes 47s ."
15,"computation, convolutional, map, spatial, feature, sharing, networks, pooling, pyramid",Spatial pyramid pooling networks were proposed to speed up R-CNN by sharing computation . The SPPnet method computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map . Features are extracted for a proposal by maxpool
18,"r-cnn, spatial, pooling, network, fine-tuning, pyramid","SPPnet is a multi-stage pipeline that involves extracting features, fine-tuning a network with log loss, training SVMs, and finally fitting bounding-box regressors . Features are also written to disk ."
24,"r-cnn, softmax, probabilit, fast, network, probability, architecture",A Fast R-CNN network takes as input an entire image and a set of object proposals . The network first processes the whole image with several convolutional and max pooling layers to produce a conv feature map . Each feature vector is fed into a sequence of fully connected layers .
26,"pooling, map, roi, feature",The RoI pooling layer uses max pooling to convert features inside any valid region of interest into a small feature map with a fixed spatial extent of H x W . Each RoI is defined by a four-tuple that specifies its top-left corner and its height and width .
29,"spatial, max, pooling, size, pyramid, approximate",RoI max pooling works by dividing the h x w RoI window into an H x W grid of sub-windows . Pooling is applied independently to each feature map channel . The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets
39,fem,"In Fast RCNN training, stochastic gradient descent minibatches are sampled hierarchically, first by sampling N images . Critically, RoIs from the same image share computation and memory in the forward and backward passes ."
41,"r-cnn, softmax, bounding-box, regressor, classifier, fast, back-propagation",Fast R-CNN uses a streamlined training process with one fine-tuning stage that jointly optimizes a softmax classifier and bounding-box regressors . The components of this procedure are described below.
42,"f-cnn, multi-task, loss","A Fast R-CNN network has two sibling output layers . The first outputs a discrete probability distribution , p = , over K + 1 categories ."
45,"indicator, class, bounding-box, bracket, iverson, regression, targets, background",The Iverson bracket indicator function evaluates to 1 when u  1 and 0 otherwise . The catch-all background class is labeled u = 0. For background RoIs there is no notion of a ground-truth .
50,"r-cnn, localizer, stage-wise, bounding-box, training","We note that uses a related loss to train a classagnostic object proposal network . Different from our approach, advocates for a two-network system that separates localization and classification ."
51,"probability, mini-, sampling, mini-batch","During fine-tuning, each SGD mini-batch is constructed from N = 2 images, chosen uniformly at random . As in, we take 25% of the RoIs from object proposals that have intersection over union overlap with a groundtruth bounding box of at least 0.5 . The remaining Ro"
57,"softmax, learning, sgd, bounding-box, regression, classification, rate",The fully connected layers used for softmax classification and bounding-box regression are initialized from zero-mean Gaussian distributions with standard deviations 0.01 and 0.001 respectively . All layers use a per-layer learning rate of 1 for weights and 2 for biases . When training on VOC
60,"pyramid, multi-scale, training, image","During multi-scale training, we randomly sample a pyramid scale each time an image is sampled . At test-time, the image pyramid is used to approximate scale-invariance ."
64,"bounding-box, probability, prediction",We assign a detection confidence to r for each object class k using the estimated probability Pr  . We then pk perform non-maximum suppression independently for each class .
68,"compression, svd, weight, network, matrix","Truncated SVD reduces the parameter count from uv to t . To compress a network, the single fully connected layer corresponding to W is replaced by two fully connected layers, without a non-linearity between them ."
81,"caffenet, vgg16, model","CaffeNet is model S, for ""small"" The second network is VGG_CNN _M_1024 from . This model has the same depth as S, but is wider ."
83,"babylearning, nus, n, network","We compare Fast R-CNN against the top methods on the comp4 track . For the NUS _NIN_c2000 and BabyLearning methods, there are no associated publications at this time ."
86,"segdeepm, map",SegDeepM is trained on VOC12 trainval plus segmentation annotations . It is designed to boost R-CNN accuracy by using a Markov random field to reason over R-cNN detections and segmentations from the O2P semantic-segmentation method .
88,"r-cnn, bound, fac, bounding-box, regression, sppnet","SPPnet uses five scales during training and testing . Fast R-CNN achieves a mAP of 66.0% . All other experiments use ""difficult"" examples."
90,"r-cnn, sppnet, fac, testing, fast, training, rate",Fast RCNN trains VGG16 2.7x faster and tests 7x faster without truncated SVD or 10x faster with it . Fast R-CNN also eliminates hundreds of gigabytes of disk storage .
93,"svd, fine-tuning, map, truncated",Truncated SVD can reduce detection time by more than 30% . Fig. 2 illustrates how using top 1024 singular values from the 25088 x 4096 matrix in VGG16's fc6 layer reduces runtime with little loss in mAP . Further speed-ups are
97,"deep, layer, networks, connected, sppnet, very","We hypothesized that fine-tuning only the fully connected layers would not hold for very deep networks . We use Fast R-CNN to fine tune, but freeze the thirteen conv layers SO . This ablation emulates single-scale SPPnet training and decreases mAP from 66.9% to"
100,"map, memory, conv, layer, networks, smaller",VGG16 found it only necessary to update layers from conv3_1 and up . The difference in mAP was only +0.3 points . All experiments with models S and M fine-tune layers conv2 and up.
108,"multi-task, baseline, networks, loss","These baselines are printed for models S, M, and L in Table 6. Note that these models do not have bounding-box regressors . Next, we take networks that were trained with the multi-task loss ."
110,"bounding-box, layer, multi-task, regression, training, lloc","mAP improves over column one, but stage-wise training underperforms multi-task training . we train baseline models , tack on bounding-box regression layer, and train them with Lloc ."
113,"vgg16, smaller, image",All single-scale experiments use s = 600 pixels; s may be less than 600 for some images as we cap the longest image side at 1000 pixels . These values were selected so that VGG16 fits in GPU memory during fine-tuning .
118,"multiscale, invariance, convnet, convnets",deep ConvNets are adept at directly learning scale invariance . The multi-scale approach offers only a small increase in mAP at a large cost .
121,"map, dpm, training",Zhu et al. found that DPM mAP saturates after only a few hundred to thousand training examples . Here we augment the VOC07 trainval set with VOC12 trainval .
130,"detection, object, proposals, sparse, dpm",Classifying sparse proposals is a type of cascade in which the proposal mechanism first rejects a vast number of candidates leaving the classifier with a small set to evaluate . This cascade improves detection accuracy when applied to DPM detections .
135,"r-cnn, object, ar, proposal, methods, quality",the state-of-the-art for measuring object proposal quality is Average Recall . AR does not correlate well with mAP as the number of proposals per image is varied .
142,"r-cnn, sppnet, proposals, fast, sparse","This paper proposes Fast R-CNN, a clean and fast update to SPPnet . We present detailed experiments that we hope provide new insights . Of particular note, sparse object proposals appear to improve detector quality ."
145,"deep, networks, recognition, visual, backpropagation, convolutional","In ECCV, 2012. 5 R. Caruana. Multitask learning . Machine learning, 28, 1997. 6 K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman . ImageNet: A large-scale hierarchical image database . In CV"
146,"detection, deep, object, decomposition, signular, neural, value, network","In ICLR, 2014. 1, 3 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition . In CVPR, 2001. 8 J. Xue, J. Li, and Y. Gong. Rapid object detection using a boosted cascade"
