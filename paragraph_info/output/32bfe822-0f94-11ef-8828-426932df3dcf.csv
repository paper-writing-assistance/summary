element_idx,keywords,summarized_text
6,"deep, learning, networks, generative, convolutional, adversarial, unsupervised","In recent years, supervised learning with convolutional networks has seen huge adoption in computer vision applications . In this work we hope to help bridge the gap between the success of CNNs ."
8,"gans, multi-layer, classification, image",GANs provide an attractive alternative to maximum likelihood techniques . One can argue that their learning process and lack of a heuristic cost function are attractive .
10,"deep, topology, architecture, gans, convolutional","We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings . We use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms."
17,"learning, computer, patches, representation, vision, image","Unsupervised representation learning is a fairly well studied problem in general computer vision research, as well as in the context of images . One can do hierarchical clustering of image patches to learn powerful image representations . Another popular method is to train auto-encoders , separating the what and where components of the"
21,"images, approach, generative, natural, mnist, network, adversarial","A variational sampling approach to generating images has had some success . However, the samples often suffer from being blurry . A laplacian pyramid extension to this approach showed higher quality images ."
23,"cnns, networks, neural, black-box, methods","Zeiler et. al. showed that by using deconvolutions and filtering the maximum activations, one can find the approximate purpose of each convolution filter in the network . Similarly, using a gradient descent on the inputs lets us inspect the ideal image that activates certain subsets of filters "
25,"archi, cnn, architecture",attempting to scale up GANs using CNNs to model images has been unsuccessful . This motivated the authors of LAPGAN to develop an alternative approach to iteratively upscale low resolution generated images which can bemodeled more reliably .
31,"global, average, pooling, features, gan, convolutional",Global average pooling has been used in state of the art image classification models . The strongest example of this is the trend towards eliminating fully connected layers on top of convolutional features .
32,"deep, generator, model, instability, batchnorm",Batch Normalization stabilizes learning by normalizing input to each unit to have zero mean and unit variance . This helps deal with training problems that arise due to poor initialization . Directly applying batchnorm to all layers resulted in sample oscillation and model instability .
33,"paper, modeling, resolution, gan",The ReLU activation is used in the generator with the exception of the output layer which uses the Tanh function . We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space .
38,"relution, relu, activation, leaky, tanh, function",All models were trained with mini-batch stochastic gradient descent . All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02 .
45,"bedrooms, overfitting/memorization, generative, lsun, training, dataset, models, image",We train a model on the LSUN bedrooms dataset containing a little over 3 million training examples . Recent analysis has shown that there is a direct link between how fast models learn and their generalization performance .
47,"generator, memorizing, semantic-hashing, input, de-duplication, examples, image",We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples . The resulting code layer activations are binarized via thresholding the ReLU activation which provides a convenient form of
49,"queries, people, opencv, names, image","The people names were acquired from dbpedia, with a criterion that they were born in the modern era . We run an OpenCV face detector on these images, keeping the detections that are sufficiently high resolution ."
63,"discriminator, cifar-10, representations, discriminator's, normal","An unsupervised multi-layered extension of the base algorithm reaches 82.0% accuracy . This technique trains discriminative CNNs in an unsupervised fashion to differentiate between specifically chosen, aggressively augmented, exemplar samples from the source dataset . Further improvements could be made by finetuning the discriminator's representations"
67,"dcgan, architecture, validation, error, set","On the StreetView House Numbers dataset, we use the features of the discriminator of a DCGAN for supervised purposes when labeled data is scarce . 1000 uniformly class distributed training examples are randomly selected and used to train a regularized linear L2-SVM classifier on top of the same feature extraction pipeline"
76,"of, space, landscape, the, latent",The first experiment we did was to understand the landscape of the latent space . Walking on the manifold that is learnt can usually tell us about signs of memorization and about the way in which the space is hierarchically collapsed .
78,"dcgan, dataset, large, image","Previous work has demonstrated that supervised training of CNNs on large image datasets results in very powerful learned features . Additionally, supervised CNNs trained on scene classification learn object detectors ."
80,"generator, representations, discriminator","the quality of samples suggest that the generator learns specific object representations for major scene components such as beds, windows, lamps, doors, and miscellaneous furniture ."
81,"bounding, drawn, box, window","On 150 samples, 52 window bounding boxes were drawn manually . On the second highest convolution layer features, logistic regression was fit to predict whether a feature activation was on a window ."
88,"arithmetic, z, representation, simple",In the context of evaluating learned representations of words demonstrated that simple arithmetic operations revealed rich linear structure in representation space . One canonical example demonstrated that the vector - vector + vector resulted in a vector whose nearest neighbor was the vector for Queen . Experiments working on only single samples
93,"bedrooms, lsun, visualization, features, backpropagation, convolutional, dataset",Notice a significant minority of features respond to beds - the central object in the LSUN bedrooms dataset . Comparing to previous responses there is little to no discrimination and random structure .
95,"model, visual, appearance","the same samples generated with dropping out ""window"" filters . Some windows are removed, others are transformed into objects with similar visual appearance such as doors and mirrors . Extended experiments could be done to remove other objects from the image ."
110,"team, indico, research, nvidia","We'd like to thank Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma for their support, resources, and conversations . Finally, Nvidia donated a Titan-X GPU used in this work ."
157,"metrics, dcgan, distributions, standard, classification, conditional, batchnorm","We trained a DCGAN on MNIST as well as a permutation invariant GAN baseline and evaluated the models using a nearest neighbor classifier . We found that removing the scale and bias parameters from batchnorm produced better results for both models . At one million samples per class, the"
