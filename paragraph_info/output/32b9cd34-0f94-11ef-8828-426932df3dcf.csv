element_idx,keywords,summarized_text
3,"likelihood, generative, value, ddpm, gan, models","Denoising diffusion probabilistic models are a class of generative models which have recently been shown to produce excellent samples . We show that with a few simple modifications, DDPMs can also achieve competitive loglikelihoods while maintaining high sample quality . Additionally, learning variances of the reverse diffusion process allows sampling"
6,"probabilistic, generative, score, ddpm, based, models, diffusion","Sohl-Dickstein et al. introduced diffusion probabilistic models . generative models learn to reverse a gradual, multi-step noising process . It has been shown that this class can produce high-quality images and audio ."
11,"future, ddim, quality, ddpm","DDPM requires hundreds of forward passes to produce good samples . We can achieve good samples with as few as 50 forward passes, thus speeding up sampling for practical applications ."
12,"likelihood, likelihood-based, gan, gans, models",We wanted to compare the distribution coverage of these models with GANs . We used the improved precision and recall metrics . This suggests that diffusion models achieve much higher recall for similar FID .
24,"lo, distribution, gaussian","To evaluate Lo for images, we assume that each color component is divided into 256 bins . Note that while LT does not depend on 0, it will be close to zero ."
30,"al., equation, 9, lt-1, et, al, lvlb., ho","Equation 9 provides an efficient way to sample from an arbitrary step of the forward noising process . We can thus randomly sample t and use the expectation Et,xo,e to estimate Lvlb ."
39,"datasets, ddpm, log-likelihood, image","Ho et al. found that DDPMs can generate highfidelity samples according to FID and Inception Score . However, they were unable to achieve competitive log-likelihoods with these models . This is a widely used metric in generative modeling ."
40,"resolution, cifar-10, model, hyperparameters, fixed, architecture",ImageNet 64 x 64 and CIFAR-10 have been studied extensively in the context of generative modeling . We train fixed model architectures with fixed hyperparameters on the datasets .
41,"lsimple, setup, log-likelihood","The setup from Ho et al. achieves a log-likelihood of 3.99 on ImageNet 64 x 64 after 200K training iterations . We found that we could get a boost by increasing T from 1000 to 4000 . With this change, we use T = 4000, but"
48,"opposite, bt, steps, extremes, diffusion","Figure 1 shows that Bt and Bt are almost equal except near t = 0, i.e. where the model is dealing with imperceptible details . This suggests that in the limit of infinite diffusion steps, the choice of t might not matter at all for sample quality ."
49,"e, (xt, log-likelihood","Figure 2 shows that the first few steps of the diffusion process contribute the most to the variational lower bound . Thus, it seems likely that we could improve log-likelihood by using a better choice of E ."
69,"offset, bt, cos2, small, s","VB0 was slightly smaller than the pixel bin size 1/127.5, which gives s = 0.08 . We chose to use cos2 in particular because it is a common mathematical function with the shape we were looking for ."
87,"lvlb, log-likelihood, log-","importance sampled objective can be seen in Figure 6 as the Lvlb curve . The figure also shows that the Lhybrid objective is considerably less noisy than the original, uniformly sampled ."
89,"imagenet, lhybrid",Table 1 summarizes the results of our ablations on ImageNet 64 x 64 . Table 2 shows them for CIFAR-10 . Lvlb and Lhybrid were trained with learned sigmas .
96,"steps, image, ddpms, lhybrid, model, diffusion","In this section, we explore how performance scales if we reduce the steps used during sampling . We find that our pre-trained Lhybrid models can produce high-quality samples ."
97,"steps, noise, schedule, t, training, diffusion","For a model trained with T diffusion steps, we would typically sample using the same sequence of t values as used during training . Given the training noise schedule t, for a given sequence S we can obtain the sampling noise schedule St which can be then used to obtain corresponding sampling variances ."
102,"checkpoint, steps, lhybrid, fully-trained, model","steps, using 25, 50, 100, 200, 400, 1000, and 4000 sampling steps . We do this for both a fully-trained checkpoint, and a checkpoint mid-way through training . For CIFAR-10 we used 200K and 500K training iterations and for ImageNet64 we"
103,"ddim, striding, implicit, model","Song et al. propose a fast sampling algorithm for DDPMs by producing a new implicit model that has the same marginal noise distributions . DDIM produces better samples with fewer than 50 sampling steps, but worse samples when using 50 or more steps ."
112,"gan, mode-coverage, class-conditional, model","to make our models class-conditional, we inject class information through the same pathway as the timestep t . In particular, we pass this embedding to residual blocks ."
118,"fid, log-likelihood, nll","In previous sections, we showed algorithmic changes that improved log-likelihood and FID without changing the amount of training compute . However, a trend in modern machine learning is that larger models and more training time tend to improve model performance ."
119,"compute, imagenet, objective, lhybrid, model, training, capacity","To measure how performance scales with training compute, we train four different models on ImageNet 64 x 64 with the Lhybrid objective described in Section 3.1 . To change model capacity, we apply a depth multiplier across all layers, such that the first layer has either 64, 96, 128, or "
127,"objective, lvlb, lhybrid, log-likelihood, training, distribution","this could be caused by a variety of factors, such as 1) an unexpectedly high irreducible loss for this type of diffusion model, or 2) the model overfitting to the training distribution ."
129,"task, ddpm, generation, image","Concurrent to our work, Chen et al. use improved schedule and L1 loss to allow sampling with fewer steps with very little reduction in sample quality . However, compared to our unconditional image generation task, their generative task has a strong input conditioning signal provided by the melspectrograms ."
132,"generative, ancestral, evaluation, implicit, model, ddpm, process, function, diffusion","Song et al. proposes fast sampling algorithms for models trained with the DDPM objective by leveraging different sampling processes . The method allows fast sampling directly from the ancestral process, which removes the need for extra hyperparameters ."
135,"objective, modifications, likelihood, lhybrid, ddpm, quality",The likelihood is improved by learning 20 using our parameterization and Lhybrid objective . This brings the likelihood of these models much closer to other likelihood-based models .
142,"language, analysis, modelling, model","Language models are few-shot learners, 2020 . 2020 is the first time language models have been developed in the world . Language models have a few shot learners ."
187,"attention, imagenet, multi-head","For all of our experiments, we use a UNet model architecture4 similar to that used by Ho et al. We changed the attention layers to use multi-head attention . In particular, instead of computing a conditioning vector v and injecting it into hidden state h as GroupNorm, we compute conditioning vectors"
188,"imagenet, mirror, downsampling, stack, image","The upsampling stack is setup as a mirror image of the stack . From highest to lowest resolution, the UNet stages use channels . We estimate that, with C = 128, our model is comprised of 120M parameters ."
190,"lear, computation, adam","For most experiments, we use Adam for a batch size of 128, a learning rate of 10-4 and an exponential moving aver, age over model parameters with a rate of 0.9999 . For our larger classconditional ImageNet 64 x 64 experiments we scaled up the batch size to 2048 for faster"
194,"imagenet, computing, unconditional, reference, statistics, distribution",unconditional ImageNet 64 x 64 models are trained and evaluated using the official ImageNet-64 dataset . LSUN training samples are used for CIFAR-10 and ImageNet .
198,"lhybrid, bedroom, lsun, mod-, lsimple, els","Lhybrid and Lsimple models were trained on the LSUN bedroom dataset . We train two models: one with batch size 64 and learning rate 2 x 10-5 as in Ho et al., and another with a larger batch size 128 ."
203,"imagenet, diffusion, likelihood-based, model",We trained two models on class conditional ImageNet 256 x 256 models . The first is a usual diffusion model that directly models the images . the second reduces compute by chaining a pretrained 64 x 64 model p .
216,"trade-off, process, lhybrid, diffusion","Figure 13 shows that the model resulting from Lvlb is better at the start and end of the diffusion process, while Lhybrid is better throughout the middle of the diffuse process . This suggests that the lower sample quality is due to the lower quality of the sample."
221,"ddim, cifar-10, timesteps, nll","Figures 15 plots negative log-likelihood as a function of number of sampling steps for ImageNet 64 x 64 and CIFAR-10 . To address this, we use a strided subset of timesteps as for FID, but also include every t from 1 to T/K"
227,"cifar-10, optimization, cosine, schedule","On CIFAR-10, we noticed that all models overfit, but tended to reach optimal performance more quickly than those trained with the linear schedule . In our experiments, we corrected for this difference by using more dropout for our cosine models than the linear models ."
231,"cifar-10, overfitting",We surprisingly observed overfitting on ImageNet 64 x 64 . The main observable result was that FID started becoming worse over the course of training . We tried runs with dropout 0.1 and 0.3 .
