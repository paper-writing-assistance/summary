element_idx,summarized_text,keywords
4,"Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism . In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance . We regard the local patches as ""visual sentences"" and present to further divide them into smaller","imagenet, transformer, visual"
6,Transformer is widely used in the field of natural language processing . The power of these transformer models inspires the whole community to investigate the use of transformer for visual tasks .,"convolutional, neural, language, (nlp), natural, vision, processing, transformer, networks, computer"
7,"researchers have explored transformer architectures for conducting visual tasks . For example, Wang et al. explore self-attention mechanism in non-local networks for capturing long-range dependencies in video .","image, transformer, architecture, recognition"
13,"Different from the data in NLP tasks, there exists a semantic gap between input images and the ground-truth labels in CV tasks . To this end, Dosovitskiy et al. develop the ViT , which paves the way for transferring the success of transformer based NLP models","imagenet, image, vision, processing, computer"
14,"the aforementioned visual transformers have made great efforts to boost the models' performance, but most of existing works follow the conventional representation scheme used in ViT . Such a exquisite paradigm can effectively capture the visual sequential information and estimate the attention between different image patches .","imagenet, transformers, visual"
15,"In this paper, we propose a novel Transformer-iN-Transformer architecture for visual recognition . We first divide the input images into several patches as ""visual sentences"" and then further divide them into sub-patches . Specifically, features and attentions between visual words in each visual sentence are calculated independently using a","words, imagenet, visual, recognition"
28,"In TNT, we view the patches as visual sentences that represent the image . Each patch is further divided into m sub-patches, i.e., a visual sentence is composed of a sequence of visual words .","image, tnt, sentence, visual"
33,The input of the first block Yo is just Yo in Eq. 5. All word embeddings in the image after transformation are Vi = . This   process builds relationships between visual words by computing interactions between any two visual words .,"word, transformation, visual"
34,"we create the sentence embedding memories to store the sequence of sentencelevel representations: Zo = E Rxd where Zclass is the class token , similar to ViT . Each layer is transformed into the domain of sentence embeddering by linear projection .","sentence, representations, level, sentence-level"
38,"In our TNT block, the inner transformer block is used to model the relationship between visual words for local feature extraction . The outer transformer block captures intrinsic information from the sequence of sentences .","tnt, network, intrinsic, information, transformer-in-transformer, block"
49,"the increase of FLOPs is small since c  d and 0 2 O in practice . For example, in the DeiT-S configuration, we have d = 384 and n = 196 . From Eq 15 and Eq. 17, we can obtain that FLOPST =","flops, tnt-s, tnt, transformer, block"
51,We build our TNT architectures by following the basic configuration of ViT and DeiT . The number of sub-patches is set as m = 4  4 = 16 by default .,"tnt-s, tnt, tnt-ti, architecture, t"
60,ImageNet ILSVRC 2012 is an image classification benchmark consisting of 1.2M training images belonging to 1000 classes . We adopt the same data augmentation strategy as that in DeiT .,"imagenet, image, benchmark, strategy, classification"
61,"The data augmentation strategy of image classification datasets is the same as that of ImageNet . For COCO and ADE20K, the data hausse strategy follows that in PVT .","imagenet, pvt, datasets, visual"
62,"Implementation Details. We utilize the training strategy provided in DeiT . The main advanced technologies apart from common settings include Adam W , label smoothing , DropPath , and repeated augmentation .","dynamics, deit, details"
66,We train our TNT models with the same training settings as that of DeiT . We also include the representative CNN-based models such as ResNet and RegNet .,"tnt, transformer, cnn, visual"
73,"Deployment of transformer models on devices is important for practical applications, SO we test the inference speed of our TNT model . Following, the throughput is measured on an NVIDIA V100 GPU and PyTorch, with 224x224 input size . We can reduce the used TNT blocks and replace","inference, test, speed, transformer"
77,"Position information is important for image recognition . In TNT structure, sentence position encoding is for maintaining global spatial information . We verify their effect by removing them separately .","position, encoding, image, tnt-s, recognition, sentence"
86,"In TNT, the input image is Table 8: Effect of #words m. split into a number of 16x 16 patches and each patch is m c Params FLOPs Top-1 . Here we test the ef16 24 23.8M 5.2B 81.5 fect of hyper-","words, tnt, visual"
91,"Visualization of Feature Maps. We visualize the learned features of DeiT and TNT to further understand the effect of the proposed method . For better visualization, the input image is resized to 1024x 1024 . The feature maps are formed by reshaping the patch embeddings according to","feature, tnt, maps, information, local"
92,We visualize the pixel-level embeddings of TNT in Fig. 4 . The averaged feature maps corresponding to the 14x 14 patches are shown . We can see that local information is well preserved in the shallow layers .,"tnt, features, embeddings, patch-level"
93,Visualization of Attention Maps. There are two self-attention layers in our TNT block . We show the attention maps of different queries in the inner transformer in Figure 5 .,"visualization, attention, maps"
101,"Pure Transformer Image Classification evaluates our models on 4 image classification datasets with training set size ranging from 2,040 to 50,000 images . These datasets include superordinate-level object classification and fine-grained object classification . We adopt the same training settings as those at the pre-training stage by preserving all data","image, transformer, deit, classification"
104,Pure Transformer Object Detection builds a pure transformer object detection pipeline . All the compared models are trained using Adam W with batch size of 16 . The training images are randomly resized to have a shorter side .,"pure, tnt-s, detr, tnt, detection, backbone, object, transformer"
109,We follow the training and testing configuration in PVT for fair comparison . All the compared models are trained by AdamW optimizer with initial learning rate of 1e-4 and polynomial decay schedule .,"optimizer, segmentation, adamw, semantic, pvt, transformer"
114,"In this paper, we propose a novel Transformer-iN-Transformer network architecture for visual recognition . In particular, we uniformly split the image into a sequence of patches and view each patch as an sequence of sub-patches . The information of visual word embeddings is added to the visual sentence embed","imagenet, tnt, visual, recognition"
119,"Attention between Patches. In Figure 6, we plot the attention maps from each patch to all the patches . DeiT-S and TNT-S, more patches are related as layer goes deeper .","tnt, attention, deit, patches"
127,"Inspired by squeeze-and-excitation network for CNNs , we propose to explore channel-wise attention for transformers . We first average all the sentence embeddings and use a two-layer MLP to calculate the attention values . From the results in Table 12, adding SE module into TNT can further","network, channel-wise, attention, squeeze-and-excitation"
132,"FPN takes 4 levels of features to form multi-level representation . To match the feature shape, we insert deconvolution/convolution layers with proper stride .","multi-scale, model, framework, vision, fpn, fpn-like"
136,"An image is worth 16x16 words: Transformers for image recognition at scale . In CVPR, 2020. Kai Han, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Shuyang Sun, Wansen Feng, Ziwe","image, vision, recognition, cvpr, transformer"
138,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, pages 1097-1105, 2012. Tsung- Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, B","network, transformer, vision, dynamic"
140,"ICCV, 2019. HongyiZhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In CVPR, 2021. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi","transformers, ade20k, dataset, imagenet"
