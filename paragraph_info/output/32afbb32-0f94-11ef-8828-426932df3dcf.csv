element_idx,keywords,summarized_text
6,"audio, text-to-audio, processing, language","LLM FLAN-T5 is the text encoder for text-to-audio generation-a task . The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5 . This improvement might also"
10,"text-to-image, automatic, multimodal, (tti), ai",researchers have succeeded in text-to-image generation by employing similar techniques as the former . Such models may have strong potential use cases in the media production . These techniques also pave the path toward general-purpose multimodal AI .
11,"level-based, llm, large, mixing, pressure, transformer, decoder","The existing works use a large text encoder, such as, RoBERTa and T5 , to encode the textual description of the audio to be generated . Subsequently, the audio prior is subsequently decoded by a pre-trained VAE, followed by an vocoder "
12,"model, llm, llm-based, large, language, state-of-the-art, dataset","Our model 1 is inspired by latent diffusion model and AudioLDM models . Instead of using CLAP-based embeddings, we used a large language model due to its powerful representational ability and fine-tuning mechanism . Our experimental results show that using an LLM greatly improves text-to-audio"
13,"test, approach, baseline, audiocaps, tango, dataset, smaller","In Section 3, we empirically show that TANGO outperforms AudioLDM and other baseline approaches on most of the metrics on AudioCaps test set . We believe that if it is trained on a larger dataset such as AudioSet, it would be able to provide even better results and improve its ability to recognize"
18,"tango, disco, discourse",The acronym TANGO stands for Text-to-Audio using iNstruction Guided diffusiOn . The word is often associated with music and dance . It was suggested by ChatGPT .
23,"textual-prompt, vae, mel-spectogram, mel-spectogram/audio, audio, model, encoder, latent, diffusion","TANGO has three major components: textual-prompt encoder, latent diffusion model . The textual representation is used to construct a latent representation of the audio or audio prior from standard Gaussian noise, using reverse diffusion . This mel-spectogram is fed to a vocoder to"
25,"gradient, transformation, audio, prior, descent, flan-t5-large, text-to-audio",We use the pre-trained LLM FLAN-T5-LARGE as the text encoder to obtain text encoding T E RLxdtext where L and dtext are the token count and token-embedding size respectively . Dai et al. posit that they are
34,"eo, training, audio-guided, estimation, noise","Noise estimation eo is parameterized with U-Net with a cross-attention component to include the text guidance T . AudioLDM uses audio as the guidance during training . During training, they switch back to text guidance ."
37,"pressure, audio, level, pair, g","To mix audio pairs, we do not take a random combination of them . We instead consider the human auditory perception for fusion . The weight of an audio sample is calculated as a relative pressure level ."
45,"audio, variational, auto-encoder","Audio variational auto-encoder compresses the mel-spectogram of an audio sample into an audio prior zo E RCxT/rxF/r where C, T. , F, r are the number of channels, m E RTxF number of time-slot"
49,"audio, text-to-audio, generation, classification, audiocaps, dataset","AudioCaps contains 45,438 audio clips paired with human-written captions . The audio clips are ten seconds long and were collected from YouTube videos ."
50,"audio, clip, signal, audiocaps",AudioCaps test set contains five human-written captions for each audio clip . We use one caption for each clip chosen at random following Liu et al.
51,"audioset, vo, vocoder, audiocaps","This VAE network was trained on AudioSet, AudioCaps, Freesound2 and BBC Sound Effect Library3 datasets . Longer audio clips in Freesound and BBC SFX were truncated to the first thirty seconds . All audio clips were resampled in 16KHz frequency for training the"
56,"adamw, learning, scheduler, optimizer, training, rate",We train the model for 40 epochs on the AudioCaps dataset . We use four A6000 GPUs for training TANGO . The effective batch size for training is 3 *4 *4 = 48 .
58,"audiogen, audioldm",AudioGen and DiffSound use text embeddings for conditional generative training . AudioLDM asserts they are effective in capturing cross-modal information . The models were pre-trained on large datasets including AudioSet .
60,"checkpoint, audioldm-l-full, audiol, audiocaps, dataset",Audi oLDM-L-Full-FT checkpoint from Liu et al. was not available for our study . This checkpoint was fine-tuned on AudioCaps and MusicCaps datasets .
63,"fad, fd","FAD is a perceptual metric that is adapted from Fechet Inception Distance for the audio domain . It measures the distance between the generated audio distribution and the real audio distribution without using any reference audio samples . In addition to FAD, we also used Frechet Distance as an objective metric ."
67,"audiogen, audioldm","We comapre our proposed method TANGO with DiffSound , AudioGen and various configurations of AudioLDM . For a fair comparison, we also use 200 inference steps ."
68,"ldm, text, model, encoder, training, audiocaps, tango",TANGO achieves new state-of-the-art results for objective metrics when trained only on the AudioCaps dataset . This is significantly better than the most direct baseline AudioLDM-L . We attribute this to the use of FLAN-T5 as text encoder .
70,"audioldm, baseline, tango, tta, models",FT indicates the model is fine-tuned on the Audiocaps dataset . The AS and AC stand for AudioSet and AudiocCaps datasets respectively .
77,"audio, dataset, label, classification",All audio clips of longer than 10 seconds were segmented into partitions of successive 10 seconds or shorter . We also resampled all audio clips to 16KHz . Urban Sound and ESC50 datasets contain various environmental sounds .
78,"tango, tango-, audiocaps, tango-full-ft","We fine-tuned the model specifically on the AudioCaps dataset . The obtained results demonstrate a remarkable performance improvement achieved by TANGO-FULL-FT compared to similar models in the AudioLDM family . To optimize the training process, we set the batch size per GPU to 2 and employed 8 gradient accumulation"
81,"audioldm-l, rca","Table 4 presents a comparison between random and relative pressure-based data augmentation strategies . In comparison, TANGO outperforms AudioLDM-L in two out of three objective metrics . This notable improvement can be attributed to the integration of a powerful large language model as a textual prompt encoder within"
84,"inference, guidance, steps, layer, scale, mod-, audiocaps, els, quality, diffusion",The number of inference steps and the classifier-free guidance scale are of crucial importance for sampling from latent diffusion models . We found that a guidance scale of 3 provides the best results for TANGO . The generated audio quality and resultant objective metrics consistently become better with more steps .
89,"guidance, classifier-free, scale","We report the effect of varying guidance scale with a fixed 100 steps in the right half of Table 5. The first row uses a guidance scale of 1 thus effectively not applying classifier-free guidance at all during inference . Not surprisingly, the performance of this configuration is poor . We obtain the best FAD metric"
90,"modelling, sequence, event, audioldm, model, multiple, temporal, tango","Temporal Sequence Modelling analyzes how TANGO and AudioLDM models perform audio generation when text prompt contains multiple sequential events . We segregate the AudioCaps test set using the presence of temporal identifiers while, before, after, then, followed into two subsets, one with multiple events and"
91,"single, event, test, events, multiple, audiocaps, set",The multiple events and single event subsets constitute the entire AudioCaps test set . It should be noted that FD and FAD are corpus-level non-linear metrics .
93,"text, audio, event, generation, classification, audiocaps, task, prompt","AudioCaps dataset contains a total of 632 audio event classes . For texts with multiple labels, AudioLDM achieves a better KL divergence score and TANGO achieves better FD and FAD scores ."
97,"relative, level, augmentation, strategy, pressure",Effect of Augmentation and Distribution of Relative Pressure-Level for Augmentations We described our augmentation strategy earlier in Section 2.3 . The distribution of the relative pressure level p in Equation across the training samples is shown in Figure 2 .
104,"tango, audiocaps","The Music category is very rare in AudioCaps and the rest appear on their own or in various combinations with others . We select the most frequently occurring category combinations . The performance of the two models is pretty balanced across the FD and KL metrics, with TANGO being better in some, and AudioLDM in others "
106,"probabilistic, stochastic, model, refinement, diffusion","Recent years have seen a surge in diffusion models as a leading approach for generating high-quality speech . These models utilize a fixed number of Markov chain steps to transform white noise signals into structured waveforms . By leveraging a stack of timeaware diffusion processes, FastDiff can generate speech samples of"
107,"augmented, text, text-to-audio, generation, strategy, decoder, non-autoregressive","In Yang et al., a text encoder is used to obtain text features . These tokens are fed to a vector quantized VAE to generate mel spectrograms . The non-autoregressive decoder is a probabilistic diffusion model ."
108,"audio, audioldm, embeddings, text","Recently, Liu et al. proposed AudioLDM, which translates the Latent Diffusion Model of textto-visual to text-to-audio generation . They pre-trained VAE-based encoder-decoder networks to learn a compressed latent representation of audio, which was then used"
111,"tango, text-to-audio, models, generation","TANGO is not always able to finely control its generations over textual control prompts as itis trained only on the small AudioCaps dataset . For example, Chopping tomatoes on a wooden table produces similar audio samples . Training text-to-audio generation models on larger datasets is thus required for the model"
113,"flan-t5, audio, text-to-audio, generation, super-resolution","In this work, we investigate the effectiveness of the instruction-tuned model, FLAN-T5 for textto-audio generation . Specifically, we use the textual embeddings produced in the latent diffusion model to generate mel-spectrogram tokens . These tokens are then fed to "
117,acoustics,"Won Chung, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation . arXiv preprint arxiv:2301.11325, 2023 ."
119,"acoustics, audio, synthesis, generation, model, quality, audio-generation, speech","Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao. Conditional diffusion probabilistic model for speech enhancement. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
121,"deep, analysis, audio, sound, raw, image","Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei. Diffusion-based voice conversion with fast maximum likelihood sampling scheme . ArXiv, abs/2204.06"
123,"text-to-sound, diffusion, model","Wikipedia. Tango. https :/ / en.wikipedia. org/wiki/Tango, 2021. Wikipedia. Diffsound: Discrete diffusion model for text-to-sound generation."
