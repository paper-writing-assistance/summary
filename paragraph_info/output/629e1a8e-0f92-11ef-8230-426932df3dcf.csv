element_idx,summarized_text,keywords
3,"Detection TRansformer is a set-based global loss that forces unique predictions via bipartite matching and a transformer encoder-decoder architecture . The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors .","pipeline, compo-, nent, detr, prediction, hand-designed, detection, set"
6,"The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest . Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals , anchors or window centers .","boxes, regression, bounding, surrogate, detection, object"
12,We adopt an encoder-decoder architecture based on transformers . The self-attention mechanisms of transformers make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions.,"encoder-decoder, transformers, architecture"
13,"DETR simplifies the detection pipeline by dropping multiple hand-designed components that encode prior knowledge . DETR doesn't require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes .","training, pipeline, detr, detection, transformer"
14,"the main features of DETR are the conjunction of the bipartite matching loss and transformers with parallel decoding . In contrast, previous work focused on autoregressive decoding with RNNs 43,41,30,36,42 .","decoding, parallel"
15,"Faster R-CNN has undergone many design iterations and its performance was greatly improved since the original publication . We evaluate DETR on one of the most popular object detection datasets, COCO .","baseline, faster, coco, fpn, r-cnn"
24,"Basic set prediction task is multilabel classification for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements . For constant-size set prediction, dense fully connected networks are sufficient but costly . A general approach is to design a loss based","matching, bipartite, multilabel, prediction, set, direct, classification"
26,"Vaswani et al. introduced Transformers as a new attention-based building block for machine translation . Attention mechanisms are neural network layers that aggregate information from the entire input sequence . Transformers introduced self-attention layers, similar to Non-Local Neural Networks . One of the main advantages of attention","machine, block, transformers, models, building, translation, attention-based"
30,Transformers were first used in auto-regressive models following early sequenceto-sequence models . The prohibitive inference cost lead to the development of parallel sequence generation .,"sequence, transformers, parallel, models, auto-regressive, generation"
32,"Most modern object detection methods make predictions relative to some initial guesses . Two-stage detectors predict boxes w.r.t. proposals, whereas single-stage method make predictions . anchors or a grid of possible object centers .","analysis, predictions, detection, handcrafted, process"
33,"Several object detectors used the bipartite matching loss . However, in these early deep learning models, the relation between different prediction was modeled with convolutional or fully-connected layers only .","matching, loss, bipartite, set-based, nms"
34,"Learnable NMS methods and relation networks explicitly model relations between different predictions with attention . Using direct set losses, they do not require any post-processing steps .","relation, model, networks, relations, nms"
35,"Closest to our approach are end-to-end set predictions for object detection and instance segmentation 41,30,36,42 Similarly to us, they use bipartite-matching losses with encoder-decoder architectures based on CNN activations to directly produce a set of bounding boxes ","segmentation, recurrent, rnn, instance, detection, cnn"
41,"DETR infers a fixed-size set of N predictions, in a single pass through the decoder . Our loss produces an optimal bipartite matching between predicted and ground truth objects .","ground, number, truth, objects, of, decoder"
44,The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes . Each element i of the ground truth set can be seen as a Yi = where Ci is the target class label and bi E 4 is a vector that defines ground truth box center coordinates and its height and width relative to,"matching, lmatch, ground, set, cost, truth"
50,class imbalance is analogous to how Faster R-CNN training procedure balances positive/ negative proposals by subsampling . In the matching cost we use probabilities Po instead of log-probabilities .,"probability, class, proposals, imbalance, faster, negative, positive/, r-cnn"
51,The second part of the matching cost and the Hungarian loss is Lbox that scores the bounding boxes . To mitigate this issue we use a linear combination of the l1 loss and the generalized IoU loss Liou that is scale-invariant .,"(bi, matching, loss, (bi), mat, bounding, cost, box, lbox"
56,"The encoder expects a sequence as input, hence new feature map zo E RdxHxW . Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network .","layer, encoder, transformer, attention"
62,"Transformer decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed self- and encoder-decoder attention mechanisms . The difference with the original transformer is that our model decodes the N objects in parallel . Vaswani et al. use","embeddings, transformer, n, decoder"
63,"The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d . The linear layer predicts the class label using a softmax function . This class plays a similar role to the ""background"" class in standard object detection approaches.","feed-forward, projection, linear, layer, networks, ffn"
69,"DETR achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO . Finally, we present results on panoptic segmentation, training only a small extension on a fixed DETR model .","detr, architecture, evaluation, qualitative"
70,"COCO 2017 detection and panoptic segmentation datasets contain 118k training images and 5k validation images . There are 7 instances per image on average, up to 63 instances in a single image in training set . If not specified, we report AP as bbox AP .","coco, validation, computer"
71,All transformer weights are initialized with Xavier init . The backbone is with ImageNet-pretrained ResNet model from TORCHVISION with frozen batchnorm layers .,"model, detr, backbone, flop, resolution, transformer"
72,"We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 . We also use random crop augmentations during training, improving the performance by approximately 1 AP .","image, augmentation, input, training"
74,Table 1: Comparison with Faster R-CNN with ResNet-50 and ResNet-101 backbones on COCO validation set . Results without R101 in the name correspond to ResNet 50 .,"fps, flops, model, r101, detr, backbones, resnet-101"
76,"To optimize for AP, we override the prediction of these slots with the second highest scoring class . This improves AP by 2 points compared to filtering out empty slots . For our ablation experiments we use training schedule of 300 epochs with a learning rate drop by a factor of 10","shorter, schedule, ap, empty"
78,"Transformers are typically trained with Adam or Adagrad optimizers with very long training schedules and dropout . Faster R-CNN is trained with SGD with minimal data augmentation . To align it with DETR, we add generalized IoU to the box loss, the same random crop augmentation and long","training, adagrad, long, adam, dropout"
82,In Table 1 we show the results for multiple DETR models . Faster R-CNN and DETR with ResNet-101 backbone show comparable results . DETR-DC5 with the same number of parameters has higher AP .,"detr, fpn"
84,"We choose ResNet-50-based DETR model with 6 encoder, 6 decoder layers and width 256 . The model has 41.3M parameters, achieves 40.6 and 42.0 AP on short and long schedules respectively, and runs at 28 FPS .","mechanisms, attention, transformer, decoder"
85,"Without encoder layers, overall AP drops by 3.9 points, with a more significant drop of 6.0 AP on large objects . In Figure 3, we visualize attention maps of the last encoder layer of a trained model .","image-level, reasoning, global, encoder, self-attention, layer, scene"
90,"Both AP and AP50 improve after every layer, totalling into a very significant +8.2/9.5 AP improvement between the first and the last layer . With its set-based loss, DETR does not need NMS by design .","depth, layer, default, increases, parameters, nms, decoder"
91,"we visualize decoder attentions in Fig. 6 coloring attention maps for each predicted object in different colors . We hypothesise that after the encoder has separated instances via global attention, it only needs to attend to the extremities to extract the class and object boundaries .","global, attention, decoder"
92,"FFN inside tranformers can be seen as 1 x 1 convolutional layers . By reducing network parameters from 41.3M to 28.7M, performance drops by 2.3 AP .","convolutional, transformer, networks, ffn"
99,"Output positional encodings are required and cannot be removed, SO we experiment with either passing them once at decoder input or adding to queries at every decoding layer . In the first experiment we completely remove spatial positional positions and pass output positionals at input and, interestingly, the model still achieve more than 32","spatial, fixed, sine, encodings, output, positional"
101,"We train several models turning them on and off . There are three components to the loss: classification loss, l1 bounding box distance loss, and GIoU loss . Results are presented in table 4 .","cost, matching, giou, loss"
105,Table 3: Results for different positional encodings compared to baseline . Learned embeddings are shared between all layers . Not using spatial positional positions leads to a significant drop in AP .,"drop, encodings, embeddings, ap, positional, learned"
112,Fig. 7: Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder . Each box prediction is represented as a point with the coordinates of its center in the 1-by-1 square normalized by each image size .,"size, image, coco, prediction, box, dataset"
115,Decoder output slot analysis In Fig. 7 we visualize the boxes predicted by different slots for all images in COCO 2017 val set . DETR learns different specialization for each query slot .,"detr, box, image-wide"
116,We create a synthetic image3 to verify the generalization ability of DETR . This experiment confirms that there is no strong class-specialization in each object query .,"training, image, detr, set, generalization"
126,"We train DETR to predict boxes around both stuff and things classes on COCO . We also add a mask head which predicts a binary mask for each of the predicted boxes . To make the final prediction, an FPN-like architecture is used .","both, mask, stuff, coco, architecture, head, fpn, fpn-like, transformer, decoder"
132,The new mask head is trained for 25 epochs . We filter out the detection with a confidence below 85% . After inference we compute the per-pixel argmax .,"bounding, coco, detection, box, dataset"
133,In Table 5 we compare our unified panoptic segmenation approach with several established methods that treat things and stuff differently . We show that DETR outperforms published results on COCO-val 2017 as well as our strong PanopticFPN baseline .,"pq, stuff, things, on, detr, break-down"
136,DETR is a new design for object detection systems based on transformers and bipartite matching loss . The approach achieves comparable results to an optimized Faster R-CNN baseline on the challenging COCO dataset .,"matching, loss, bipartite, transformers, detr, prediction, set, direct"
141,"In: AAAI Conference on Artificial Intelligence 2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate . In: ICCV 4. Bodla, N., Singh, B., Chellappa, R.","machine, self-attention, conditional, attention, translation"
143,"In: CVPR 18. He, K., Gkioxari, G., Dollar, P., Girshick, R.B.: Panoptic feature pyramid networks for object detection . In: ICCV 16. Hosang, J.H., Benenson R., Schiele, B","pre-training, image-to-set, common, detection, object"
145,"ICCV 41. Romera-Paredes, B., Abbasnejad, E., Dick, A., Reid, I.: Deepsetnet: Predicting sets with unknown permutation and cardinality using deep neural networks. In: CVPR 44. Sutskever, I","deep-set, neural, segmentation, network, semantic, deep, instance, networks"
151,"the output is the same size as the query sequence . To fix the vocabulary before giving details, multi-head self-attention is the special case Xq = Xkv .","xkv, sequence, value, query"
157,"the i-th row is given by attni = Ej=1 i,jVj . The final output is the aggregation of values weighted by attention weights .","attention, value, weights"
158,"The original transformer alternates multi-head attention and so-called FFN layers . The FFN we consider is composed of two-layers of 1x1 convolutions with ReLU activations . There is also a residual connection/dropout/layernorm after the two layers, similarly to equation 6.","attention, layer, multi-head, ffn"
160,All losses are normalized by the number of objects inside the batch . It is important to normalize by the total number of items in all sub-batches .,"gpu, completeness, losses"
163,"1.1 means ""area"" . The areas of unions or intersections are computed by min / max of the linear functions of bo and bi . B, bi means the largest box containing bo, bi.","coordinates, b, stochastic, gradients, intersection, box"
168,image features from the CNN backbone are passed through the transformer encoder . Positional encodings are added to queries and keys at every multihead self-attention layer . The decoder produces the final set of predicted class labels .,"encoding, detr, self-attention, positional, decoder"
171,"In the encoder, each self-attention is in O, while O2 is the cost of computing the attention weights for one head . Other computations are negligible .","self-attention, encoder, complexity"
173,FLOPS for Faster R-CNN depends on the number of proposals in the image . We use it without modifications for Detectron2 models and extend it to take batch matrix multiply into account for DETR models.,"flops, model, count, computation, flop"
176,Backbone ImageNet pretrained backbone ResNet-50 is imported from Torchvision . Backbone batch normalization weights and statistics are frozen during training .,"renewable, learning, rate, network, backbone"
179,Baseline Our enhanced Faster-RCNN+ baselines use GIoU loss along with the standard l1 loss for bounding box regression tasks respectively . For the baselines we adopt the same data augmentation as in DETR and train it with 9x schedule .,"regression, baseline, faster-rcnn+, bounding, box"
180,In our model we use a fixed absolute encoding to represent these spatial positions . For both spatial coordinates of each embedding we independently use S2 sine and cosine functions with different frequencies .,"s2, sine, case, function, 3d"
189,"In this section, we analyze the behavior of DETR when approaching this limit . We select a canonical square image of a given class, repeat it on a 10 x 10 grid, and compute the percentage of instances that are missed by the model . To test the model with less than 100 instances, we randomly","image, instances, number, detr, of"
190,"This test is a test of generalization out-of-distribution by design . There are very few examples of a single class . It is difficult to disentangle, from the experiment, two types . This type of experiment represents our best effort to understand whether query objects overfit the label and position distribution","out-of-domain, image, out-of-distribution, coco, generalization, computer"
195,"Inference code runs with Python 3.6+, PyTorch 1.4 and Torchvision 0.5 . Note that it does not support batching . The entire code to reproduce the experiments will be made available before the conference .","inference, pytorch, code"
212,DETR PyTorch inference code uses learnt positional encodings in encoder instead of fixed . The entire code to reproduce the experiments will be made available before the conference .,"inference, pytorch, code, transformer"
