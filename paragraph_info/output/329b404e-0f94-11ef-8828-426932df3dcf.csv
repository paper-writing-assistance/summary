element_idx,keywords,summarized_text
3,"tree-structured, design, networks, network, lstm","Tree-structured neural networks encode a particular tree geometry for a sentence in the network design . However, these models have at best only slightly outperformed simpler sequence-based models . We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive"
5,"nlp, analysis, networks, neural, sentiment, recurrent","Neural networks encode sentences as real-valued vectors have been successfully used in a wide array of NLP tasks . These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right ."
6,"tree, complex, derivation, model, compositional, meanings","tree models are often presented as the more principled choice, since they align with standard linguistic assumptions about constituent structure and compositional derivation of complex meanings . however, head-to-head comparisons with sequence models show either modest improvements or none at all ."
7,"lstm, sequence, sentence, meaning, models",Standard sequence models can learn to exploit recursive syntactic structure in generating representations of sentence meaning . This requires sequence models be able to use the structure that tree models are explicitly designed around . We believe this is plausible on the basis of other recent research .
8,"tree, recursive, artificial, dataset, models, structure","We compare standard tree and sequence models on their handling of recursive structure by training the models on sentences whose length and recursion depth are limited . We then test them on longer and more complex sentences . Our methods extend those of our earlier work in , which introduces an experiment and corresponding artificial dataset to"
14,"structures, generalization, large",standard tree neural networks are able to make the necessary generalizations . extending the training set to include larger structures mitigates this decay .
15,"natural, syntactic, language, hashes, simple, lstm, structure","LSTMs can learn to recognize syntactic structure in natural language . The simplest model presented in uses a sequence model to encode each sentence as a vector, and then generates a linearized parse with high accuracy using only the information present in the vector . However, the massive size of the"
17,"entailment, recognization, network, model",recognizing textual entailment The data that we use define a version of the task . This task is well suited to evaluating neural network models for sentence interpretation . models must develop comprehensive representations of the meanings of each sentence to do well .
19,"re-, cursive, artificial, language, structure",artificial language The language described in is designed to highlight the use of recursive structure with minimal additional complexity . Its vocabulary consists only of six unanalyzed word types . Sentences of the language can be straightforwardly interpreted as statements of propositional logic .
23,"token, bracketing, binary, word","Our models use this information in two ways . For the tree models, the parentheses are word tokens with associated learned embeddings . This approach provides the models with equivalent data ."
24,"interpretation, function, training, bin","Our sentence pairs are divided into thirteen bins according to the number of logical connectives in the longer of the two sentences in each pair . In three experiments, we train our models on the training portions of bins 0-3 , 0-4 , and 0-6 and test on every bin but the trivial bin"
26,"model, classifier, architecture, sentence, multilayer",model architecture builds on the one used in Figure 1a . The model architecture uses two copies of a single sentence model to encode the premise and hypothesis expressions .
27,"plain, classifier, neural, network, recursive/recurrent, tensor","Classifier The classifier component of the model consists of a combining layer which takes the two sentence representations as inputs, followed by two neural network layers, then a softmax classifier . For the blending layer, we use a neural tensor network layer, which sums the output of "
29,"content, system, learning, addressing, complex, attention, correspondences","We only study models that encode entire sentences in fixed length vectors . We set aside models with attention, a technique which gives the downstream model the potential to access each input token individually through a soft content addressing system ."
34,"lstm, sentence, models","Sentence models The sentence encoding component of the model transforms the embeddings of the input words for each sentence into a single vector representing that sentence . In experiments with a simpler non-LSTM RNN sequence model, the model tended to badly underfit the training data ."
35,"likelihood, training, classification, l2, adadelta, regularization",Training We randomly initialize all embeddings and layer parameters . We train them using minibatch stochastic gradient descent . Our objective is the standard negative log likelihood classification objective .
37,"of, familiar, data, length, training, dynamics, lstm","the LSTM underfitting slightly at 94.8% in the 6 setting . On longer test sentences, the tree models decay smoothly in performance across the board . In that setting, all models generalized well to structures of familiar length ."
46,"sequence-based, model, recursive, artificial, language, structure",all four models are able to effectively exploit a recursively defined language to interpret sentences with complex unseen structures . We find that tree models' biases allow them to do this with greater efficiency .
47,"ambiguity, lexical, models, sequence","Tree architectures provide an explicit bias that makes it possible to efficiently learn to compositional interpretation . Sequence models lack this bias, but have other advantages ."
52,"science, of, tering, force, national, text, air, foundation, deft, research, laboratory","Text Program under Air Force Research Laboratory contract no. FA875013-2-0040, the National Science Foundation under grant no. IIS 1159679, and the Department of the Navy, Office of Naval Research . Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect"
54,"short-term, backpropagation, long, memory","In Proc. NIPS, 2014. Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. Transitionbased dependency parsing with stack long short-term memory . In proc. ACL, 2015. Jeffrey L. Elman. Learning task-dependent distributed representations by back"
