element_idx,keywords,summarized_text
6,"synthesis, generation, dm, state-of-the-art, image","diffusion models achieve state-of-the-art synthesis results on image data and beyond . Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining ."
8,"computer, vision, multi-modal, gan, distribution","Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands . Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive transformers "
12,"dm, synthesis, state-of-the-art, image","results in image synthesis and beyond , and define the state-of-the-art in class-conditional image-synthesis and super-resolution . Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization or stroke-based synthesis . Being likelihood-based"
15,"approximately, a100, training, gpu","SO that producing 50k samples takes approximately 5 days on a single A100 GPU . Training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint ."
17,"space, pixel, model, trained, likelihood-based, diffusion","Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space . In the second stage, the actual generative model learns the semantic and conceptual composition of the data ."
18,"pass, latent, network, (ldms), models, diffusion","We train an autoencoder which provides a lower-dimensional representational space perceptually equivalent to the data space . We train DMs in the learned latent space, which exhibit better scaling properties with respect to the spatial dimensionality ."
19,"token-based, conditioning, model, dm, multiple, trainings, diffusion","a notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings . For the latter, we design an architecture that connects transformers to the DM's UNet backbone and enables arbitrary types of token-based conditioning mechanisms"
33,"resolution, images, pixels, density, low, raw, estimation",cult to optimize and struggle to capture the full data distribution . Variational autoencoders and flow-based models emphasize good density estimation . computationally demanding architectures limit them to low resolution images .
34,"probabilistic, density, dm, estimation, models, diffusion","Diffusion Probabilistic Models , have achieved state-of-the-art results in density estimation as well as sample quality . The best synthesis quality is usually achieved when a reweighted objective is used for training . Evaluating and optimizing these models in pixel space has the downside of low"
35,"two, transfer, stage, synthesis, generic, arm, image",VQ-VAEs use autoregressive models to learn an expressive prior over a discretized latent space . extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations .
36,"ldm, compression, computational, low, cost","Proaches and less compression comes at the price of high computational cost . Our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone . Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage ."
41,"unet, dm, low, architecture","By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space . This makes them particularly effective for data with spatial structure and alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches ."
43,"compression, loss, perceptual, model",Our perceptual compression model is based on previous work and consists of an autoencoder trained by combination of a perceptionual loss and a patch-based adversarial objective . This ensures that the reconstructions are confined to the image manifold by enforcing local realism
47,"compression, space, learned, latent, regularization",VQ-reg. uses a vector quantization layer within the decoder . This model can be interpreted as a VQGAN . We can use relatively mild compression rates and achieve very good reconstructions .
49,"model, score-matching, data, variable, distribution","Diffusion Models are probabilistic models designed to learn a data distribution p by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T . These models can be interpreted as an equally weighted sequence of denoizing autoencode"
51,"modeling, generative, representations, latent","Generative Modeling of Latent Representations With our trained perceptual compression models consisting of 3 and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away . This space is more suitable for likelihood-based generative"
60,"intermediate, layer, representation, cross-attention",We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism . We introduce a domain specific encoder that projects y to an intermediate representation To E R MxdT which is then mapped to the intermediate . Attention = softmax 
69,"ldm, first, stage, schemes, regularization","LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities . We analyze the gains of our models compared to pixel-based diffusion models in both training and inference . In E.2 we list details on architecture, implementation, training and evaluation for all results presented"
71,"ldm, nvidia, factors, a100, pixel-based, dms, downsampling","This section analyzes the behavior of our LDMs with different downsampling factors . To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section ."
73,"imagenet, information, progress, training, ldm-i, ldm-8, dataset, loss","Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet dataset . We see that small downsampling factors for LDM-1,2 result in slow training progress, whereas ii) overly large values of f cause stagn"
74,"imagenet, compression, synthesis, rate, quality",LDM-4-8 outperform models with unsuitable ratios of perceptual and conceptual compression . Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality .
76,"celeba-hq, lsun-churches, lsgm, lsun-bedrooms","On CelebA-HQ, we report a new state-of-the-art FID of 5.11 . We also outperform LSGM where a latent diffusion model is trained together ."
82,"downsampling, ldm, training, factors",Pixel-based LDM-1 requires substantially larger train times . All models are trained on a single NVIDIA A100 with the same computational budget . Results obtained with 100 DDIM steps and K = 0.
84,"ddim, imagenet, compression, varying","Different markers indicate 10, 20, 50, 100, 200 sampling steps using DDIM . The dashed line shows the FID scores for 200 steps . All models were trained for 500k / 2M steps on an A100."
97,"conditioning, ldm, text-to-image, cross-attention, based, diffusion","introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models . For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M "
103,"ldm, translation, synthesis, image-to-image, semantic","We use this to train models for semantic synthesis, super-resolution and inpainting . We train on input resolution of 2562 but find that our model generalizes to larger resolutions and can generate large images between 5122 and 10242 ."
112,"ldm-sr, degradation, regression, image","We use the f = 4 autoencoding model pretrained on OpenImages and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e. TO is the identity . A simple image regression model achieves the highest PSNR and SSIM scores ."
121,"fourier, inpainting, convolutions, approach, state-of-the-art, fast","Inpainting is the task of filling masked regions of an image with new content . We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task ."
122,"ldm-4, ldm-1, l","We first analyze the effect of different design choices for the first stage . For comparability, we fix the number of parameters for all models . Tab. 6 reports the training and sampling throughput at resolution 2562 and 5122 . Overall, we observe a speed-up of at least 2.7x between pixel- and"
123,"fid, lpips, quality, image",the comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of . LPIPS between the unmasked images and our samples is slightly higher .
128,"resolution, modules, inpainting, attention, additional, image",we noticed a discrepancy in the quality of samples produced at resolutions 2562 and 5122 . but fine-tuning the model for half an epoch allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting .
130,"ldm, pixel-based, approach","Limitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs ."
134,"deep, images, of, manipulation, manipulated, fakes, deliberate","enable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration . On the other hand, it becomes easier to create and disseminate manipulated data or spread misinformation and spam ."
136,"deep, modules, learning, gan-based, approach, likelihood-based, gan",deep learning modules tend to reproduce or exacerbate biases that are already present in the data . the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.
139,"models, methods, state-of-the-art, diffusion","We have presented latent diffusion models, a simple and efficient way to significantly improve the training and sampling efficiency of denoising diffusion models without degrading their quality . Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of"
143,"ieee, puter, com-, society, computer, scale, synthesis, cvpr, waveform, large, image","In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122-1131. IEEE Computer Society, 2017. 1 Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasser"
144,"analysis, transformers, image-based, generative, image","1, 3 Ming Ding, Zhuoyi Yang, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang . CoRR, abs/2105.13290, 20"
146,"quali, networks, generative, cvpr, quality, adversarial","In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 6, 7, 16, 22, 28, 37, 38 Phillip Isola, Jun- Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversari"
147,gad,"Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021. 1, 3, 4, 29 Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In 2nd"
151,"translation, communication, synthesis, network, image","Semantic image synthesis with spatially-adaptive normalization . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2019. 22 Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencode"
152,"deep, analysis, thermodynamics, generation, large-scale, recognition, conditional, image","CoRR, abs/2104.02600, 2021. 3 Axel Sauer, Kashyap Chitta, Jens M ller, and Andreas Geiger. Projected gans converge faster . CoRR . abs/2111.01007, 20"
154,"nips, vahdat, jan, arash, kautz","Wei Sun, Tianfu Wu, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky . Identity leakage in generative models ."
156,"deep, deep-learning, learning, vector-quantized, modeling, image","CoRR, abs/2104.10157, 2021. 3 Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop . CoRR"
168,"model, synthesis, class-conditional, text-to-image",We updated results on text-to-image synthesis on ImageNet in Sec. 4.3 . This also includes a new comparison to very recent competing methods on this task published on arXiv .
174,"ratio, elbo, snr(t), standard, distribution, normal","a common choice to parameterize p is to specify it in terms of the true posterior q but with the unknown xo replaced by an estimate x0 based on the current step Xt . To minimize the remaining terms, the first term of the ELBO depends only on the final signal-to-"
192,"lpips, metric, mechanism, upsampling","unconditional samples of size 2562 guide the convolutional synthesis of 5122 images and T is a 2x bicubic downsampling . Following this motivation, we also experiment with a perceptual similarity guiding and replace the L2 objective with the LPIPS metric."
198,"ratio, synthesis, model, to-noise, signal, sampling, convolutional, image","the signal-to-noise ratio induced by the variance of the latent space /o2) significantly affects the results for convolutional sampling . Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled ."
202,"openimages, model, synthesis, layout-to-image",In Fig. 16 we show additional samples of the model finetuned on COCO . We train a model on the COCO and one on the OpenImages dataset .
225,"ldm, space, pixel, model, diffusion",We extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number 1 of parameters to our LDM . The results of this comparison are shown in the last two rows of Tab. 11 .
229,"lsun-cows, l, ldm-sr, ldm-bsr","LDM-SR, trained only with a bicubicly downsampled conditioning as in , does not generalize well to images which do not follow this pre-processing . To obtain a superresolution model for a wide range of real-world images, we replace the bsr-de"
241,"text-to-im, layer-normalization","the conditioner is an unmasked transformer which processes a tokenized version of the input y and produces an output 5 := TO, where 5 E RMxdT More specifically, the transformer is implemented from N transformer blocks ."
253,"unet, layout-to-image, text-to-image, model",The layout-to-image model discretizes the spatial locations of bounding boxes . Class information is contained in c. See Tab. 17 for the hyperparameters of and Tab. 13 for those of the UNet .
260,"synthetic, image-inpainting, ldm-4, masks","For our experiments on image-inpainting in Sec. 4.5, we used the code of to generate synthetic masks . We use a fixed set of 2k validation and 30k testing samples . This follows the training and testing protocol in ."
264,"package, fid, pipelines, torch-fidelity, data, processing","We follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores shown in Tab. 1 and 10 based on 50k samples from our models . For calculating FID scores we use the torch-fidelity package ."
271,"compute, layout-to-image, fid, coco, scores, layout-to-images, challenge, dataset, models, segmentation","To obtain better comparability, we follow common practice and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split . For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set ."
273,"imagenet, torch-fidelity, low-resolution, anti-aliasing, image","On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing . For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11."
275,"metrics, 3k, quality, samples","For efficiency reasons we compute sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples . The results might vary from those shown in Tab. 1 and 10. We maximize the learning rates of the individual models such that they still train stably."
277,"resolution/masked, low, image, resolution/unmasked",In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image . For SuperResolution subjects were asked: 'Which of the two images is a better high quality version of the low resolution image in the middle?
281,"compute, generative, requirements, state-of-the-art, models","compute requirements during training and inference throughput with state-of-the-art generative models . Compute during training in V100-days, numbers of competing methods taken from unless stated differently;*: Throughput measured in samples/sec on a single NVIDIA A100;t: Numbers taken from"
282,"celeba-hq, imagenet, adm, imagen","In Tab 18 we provide a more detailed analysis on our used compute ressources . We compare our best performing models on the CelebA-HQ, FFHQ and LSUN datasets with the recent state of the art models . To assess sample quality, we additionally report FID scores on the reported datasets ."
286,"kl, variational, autoencoder, model, low-weighted, term, regularization",To obtain high-fidelity reconstructions we only use a very small regularization for both scenarios . We either weight the KL term by a factor  10-6 or choose a high codebook dimensionality 121 .
291,"imagenet, landscapes, model","We also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on 5122 images . For those models trained on comparably small datasets, we additionally show nearest neighbors in VGG feature space for samples from our models ."
