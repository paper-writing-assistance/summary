element_idx,keywords,summarized_text
4,"global, context, factorization, window, word, model, bilinear, regression, matrix, log-","a new global logbilinear regression model combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods . The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task ."
8,"word, vector, king, queen, queen-man","the analogy ""king is to queen as man is to woman"" should be encoded in the vector space by the vector equation king queen = man - woman . This evaluation scheme favors models that produce dimensions of meaning ."
9,"global, skip-gram, context, local, factorization, window, lsa, matrix","The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis and 2) local context window methods . Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on"
10,"global, space, word, vector, regression, log-bilinear","In this work, we argue that global log-bilinear regression models are appropriate for doing so . We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics ."
13,"word, low-rank, representations, low-dimensional, hal, approximations, lsa","Matrix Factorization Methods have roots stretching as far back as LSA . These methods use low-rank approximations to decompose large matrices that capture statistical information about a corpus . The particular type of information captured varies by application . In LSA, the matric"
14,"co-occurrence, matrix, similarity, hal","a main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure . The number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness "
15,"shallow, context, win-, window-based, local, dows, word, representations, methods, vector",Bengio et al. introduced a model that learns word vector representations as part of a neural network architecture for language modeling . Collobert and Weston decoupled the word vector training from the downstream training objectives .
17,"skip-gram, bag-of-words, repre-, word, sentations, cbow, continuous","the skip-gram and continuous bag-of-words models of Mikolov et al. propose a simple single-layer architecture based on the inner product between two word vectors . Mnih and Kavukcuoglu also proposed closely-related vector log-bilinear models, v"
21,"representation, word, glove, global, vectors","The global corpus statistics are the primary source of information available to all unsupervised methods for learning word representations . The question remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning ."
26,"phase, words, thermodynamic, co-occurrence, probabilities","The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, k . For words k related to ice but not steam, say k = solid, we expect the ratio Pik /Pjk will be large ."
31,"a, context, and, between, word, f, homomorphism","the distinction between a word and a context word is arbitrary . To do so consistently, we must exchange W  w but also X - XT ."
35,"log(xik), +, xik), co-occurrence, log(1, lsa, matrix",the logarithm diverges whenever its argument is zero . We will use the resulting model as a baseline in our experiments . A main drawback to this model is that it weighs all co-occurrences equally .
44,"learning, window-based, word, vector, methods","Unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus . Nevertheless, certain models remain somewhat opaque in this regard ."
50,"cross-entropy, eqn., skip-gram, model",this objective bears some formal resemblance to the weighted least squares objective of Eqn . it is possible to optimize the objective directly as opposed to the on-line training methods used in the skip-gram models .
51,"cross, entropy, error",cross entropy error is just one of many possible distance measures between probability distributions . It has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events . For the measure to be bounded it requires that the model distribution Q be properly normalized .
54,"skip-gram, xi, weighting, factor","Mikolov et al. observe that while weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal . With this in mind, we introduce a more general weighting function, which we"
57,"nonzero, eqn.8, weighting, size, elements, corpus, function","the computational complexity of the model depends on the number of nonzero elements in the matrix X . As this number is always less than the total number of entries of the matrix, the model scales no worse than O . For this reason it is important to determine whether a tighter bound can be placed on the numbers of"
69,"svd, vector, model","Table 2: Results on the word analogy task, given as percent accuracy . Underlined scores are best within groups of similarly-sized models . vLBL results are from ."
72,analogy,"The dataset contains 19,544 such questions, divided into a semantic subset and a syntactic subset . The semantic questions are typically analogies about verb tenses or forms of adjectives . To correctly answer the question, the model should uniquely identify the missing term ."
78,"of, discrete, comprehensive, organization, features, recognition, entity, set","The CoNLL-2003 English benchmark dataset for NER is annotated with four entity types: person, location, organization, and miscellaneous . We train model models on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 and ACE-2003 data, and 3) M"
82,"tokenizer, co-occurrence, crawl, common, counts","We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6 and then construct a matrix of cooccurrence counts X . In all cases we use a decreasing weighting function, SO that word pairs that are d words apart contribute 1/d to the"
83,"adagrad, xmax","For all our experiments, we set Xmax = 100, a = 3/4, and train the model using AdaGrad . We run 50 iterations for vectors smaller than 300 dimensions ."
84,"networks, word, neural, vector, network","The model generates two sets of word vectors, W and W . When X is symmetric, the two sets should perform equivalently . On the other hand, there is evidence that training multiple instances of the network can help reduce overfitting and noise and generally improve results ."
88,"state-of-the-art, models","We train the skip-gram and continuous bag-of-words models on the 6 billion token corpus with a vocabulary of the top 400,000 most frequent words . We used 10 negative samples, which we show in Section 4.6 to be a good choice for this corpus."
92,"word, model, analogy, glove, word2vec, task",The GloVe model performs significantly better than the other baselines . Our results using the word2vec tool are somewhat better than most of the previously published results .
94,"ppmi, schemes, weighting","weighting schemes like PPMI destroy the sparsity of X . With smaller vocabularies, these information-theoretic transformations do indeed work well on word similarity measures ."
98,"similarity, word, cosine",A similarity score is obtained from the word vectors by normalizing each feature across the vocabulary . We compute Spearman's rank correlation coefficient between this score and the human judgments . GloVe outperforms it while using a corpus less than half the size .
99,"cbow, ner, l-bfgs, task",Table 4 shows results on the NER task with the CRF-based model . The L-BFGS training terminates when no improvement has been achieved . Otherwise all configurations are identical to those used by Wang and Manning .
105,"context, vector, length, window","In Fig. 2, we show the results of experiments that vary vector length and context window . A context window that extends to the left and right of a target word will be called symmetric . In and, we examine the effect of varying the window size for small and asymmetric context windows . Performance is better"
107,"size, analysis, word, corpus","In Fig. 3, we show performance on the word analogy task for 300-dimensional vectors trained on different corpora . On the syntactic subtask, there is a monotonic increase in performance as the corpus size increases . The same trend is not true for the semantic subtask ."
112,"x, populating, size, total, run-time, corpus, machine","The total run-time is split between populating X and training the model . Given X, the time it takes to train the model depends on the vector size and the number of iterations ."
120,"negative, glove, sampling, method","In Fig. 4, we plot the overall performance on the analogy task as a function of training time . The two x-axes at the bottom indicate the number of training iterations for GloVe and negative samples for word2vec . Presumably this is because the negative sampling method does not approximate"
125,"global, count, word, analogy, data, regression, similarity, log-bilinear","Currently, prediction-based models garner substantial support . Baroni et al. argue that these models perform better across a range of tasks . We construct a model that utilizes this main benefit of count data ."
127,"darpa, deft, d","Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency under Air Force Research Laboratory contract no. FA865010-C-7020 and the Defense Advanced Research Projects Agency Deep Exploration and Filtering of Text Program . Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors"
