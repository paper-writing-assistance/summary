element_idx,keywords,summarized_text
6,"system, fixed-length, translation, neural, vector, phrase-based, state-of-the-art, machine",Neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance . The models proposed recently for neural machine translation belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector .
8,"neural, translation, machine","Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom . Unlike the traditional phrase-based translation system, neural machine translation attempts to build and train a single, large neural network ."
9,"sentence, source, encoder-decoder, encoder, source-specific",encoder neural network reads and encodes a source sentence into a fixed-length vector . a decoder then outputs the translation from the encoded vector. The whole encoder-decoder system is jointly trained to maximize the probability of a correct translation given the source sentence .
10,"sentence, encoder-decoder, source, input",Cho et al. showed that indeed the performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases . This may make it difficult for the neural network to cope with long sentences .
11,"translation, encoder-decoder, model","the encoder-decoder model learns to align and translate together . Each time the proposed model generates a word in a translation, it searches for a set of positions in the source sentence where the most relevant information is concentrated . The model then predicts the target word based on the context vectors associated"
16,"fixed-length, source, vector, input, sentence","the basic encoder-decoder does not attempt to encode a whole input sentence into a single fixed-length vector . Instead, it chooses a subset of these vectors adaptively while decoding the translation . This allows a model to cope better with long sentences ."
17,"and, learning, translation, align, to, english-to-french, jointly","the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder-decoder approach . The improvement is more apparent with longer sentences, but can be observed with sentences of any length ."
19,"translation, training, conditional, parallel, probability, corpus","translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i.e., arg maxy p. In neural machine translation, we fit a parameterized model to maximize the conditionality probability of sentence pairs using a parallel training corp"
20,"variable-length, translation, source, neural, rnn, sentence, machine","Recently, a number of papers have proposed the use of neural networks to directly learn this conditional distribution . For example, two recurrent neural networks were used by and to encode a variable-length source sentence into a fixed-length vector ."
21,"translation, neural, state-of-the-art, performance, units, lstm, machine",Sutskever et al. reported that the neural machine translation achieved close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task .
52,"energy, mechanism, eij, probability, source, sentence, attention","the annotation hj reflects the respect to the previous hidden state Si-1 in deciding the next state Si and generating Yi Intuitively . By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed"
54,"x, sequence, rnn, recognition, input, speech, bidirectional","The usual RNN reads an input sequence x in order starting from the first symbol X1 to the last one XTx . In the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words ."
56,"alignment, annotation, model, later","We obtain an annotation for each word x  by concatenating the forward hidden state h j . The annotation hj contains the summaries 0 , 0 of both the preceding words and the following words . Due to the tendency of RNNs to better represent recent inputs,"
61,"corpora, words, un, european-french, parallel","WMT '14 contains the following English-French parallel corpora: Europarl , news commentary , UN . We reduce the size of the combined corpus to have 348M words ."
72,"layer, hidden, rnn, network, units","7 The encoder of the RNNsearch consists of forward and backward neural networks each having 1000 hidden units . In both cases, we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word ."
78,"system, conventional, translation, bleu, score","In Table 1, we list the translation performances measured in BLEU score . The proposed RNNsearch outperforms the conventional RNNencdec . This is a significant achievement considering that Moses uses a separate monolingual corpus ."
84,"basic, context, fixed-length, deterioration, encoder-decoder, vector, performance","RNNsearch-30 even outperforms RNNencdec-50 as length of the sentences increases . In Fig. 2, we see that the performance drops as the length increase . On the other hand, both are more robust to the length of those sentences ."
88,"tokens, bleu, test, rnnsearch-50*, set","Table 1: BLEU scores of the trained models computed on the test set . The second and third columns show respectively the scores on all the sentences and, on the sentences without any unknown word in themselves and in the reference translations ."
91,"annotation, soft-alignment, source, weights, sentence",This is done by visualizing the annotation weights ij Eq. Each row indicates the weights from associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word.
92,"of, words, economic, area, alignment, european","We see strong weights along the diagonal of each matrix . However, we also observe a number of non-trivial, non-monotonic alignments . From this figure, we see that the model correctly translates a phrase into ."
93,"alignment, strength, soft-alignment, hard","Consider the source phrase which was translated into . Any hard alignment will map to and to . This is not helpful for translation, as one must consider the word following to determine whether it should be translated into or . Our soft-alignment solves this issue naturally by letting the model look at both and , and in"
115,"alignment, handwriting, synthesis","Graves proposed a similar approach of aligning an output symbol with an input symbol . In his work, he used a mixture of Gaussian kernels to compute the weights of the annotations ."
121,"translation, neural, statistical, network, conditional, probability, machine","Bengio et al. introduced a neural probabilistic language model . neural networks have been widely used in machine translation . However, the role of neural networks has been limited to simply providing a single feature to an existing statistical machine translation system ."
122,"target, and, phrases, target-side, source, model, neural, network, language","Schwenk proposed using a feedforward neural network to compute the score of a pair of source and target phrases . Traditionally, a neural network trained as a target-side language model has been used to rescore or rerank a list of candidate translations ."
123,"system, translation, neural, network, machine","the neural machine translation approach we consider in this paper is a radical departure from these earlier works . Rather than using a neural network as a part of the existing system, our model works on its own and generates a translation from a source sentence directly."
125,"translation, approach, neural, encoder-decoder, machine",the conventional approach to neural machine translation is called an encoder-decoder approach . We conjectured that the use of a fixed-length context vector is problematic for translating long sentences .
126,"alignment, soft-search, mechanism","In this paper, we proposed a novel architecture that addresses this issue . We extended the basic encoder-decoder by letting a model search for a set of input words . This frees the model from having to encode a whole source sentence into a fixed-length vector ."
127,"conventional, soft-alignment, translation, model, encoder-decoder, english-to-french","We tested the proposed model, called RNNsearch, on the task of English-to-French translation . The model outperforms the conventional encoder-decoder model significantly ."
128,"translation, statistical, phrase-based, architecture, machine",proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation . We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general .
133,"support, theano","NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR . Bahdanau thanks the support from Planet Intelligent Systems GmbH ."
172,"short-term, computation, memory, long, (lstm), unit, lstm",The gated hidden unit is similar to a long-term memory unit proposed earlier by Hochreiter and Schmidhuber . This is made possible by having computation paths in the unfolded RNN . These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .
215,"of, and, gaussian, 0, variance, distribution, 0.012, mean","For Wa and Ua, we initialized the recurrent weight matrices . All the elements of Va and bias vectors were initialized to zero . Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.0012 ."
