element_idx,keywords,summarized_text
3,"make-an-audio, generative, modeling, text-to-, video, multimodal","We propose Make-An-Audio with a prompt-enhanced diffusion model . This model addresses these gaps by introducing pseudo prompt enhancement . We present its controllability and generalization for X-to-audio in ""No Modality Left Behind"""
11,"language-audio, contrastive, text-to-audio, generation, model, data, pretraining, language-free, t2a, prompt-enhanced, diffusion","In this work, we propose Make-An-Audio with a promptenhanced diffusion model for text-to-audio generation . To alleviate data scarcity, we introduce a pseudo prompt enhancement approach to construct natural languages that align well with audio ."
15,"audio, quality, make-an-audio, text-to-audio",Make-An-Audio achieves new state-of-the-art in text-to-audio with natural and controllable synthesis . Both subjective and objective evaluations demonstrate that Make a good result . The benchmark AudioCaption dataset also generalizes well to the unsupervised Clotho dataset in
18,"language-free, audios, make-an-audio, distill-then-reprogram",Make-An-Audio is an effective method that leverages latent diffusion with a spectrogram autoencoder to model the long continuous waveforms . We introduce a pseudo prompt enhancement with the distill-then-reprogram approach .
21,"deep, generative, videos, video, models, image",pioneering work of DALL-E encodes images into discrete latent tokens using VQVAE and considers T2I generation as a sequence-to-sequence translation problem .
22,"diffusion, glide, large-scale, model",GLIDE trains a T2I upsampling model for a cascaded generation . Stable diffusion utilizes latent space diffusion instead of pixel space to improve computational efficiency . CogVideo is built on top of a CogView2 model with a multi-framerate hierarchical training
25,"audio, generation, text-guided, visual, diffusion",DiffSound is the first to explore text-to-audio generation with a discrete diffusion process that operates on audio codes obtained from a VQ-VAE . AudioLM introduces the discretized activations of a masked language model pre-trained on audio and generates syntactically plausible
26,"audio, pre-training, samples",AudioGen propose to generate audio samples autoregressively conditioned on text inputs . We introduce pseudo prompt enhancement and leverage the power of contrastive language-audio pre-training and diffusion models for highfidelity generation .
28,"chical, learning, self-supervised, hierar-, ssl, architecture",SoundStream presents hierarchical architecture for high-level representations that carry semantic information . Data2vec uses a fast convolutional decoder and explores the contextualized target representations in a self-supervised manner .
34,"self-supervision, autoencoder, audio, masked, spectrogram","Recently, spectrograms autoencoder with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer . Xu et al. study the Masked Auto Encoders to self-supervised representation learning from audio spectrons . Inspired by these"
38,"digital, text-to-audio, synthesis, generation, visual, t2a","Model training is faced with data scarcity, as human-labeled audios are expensive to create . Modeling long continuous waveforms poses a challenge for all high-quality neural synthesizers."
39,"audio, contrastive, clap, magnitude, pretraining, language-free, language-audio",Make-An-Audio consists of the following main components: 1) the pseudo prompt enhancement to alleviate the issue of data scarcity . 2) a spectrogram autoencoder for predicting self-supervised representation instead of long continuous waveforms; 3) a diffusion model that maps natural language to
41,"text-audio, reprogramming, scarcity, data, dynamics","to mitigate data scarcity, we propose to construct prompts aligned well with audios . This enables a better understanding of text-audio dynamics from unsupervised data . a dynamic reprogramming procedure to construct a variety of concept compositions."
43,"audio, pre-trained, captioning, automatic","audio-text retrieval takes a natural language as a query to retrieve relevant audio files in a database . To this end, experts distill knowledge to construct a caption aligned with audio . We select from these candidates that endow high CLAP score as the final caption ."
47,"appendix, dynamic, reprogramming, f, compositions, concept, time","Dynamic reprogramming technique constructs a variety of concept compositions . It proceeds in three steps as illustrated in Figure 3, where we elaborate the process ."
49,"contrastive, synthesis, text-guided, pretraining, language-audio","Text-guided synthesis models need powerful semantic text encoders to capture the meaning of arbitrary natural language inputs . Contrastive pretraining brings audio and text descriptions into a joint space . Language models are trained on text-only corpus significantly larger than paired multimodal data, thus being exposed to a"
53,"discriminator, audio, multi-window, mel-spectrogram, signal, dis, discrimin","Audio signal is a sequence of mel-spectrogram sample x E CaxT, where Ca, T respectively denote the mel channels and the number of frames . Our autoencoder is composed of 1) an encoder network E which takes samples x as input and outputs latent representations z"
57,"space, ddpm, training, latent, (ldms), loss, models, diffusion","We implement our method over Latent Diffusion Models . It is conditioned on textual representation, breaking the generation process into several conditional diffusion steps . The training loss is defined as the mean squared error in the noise E  N space."
60,"diversity, diffusion, training, model","the textual condition in a latent diffusion model EA is replaced by an empty prompt c with a fixed probability during training . During sampling, the output of the model is fixed probability ."
67,"audio, generation, model, conditional, dif-, x-to-audio, fusion","Make-An-Audio empowers humans to create rich and diverse audio content with unprecedented ease . For the first time, we contextualize the need for audio generation with different conditional modalities, including: 1) text, 2) audio ."
69,"learning, sound, background, adding, ma, machine","Adapting models to a specific individual or object is a long-standing goal in machine learning research . Our model produces realistic and faithful audio describing ""a baby cries in the thunder day"" Distinctly, it has a wide range of uses for audio mixing and tuning ."
70,"audio, editing, stochastic, generation, differential","We investigate the personalized text-to-audio generation by stochastic differential editing . Given input audio with a user guide, we select a particular time to with total denoising steps N, and add noise to the raw data ZO for ZT according to Equation 4 ."
73,"artifacts, inpainting, edge, sdedit",diffusion model inpainting can be performed by adding noise to initial audio and sampling with SDEdit . it may result in undesired edge artifacts since there could be an information loss during the sampling process .
74,"masking, frame-based, way, lama, strategy","During training, the way masks are generated greatly influences the final performance of the system . In addition, we investigate the frame-based masking strategy commonly adopted in speech liteature ."
79,"visual-to-audio, clip, image-to-audio, generation",We extend Make-An-Audio for visual-to-audio generation . We use contrastive language-image pretraining with CLIP-guided T2A model . It is natural to leverage image priors for videos to simplify the learning process .
83,"pairs, audio, audioset, audio-text, validation, set","AudioSet, BBC sound effects, Audiostock, AudioCaps-train, ESC-50, FSD50K, Free To Use Sounds, Sonniss Game Effects, WeSoundEffects, MACS, Epidemic Sound, UrbanSound8K, WavText5Ks, Li"
84,"audio, token, text","preprocessing on text and audio data: 1) convert the sampling rate of audios to 16kHz and pad short clips to 10-second long; 2) extract the spectrogram with the FFT size of 1024, hop size of 256 ."
86,"space, nvidia, v100, percep-, hifi-gan, waveform, gpu, tual",We train a continuous autoencoder to compress the perceptual space with downsampling to a 4-channel latent representation . The base learning rate is set to 0.005 and we scale it by the number of GPUs and the batch size following LDM .
88,"text-audio, audio, clap, score, alignment, quality",We evaluate models using objective and subjective metrics over audio quality and text-audio alignment faithfulness . CLAP score is adapted from the CLIP score to the audio domain .
89,"mechanical, metrics, subjective, opinion, turk, evaluation, score, amazon, mean",We use crowd-sourced human evaluation via Amazon Mechanical Turk . Raters are asked to rate MOS on a 20-100 Likert scale . We assess audio quality and text-audio alignment faithfulness .
92,"evaluation, objective, baseline, diffsound","In terms of audio qualty, Make-An-Audio achieves the highest perceptual quality in AudioCaption with FID of 4.61 and KL of 2.79 . For zero-shot generation, it also shows the outperformed results superior to the baseline model . On textaudio similarity"
101,"irregular, audio-inpainting, world, visual, strategy","During evaluation, we randomly mask the wide or narrow regions and use FID and KL metrics to measure performance . In both frame-based or irregular strategies, larger masked regions in training have witnessed improved perceptual quality, which force the network to exploit the high receptive field ."
107,"gaussian, personal, text-to-audio, generation, noise","On the personalized text-to-audio generation, we explore different to E to add Gaussian noise . We find that to E works well for faithful guidance with realistic generation ."
119,"clip, large, t5-",CLIP and T5Large achieve similar performances on benchmarks dataset . CLAP could be more computationally efficient without the need for offline computation .
122,"enhancement, approach, scarcity, distill-then-reprogram, data, prompt",our prompt enhancement approach alleviates data scarcity . The joint expert distillation produces high-quality captions aligned well with audio .
123,"ing, dynamic, clotho, reprogramming, static, strategy, train-, dataset",Removing the dynamic reprogramming approach results in a slight drop in evaluation . We also train our Make-An-Audio in the static training dataset .
125,"reprogram, distill-then-, make-an-audio, text-to-audio","In this work, we presented Make-An-Audio with a promptenhanced diffusion model for text-to-audio generation . We investigated textual representation and emphasized the advantages of contrastive pre-training for a deep understanding of natural languages with computational efficiency . Both objective and subjective evaluation demonstrated that"
195,"audio, natural, language, sounds","We collect a large-scale audio-text dataset consisting of 1M audio samples with a total duration of 3k hours . It contains audio of human activities, natural sounds, and audio effects . For audio with text descriptions, we discard the corresponding class label ."
196,"text-to-audio, audiocaption, model, pair, audio-text","AudioCaption validation set contains 494 samples with five human-annotated captions in each audio clip . In both training and inference, we randomly crop a 624 x 80 mel-spectrogram from 10-second 16 kHz audio ."
207,"mechanical, subjective, tests, turk, evaluation, amazon",Our subjective evaluation tests are crowd-sourced and conducted via Amazon Mechanical Turk . These ratings are obtained independently for model samples and reference audio . We paid $8 to participants hourly and totally spent about $750 on participant compensation .
228,"retrieval, learning, audio, representation, captioning, automatic, audio-text",the model consists of a 10-layer convolution neural network encoder and a single-layer gated unit decoder . The CNN encoder is pre-trained on a large-scale Audioset dataset .
230,"clip, t2a, multimodal, prediction","We use the CLIP-guided T2A model and leverage global textual representations to bridge the modality gap between visual and audio worlds . However, global CLIP conditions have a limited ability to control faithful synthesis with high text-audio similarity ."
231,"dataset, hinders, fsd50k, audiocaps, multimodal, distribution",We conduct ablation studies to compare various training settings . The results have been presented in Table 8 . Removing the normalization in the condition vector has witnessed the realism degradation measured by FID .
241,"digital, creation, text-to-audio, generation, open-domain, art, models",This paper aims to advance open-domain text-to-audio generation . The efficient training method also transfers knowledge from text to audio models . This helps avoid training from scratch and thus reduces data scarcity .
244,"high-quality, lightweight, synthesis, models, diffusion",Make-An-Audio adopts generative diffusion models for high-quality synthesis . Latent diffusion models require typically more computational resources .
