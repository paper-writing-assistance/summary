element_idx,keywords,summarized_text
3,"estimator, bound, gradient, variables, lower, stochastic, methods, latent, continuous","We introduce a stochastic variational inference and learning algorithm that scales to large datasets . First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized ."
5,"posterior, gradient, techniques, ascent, stochastic, approximate","the variational Bayesian approach involves the optimization of an approximation to the intractable posterior . The common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intracable in the general case ."
6,"purposes, vb, representation, model, recognition, auto-encoding, dataset","In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator . The learned approximate posterior inference model can also be used for recognition, denoising, representation and visualization purposes ."
8,"graphical, estimator, bound, objective, lower, stochastic, models, function","The strategy in this section can be used to derive a lower bound estimator for a variety of directed graphical models with continuous latent variables . We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variable per datapoint, and where we like to perform maximum"
15,"x, discrete, z(i), value, variable, dataset","We assume that the data are generated by some random process, involving an unobserved continuous random variable Z . The process consists of two steps: a value z is generated from some prior distribution po* ."
17,"em, carlo, algorithm, monte, functions",Intractabilities appear in cases of moderately complicated likelihood functions pe . We have SO much data that batch optimization is too costly .
19,"x, map, ml, computer, natural, vision, value, process, estimation","The parameters can be of interest themselves, e.g. if we are analyzing some natural process . They also allow us to mimic the hidden random process and generate artificial data that resembles the real data ."
21,"posterior, model, true, recognition, approximate","a recognition model 9: an approximation to the intractable true posterior pe . Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial . Instead, we'll introduce a method for learning the recognition model parameters 0 jointly with the generative"
22,"x, coding, model, theory, recognition, datapoint","In a similar vein we will refer to pe as a probabilistic encoder, since given a datapoint x it produces a distribution over the possible corresponding values of x ."
27,"estimator, bound, gradient, carlo, lower, monte, w.r.t.",We want to differentiate and optimize the lower bound L . w.r.t. both the variational parameters O and generative parameters 0. This gradient estimator exhibits very high variance and is impractical for our purposes.
39,"中, kl-divergence, (3), eq., regularizing",KL-divergence term can then be interpreted as regularizing  . The SGVB estimator LB ) 12 L) can be used to estimate posterior eq .
41,"minibatch, analysis, optimization, stochastic","the minibatch XM = xN=1 is a randomly drawn sample of M datapoints from the full dataset X . In our experiments we found that the number of samples L per datapoint can be set to 1 . Derivatives  , L can be taken, and"
42,"pe(x@iz(iz(i), log, auto-encoder, pe(x@iz(iz(i)), function",function 9 is chosen so that it maps a datapoint x and a random noise vector E to a sample from the approximate posterior for that datapoint: z = 9 x . This term is a negative reconstruction error in auto-encoder parlance .
44,"parameterization, 9⌀, able, vari-, random, function, 3d","In order to solve our problem we invoked an alternative method . Let Z be a continuous random variable, and Z  9 be some conditional distribution . It is then often possible to express the random variable Z as a deterministic variable Z = g ."
51,"gaussian, 9⌀, e, cdf, inverse, let, example, distribution","In this case, let E  U, let 9 be the inverse CDF of 9 . Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions ."
55,"approximate, bernoulli, gaussian, multivariate","We let the prior over the latent variables be the centered isotropic multivariate Gaussian pe = N . Note that in this case, the prior lacks parameters . We'll assume the true posterior takes on a Gausian form with an approximately diagonal covariance ."
57,"estimator, ⊙, gaussian","We sample from the posterior z  9 using z = 9 , E) = M +  e where E  N . In this model both pe and 1 are Gaussian; in this case, we can use the estimator of eq."
62,"likelihood, wake-sleep, algorithm, aevaluation, marginal",wake-sleep algorithm employs a recognition model that approximates the true posterior . Wake-Seep has the same computational complexity as AEVB per datapoint .
63,"variational, inference, control, variate, stochastic, scheme",Stochastic variational inference has recently received increasing interest . Recently introduced a control variate schemes to reduce the high variance of the naive gradient estimator discussed in section 2.1 .
64,"aevb, linear-gaussian, model",The AEVB algorithm exposes a connection between directed probabilistic models and auto-encoders . PCA corresponds to the maximum-likelihood solution of a special case of the linear-Gaussian model with a prior p = N .
65,"criterion, decoder-decoder, coding, boltzmann, machines, recognition, architecture, sparse",In relevant recent work on autoencoders it was shown that the training criterion corresponds to maximization of a lower bound of the mutual information between input X and latent representation Z . Maximizing the conditional entropy is lower bounded by the expected loglikelihood of
66,"variables, auto-encoding, stochastic, structure","The recently proposed DARN method learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables . Their work was developed independently of ours ."
76,"adagrad, aeval, wake-sleep, auto-encoder, algorithm","We compared performance of AEVB to the wake-sleep algorithm . All parameters, both variational and generative, were initialized by random sampling from N ."
77,"bound, lower, corresponding, encoders, recognition","Likelihood lower bound We trained generative models and corresponding encoders having 500 hidden units in case of MNIST . The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices ."
78,"estimator, likelihood, mnist, speed, marginal","Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator . For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables . Again, the MNIST dataset"
84,"gradient, vb, stochastic, methods, auto-encoding","We introduce a novel estimator of the variational lower bound, Stochastic Gradient VB . The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods ."
86,"aevb, of, dynamic, bayesian, sgvb, networks, time-series, application, models",the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables . the future directions are learning hierarchical generative architectures with deep neural networks .
96,"aevb, space, generative, model, latent","Figure 4: Visualisations of learned data manifold for generative models . Since the prior of the latent space is Gaussian, linearly spaced coordinates on the unit square were transformed through the inverse CDF . For each of these values Z, we plotted the corresponding generative pe with the"
100,"kl, variational, lower, bound",The variational lower bound contains a KL term that can often be integrated analytically . Let H be the dimensionality of Z. Let Mj and j simply denote the j-th element of these vectors .
106,"perceptrons, multi-layered, mlp, mlps","In variational auto-encoders, neural networks are used as probabilistic encoders . There are many possible choices of encoder . In our example we used relatively simple neural networks ."
120,"em, annealing, carlo, algorithm, schedule, stepsize, monte",Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize . For all algorithms the parameters were updated using Adagrad stepsizes .
138,"gaussian, covariance, variables, multivariate, diagonal","In this case, the prior lacks parameters . Let's assume that the true posteriors are approximatily Gaussian with an approximately diagonal covariance ."
