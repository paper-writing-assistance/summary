element_idx,summarized_text,keywords
5,"Transformer architecture has become the de-facto standard for natural language processing tasks . In vision, attention is either applied in conjunction with convolutional networks . a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.","image, cnn, vision, classification"
7,"Self-attention-based architectures, in particular Transformers , have become the model of choice in natural language processing . The dominant approach is to pre-train on a large text corpus and then fine-tune ona smaller task-specific dataset . Thanks to Transformers' computational efficiency and scalability,","natural, nlp, language, architectures, processing, self-attention-based"
8,"In computer vision, however, convolutional architectures remain dominant . Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention , some replacing the convolutions entirely .","cnn-like, convolutional, self-attention, architecture"
9,"Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images . To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer .","image, nlp, transformer, classification"
16,"Our Vision Transformer attains excellent results when pre-trained at sufficient scale and transferred to tasks with less datapoints . Best model reaches the accuracy of 88.55% on ImageNet, 90.72% on imageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the","large, imagenet, vision, vtab, picture, suite, changes, transformer, dataset"
18,Vaswani et al. proposed Transformers for machine translation . Large Transformer-based models are often pre-trained on large corpora . GPT line of work uses language modeling as its pre-training task .,"machine, large, bert, transformer, translation"
19,Paramar et al. applied the self-attention only in local neighborhoods for each query pixel instead of globally . Such local multi-head dot-product self attention blocks can completely replace convolutions .,"image, self-attention, approximation, processing"
20,Cordonnier et al. use a small patch size of 2 x 2 pixels . This makes the model applicable only to small-resolution images .,"al., image, et, (2020), vit, cordonnier, self-attention, small-resolution"
21,There has also been a lot of interest in combining convolutional neural networks with forms of self-attention . e.g. by augmenting feature maps for image classification or by further processing the output of a CNN .,"image, cnn, self-attention, classification"
23,Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset . The use of additional data sources allows to achieve state-of-the-art results on standard benchmarks .,"results, jft-300m, large, imagenet, scale, transfer, learning, cnn, state-of-the-art, dataset"
31,"Transformer receives as input a 1D sequence of token embeddings . To handle 2D images, we reshape the image x E RH x W xC into a sequence of flattened 2D patches Xp E RNx . The Transformer uses constant latent","token, transformer, embeddings, handle"
32,a classification head is attached to z2 during pre-training and fine-tuning . We prepend a learnable embedding to the sequence of embedded patches . The state at the output of the Transformer encoder serves as the image representation .,"image, encoder, bert, y, transformer, representation"
45,"Vision Transformer has much less image-specific inductive bias than CNNs . In ViT, only MLP layers are local and translationally equivariant . The two-dimensional neighborhood structure is used very sparingly .","locality, two-dimensional, structure, vision, neighborhood, transformer"
46,"In this hybrid model, the patch embedding projection E is applied to patches extracted from a CNN feature map . The input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension .","sequence, input, image, projection, e, raw, patches, cnn"
48,"We pre-train ViT on large datasets, and fine-tune to downstream tasks . For this, we remove the pre-trained prediction head and attach a zero-initialized D x K feedforward layer, where K is the number of downstream classes . The Vision Transformer can handle arbitrary sequence","vision, prediction, head, fine-tune, transformer"
50,"We evaluate the representation learning capabilities of ResNet, Vision Transformer , and the hybrid . To understand the data requirements of each model, we pre-train on datasets of varying size and evaluate many benchmark tasks .","capabilities, vision, learning, resnet, hybrid, transformer, representation"
52,We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following Kolesnikov et al. We transfer the models trained on these datasets to several benchmark tasks .,"ilsvrc-2012, imagenet"
58,"Model Variants are directly adopted from BERT . We add the larger ""Huge"" model . Note that the Transformer's sequence length is inversely proportional to the square of the patch size .","bert, size, model, variant"
59,"We use ResNet , but replace the Batch Normalization layers with Group Normalization . We denote the modified model ""ResNet"" For the hybrids, we feed the intermediate feature maps into ViT .","model, bit, vit, batch, normalization"
60,"We train all models, including ResNets, using Adam with B1 = 0.9, B2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1 . For fine-tuning we use SGD with momentum, batch size 512, for all models .","fine-tuning, resnets, appendix, table, b.1, 2"
61,"We report results on downstream datasets either through few-shot or fine-tuning accuracy . Few-shot accuracies are obtained by solving a regularized least-squares regression problem that maps the representation of a subset of training images to -1, 1K target vectors .","accuracy, performance, fine-tuning"
63,"We compare our largest models to state-of-the-art CNNs from the literature . The first comparison point is Big Transfer , which performs supervised transfer learning with large ResNets. The second is Noisy Student . We report the number of TPUv3-core-days taken to pre-","large, imagenet, noisy, state, student, resnet, art, of, the"
71,model still took substantially less compute to pre-train than prior state of the art . We provide a controlled study of performance VS. compute for different architectures in Section 4.4 .,"performance, choice, imagenet-21k, vit-l/16, architecture"
72,Figure 2 decomposes the VTAB tasks into their respective groups . ViT-H/14 outperforms BiT-R152x4 and other methods . On the Specialized the performance of the top two models is similar .,"v, vtab-r152x4, vtab, imagenet"
75,"ImageNet, ImageNet-21k, and JFT300M pre-train ViT models . To boost performance on smaller datasets, we optimize three basic regularization parameters . Figure 3 shows the results after finetuning to ImageNet 2.","vit-base, jft-300m, models, imagenet"
86,"We train our models on random subsets of 9M, 30M, and 90M as well as the full JFT300M dataset . This way, we assess the intrinsic model properties, and not the effect of regularization . To save compute, we report few-shot linear accuracy instead of full finetuning accuracy","computational, smaller, fine-tuning, transformers, vision, accuracy, cost, dataset"
91,"We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M . In this setting data size does not bottleneck the models' performances . We assess performance versus pre-training cost of each model . 6 Vision Transformers, ViT-B/32, B/16, L/","jft-300m, performance, transfer"
92,"Figure 5 contains the transfer performance versus total pre-training compute . A few patterns can be observed . First, Vision Transformers dominate ResNets on the performance/compute trade-off .","transformers, graphics, vision, apendix, d.5"
94,The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space . Figure 7 shows the top principal components of the learned embedding filters .,"low-dimensional, transformer, vision, space"
95,"Figure 7 shows that the model learns to encode distance within the image in the similarity of position embeddings . Further, the row-column structure appears; patches in the same row have similar embedderings. Finally, a sinusoidal structure is sometimes apparent for larger grids.","position, topology, image, handcrafted, embedding, 3d"
97,Self-attention allows ViT to integrate information across the entire Figure 6: Representative eximage even in the lowest layers . We compute output token to the input the average distance in image space across which information is space.,"depth, network, attention"
104,"We also perform a preliminary exploration on masked patch prediction for self-supervision . With self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet . Appendix B.1.2 contains further details .","bert, self-supervision, pre-training, contrastive"
106,"We have explored the direct application of Transformers to image recognition . We do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step . Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP .","image, transformer, vision, recognition"
107,Initial results show improvement from self-supervised pre-training . Further scaling of ViT would likely lead to improved performance . The results in Carion et al. are encouraging .,"self-supervised, segmentation, pre-training, vit, detection"
109,"the work was performed in Berlin, Zurich, and Amsterdam . We thank Google colleagues for their help . Andreas Steiner for crucial help with the infrastructure and opensource release of the code .","infrastructure, training, release, large-scale, open-source, google"
185,Table 3 summarizes our training setups for different models . We found strong regularization to be key when training models from scratch . Dropout is applied after every dense layer except for the qkv-projections .,"modeling, model, imagenet, training, setup"
187,We fine-tune all ViT models using SGD with a momentum of 0.9 . We run a small grid search over learning rates . For final results we train on the entire training set and evaluate on the respective test data .,"sweep, respective, imagenet, rate, learning, test"
194,"For VTAB we follow the protocol in Kolesnikov et al. , and use the same hyperparameter setting for all tasks . We use a learning rate of 0.01 and train for 2500 steps .","hyperparameter, vtab, setting"
196,To do SO we corrupt 50% of patch embeddings . This setup is very similar to the one used for language by Devlin et al.,"patch, self-supervision, experiment, embedding"
197,"We used Adam, with a base learning rate of2  10-4 warmup of 10k steps and cosine learning rate decay . All worked quite well, though L2 was slightly worse . We also experimented with 15% corruption rate as used by Devlin et al.","metrics, model, self-supervised, adam, few-shot"
217,Adam pre-trains ResNets on JFT with SGD and Adam . This justifies the choice of Adam as the optimizer .,"jft, resnets, adam"
219,"Figure 8 shows 5-shot performance on ImageNet for different configurations . All configurations are based on a ViT model with 8 layers, D = 1024, D MLP = 2048 and a patch size of 32 . We can see that scaling the depth results in the biggest improvements which are clearly visible up until","improvements, robust, imagenet"
232,"Considering the inputs as a bag of patches in the raster order . In this case, two sets of embeddings are learned, each for one of the axes . X-embedding, and Y -emedding, each with size D/2 . Then","softmax, information, attention"
239,"Table 8 summarizes the results from this ablation study on a ViT-B/16 model . We speculate that since our Transformer encoder operates on patch-level inputs, the differences in how to encode spatial information is less important .","position, spatial, vit-b/16, embedding, dimensions"
252,"Axial Attention is a simple, yet effective technique to run selfattention on large inputs that are organized as multidimensional tensors . In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent . We have implemented axial self-","tensor, axial, attention, multidimensional"
255,"Our implementation is based on the open-sourced PyTorch implementation in https :/ / github  com/ csrhddl am/ axial- deeplab . In our experiments, we reproduced the scores reported in terms of accuracy, however, our implementation, similar to","implementation, open-sourced, pytorch"
260,"each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column selfattention . naive implementation is extremely slow on TPUs .","axial-vit, model, self-attention, axial, transformer, block"
262,"ViT uses self-attention to integrate information across the image . Average attention distance is highly variable across heads in lower layers . As depth increases, attention distance increases for all heads .","distance, information, self-attention, attention"
271,1 2 3 4 5 6 7 8   9 10 11 12 13 14 15 16 9 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ,"11, Â·, 9"
