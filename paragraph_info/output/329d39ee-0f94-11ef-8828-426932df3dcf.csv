element_idx,keywords,summarized_text
3,"skip-gram, softmax, air, word, model, order, continuous",Continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships . In this paper we present several extensions that improve both the quality of the vectors and the training speed . By subsampling the frequent words we obtain significant
6,"nlp, representations, word, natural, language, processing","Distributed representations of words help learning algorithms to achieve better performance in natural language processing tasks . One of the earliest use of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams . The follow up work includes applications to automatic speech recognition and machine translation ."
7,"skip-gram, model, vector, network, architecture, quality","Mikolov et al. introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data ."
8,"linguistic, vector, calculation, regularities","word representations computed using neural networks are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns . For example, the result of a vector calculation is closer to vec than any other word vector ."
13,"skip-gram, softmax, contrastive, model, estimation, (nce), speedup, noise","In this paper we present several extensions of the original Skip-gram model . We show that subsampling of frequent words during training results in a significant speedup . In addition, we present a simplified variant of Noise Contrastive Estimation ."
14,"boston, globe, word, representations, recursive, autoencoders","Word representations are limited by their inability to represent idiomatic phrases . For example, ""Boston Globe"" is a newspaper . Also, using vectors to represent the whole phrases makes Skip-gram model considerably more expressive ."
15,"word, analogy, model, phrase, based","The extension from word based to phrase based models is relatively simple . To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases ."
16,"skip-gram, vector, addition, model","We found that simple vector addition can often produce meaningful results . For example, vec + VEc is close to vece . This compositionality suggests that a non-obvious degree of language understanding can be obtained by using basic mathematical operations on the word vector representations ."
26,"softmax, ch(n)",Let n be the j-th node on the path from the root to w . Let ch be an arbitrary fixed child of n and let be 1 if x is true and -1 otherwise .
27,"skip-gram, softmax, formulation","Log p and  log p are proportional to L, which on average is no greater than log W . The hierarchical softmax formulation has one representation Vw for each word w and one representation v'n for every inner node n of the binary tree."
28,"perfor-, tree, softmax, huffman, mance, training, hierarchical, time",Mnih and Hinton explored a number of methods for constructing the tree structure . In our work we use a binary Huffman tree as it assigns short codes to the frequent words which results in fast training .
30,"contrastive, estimation, noise",Noise Contrastive Estimation was introduced by Gutmann and Hyvarinen . NCE posits that a good model should be able to differentiate data from noise by means of logistic regression .
35,"softmax, distribution, nce, noise","the task is to distinguish the target word wo from draws from the noise distribution Pn using logistic regression, where there are k negative samples for each data sample . Our experiments indicate that values of k in the range 5-20 are useful for small training datasets, while for large datasets the k can be as small as 2"
38,"skip-gram, corpora, frequent, words, model, large","In very large corpora, the most frequent words can easily occur hundreds of millions of times . For example, while the Skip-gram model benefits from observing the co-occurrences of ""France"" and ""Paris"""
43,"for-mula, frequency, subsampling",We chose this subsampling formula because it aggressively subsamples words whose frequency is greater than t while preserving the ranking of the frequencies . It accelerates learning and even significantly improves the accuracy of the learned vectors of the rare words .
45,"no, softmax, contrastive, hierarchical, estimation, noise","In this section we evaluate the Hierarchical Softmax , Noise Contrastive Estimation, Negative Sampling, and subsampling of the training words . The task consists of analogies such as ""Germany"" : ""Berlin :: ""France"": ?,"
46,"skip-gram, representations, analogy, word, model, speed, dynamics",The performance of various Skip-gram models on the word analogy test set is reported in Table 1 . Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task . The subsampling of frequent words improves the training speed several times .
47,"skip-gram, networks, analogical, model, recurrent, reasoning, linear, sigmoidal",Mikolov et al. also showed that the vectors learned by the standard sigmoidal recurrent neural networks improve on this task significantly as the amount of training data increases .
49,"phrase, word, earlier","To learn vector representation for phrases, we first find words that appear frequently together . For example, ""New York Times"" and ""Toronto Maple Leafs"" are replaced by unique tokens in the training data ."
54,"skip-gram, data-driven, phrases, approach, model","This way, we can form many reasonable phrases without greatly increasing the size of the vocabulary . Many techniques have been previously developed to identify phrases in the text; however, it is out of scope of our work to compare them ."
55,"phrase, analysis, representations",The 8 is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed . The bigrams with score above the chosen threshold are then used as phrases . Table 2 shows examples of the five categories of analogies used in this task .
57,"skip-gram, hyper-parameters, model, phrase, dataset","We first constructed the phrase based training corpus and then trained several Skip-gram models using different hyperparameters . As before, we used vector dimensionality 300 and context size 5. This setting already achieves good performance on the phrase dataset ."
58,"softmax, chical, accuracy, subsampling, hierar",Negative Sampling achieves a respectable accuracy even with k = 5 . The Hierarchical Softmax became the best performing method when we downsampled frequent words . This shows that the subsampling can result in faster training .
67,"accuracy, analogy, data, training, phrase","To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words . We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context . This resulted in a model that reached an accuracy of 7"
70,"skip-gram, representations, word, model, linear, structure",We demonstrated that the Skip-gram representations exhibit a linear structure that makes it possible to perform precise analogical reasoning using simple vector arithmetics . This phenomenon is illustrated in Table 5.
71,"softmax, vector, nonlinearity, word",The word vectors are in a linear relationship with the inputs to the softmax nonlinearity . The vectors can be seen as representing the distribution of the context in which a word appears . These values are related logarithmically to the probabilities computed by the output layer .
73,"word, representations, analogy","authors who previously worked on neural network based representations of words have published their resulting models for further use and comparison . amongst the most well known authors are Collobert and Weston , Turian et al., and Mnih and Hinton ."
78,"of, skip-gram, model, learned, the, quality, vectors","The big Skip-gram model trained on a large corpus visibly outperforms all the other models in the quality of the learned representations . This can be attributed in part to the fact that this model has been trained on about 30 billion words, which is about two to three orders of magnitude more data ."
81,"of, frequent, words, negative, algorithm, the, learned, sampling, quality","We successfully trained models on several orders of magnitude more data than previously published models . This results in a great improvement in the quality of the learned word and phrase representations, especially for the rare entities ."
83,"operations, complexity, matrix-vector, computational, recursive",Combination of these two approaches gives a powerful yet simple way to represent longer pieces of text . Our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations .
88,"space, analogy, model, natural, language","In Proceedings of the 25th international conference on Machine learning, pages 160-167. ACM, 2008. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, 513-520, 2011. Michael U Gutmann and"
