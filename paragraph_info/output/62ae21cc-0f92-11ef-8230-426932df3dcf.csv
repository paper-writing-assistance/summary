element_idx,summarized_text,keywords
4,"Convolutional networks have been the paradigm of choice in many computer vision applications . The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information . In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions ","image, vision, self-attention, classification, computer"
10,"Self-attention has emerged as a recent advance to capture long range interactions . The key idea behind self-attention is to produce a weighted average of values computed from hidden units . As a result, the interaction between input signals depends on the signals themselves rather than being predetermined by their relative location like in con","generative, self-attention, values, modeling"
15,"In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions . We develop a novel two-dimensional relative selfattention mechanism that maintains translation equivariance while being infused with relative position information, making it well suited for images .","self-attentional, feature, convolution, self-attention, maps"
16,"Attention Augmentation achieves 1.3% top-1 accuracy ImageNet on top of a ResNet50 baseline and 1.4 mAP increase in COCO object detection . Suprisingly, experiments also reveal that fully self-attentional models only perform slightly worse than their fully convolutional counterparts on ImageNet .","squeeze-and-excitation, cifar-100, coco, detection, sque, object"
19,"Modern computer vision has been built on powerful image featurizers learned on image classification tasks such as CIFAR-10 and ImageNet . These datasets have been used as benchmarks for delineating better image featurizations and network architectures across a broad range of tasks . For example, improving the ""backbone","image, vision, network, architecture, cifar-10, classification, computer"
21,"Attention has enjoyed widespread adoption as a computational module for modeling sequences because of its ability to capture long distance interactions . Most notably, Bahdanau et al. first proposed to combine attention with a Recurrent Neural Network for alignment in Machine Translation . The self-attentional Transformer architecture achieved state-of","interactions, distance, self-attention, attention, long, alignment"
22,"former architectures alternate between self-attention layers and convolution layers . For example, Squeezeand-Excitation and Gather-Excite reweigh feature channels using signals aggregated from entire feature maps . BAM and CBAM refine convolutional features independently in the channel and spatial dimensions .","convolutional, mechanisms, self-attention, layer, architecture"
23,"Multi-head attention allows the model to attend jointly to both spatial and feature subspaces . Additionally, we extend relative self-attention to two dimensional inputs allowing us to model translation equivariance in a principled way . This property allows us to flexibly adjust the fraction of attentional channels and","attention, multi-head"
25,"Nh, dv and dk respectively refer the number of heads, depth of values and the depth of queries/keys per attention head .","head, attention, multihead-attention, map"
34,Multiple positional encodings have been proposed to alleviate related issues . Image Transformer extends the sinusoidal waves first introduced in the original Transformer to 2 dimensional inputs and CoordConv concatenates positional channels to an activation map.,"image, activation, augment, transformer, map"
35,"positional encodings do not satisfy translation equivariance . This is a desirable property when dealing with images . As a solution, we propose to extend the use of relative position encodes .","image, classification, translation, equivariance"
36,"Relative positional embeddings: Introduced in for the purpose of language modeling, relative self-attention augments self attention . We implement relative height information and relative width information .","position, modeling, relative, language, embeddings, information, width"
40,The relative attention algorithm in explicitly stores all relative embeddings rij in a tensor of shape . This compares to O2 Nh for the position-unaware version self-attention that does not use position encodings .,"relative, attention, algorithm"
43,"Multiple previously proposed attention mechanisms over images suggest that the convolution operator is limited by its locality and lack of understanding of global contexts . These methods capture long-range dependencies by recalibrating convolutional feature maps . In particular, Squeeze-and-Excitation and GatherExcite perform channelwise ","spatial, positions, mechanism, augmented, convolution, erator, attention, op-"
45,dv the ratio of attentional channels to We denote v = Fout dk the ratio . K= Fout key depth to number of original output filters .,"original, output, filters, attentional, channels"
46,"Multihead attention introduces a 1x1 convolution with Fin input filters and = Fout output filters to compute queries, keys and values . Considering the decrease in filters in the convolutional part, this leads to the following change in parameters:","input, output, multihead, attention, filters, fin"
47,"In practice, this causes a slight decrease in parameters when replacing 3x3 convolutions . In some experiments, attention augmented networks still outperform their fully convolutional counterparts while using less parameters .","position, relative, convolution, simplicity, embeddings"
48,"Attention Augmented Convolutional Architectures: In all our experiments, the augmented convolution is followed by a batch normalization layer which can learn to scale the contribution of the convolution feature maps and the attention feature maps .","feature, augmented, convolution, attention, maps"
49,"Since the memory cost O2) can be prohibitive for large spatial dimensions, we augment convolutions with attention starting from the last layer until we hit memory constraints . To reduce the memory footprint of augmented networks, we typically resort to a smaller batch size and sometimes additionally downsample the inputs to self-attention in the","memory, augmented, self-attention, cost, networks"
52,"Attention Augmentation leads to systematic improvements on image classification and object detection tasks across a broad array of architectures and computational demands. In all experiments, we substitute convolutional feature maps with self-attention feature maps as it makes for an easier comparison against the baseline models .","augmentation, two-dimensional, relative, mechanism, vision, attention, computer"
54,We augment the Wide-ResNet-28-10 by augmenting the first convolution of all residual blocks with relative attention using Nh=8 heads and k=2v=0.2 . Table 1 shows that Attention Augmentation improves performance both over the baseline network and Squeeze-and-Excitation at a,"cifar-100, attention, residual, imagery, resolution, block"
59,"ResNet-50 and its larger counterparts use a bottleneck block comprising of 1x1, 3x3, 1x1 convolutions . The first one contracts the number of filters . All attention augmented networks use k=2v=0.2, except for ResNet-34 .","augmentation, widespread, attention, residual, block"
68,"Table 3 compares the non-augmented networks and Squeeze-and-Excitation across different network scales . In all experiments, Attention Augmentation significantly increases performance . This results suggest that attention augmentation is preferable to simply making networks deeper .","scales, network, augmentation, attention"
70,"In this section, we inspect the use of Attention Augmentation in a resource constrained setting . The MnasNet was found by neural architecture search using only the highly optimized mobile inverted bottleneck block and the Squeeze-andExcitation operation as the primitives in its search space .","mobile, imagenet, bottleneck, inverted, mnasnet"
77,Attention Augmentation yields a 1.4% improvement over a strong RetinaNet baseline from ResNet-50 and ResNet101 . We hypothesize that localization requires precise spatial information which SE discards during the spatial pooling operation . Self-attention on the other hand maintains spatial information and is likely to,"image, self-attention, classification"
83,"In this section, we investigate the performance of Attention Augmentation as a function of the fraction of attentional channels . As we increase this fraction to 100%, we begin to replace a ConvNet with a fully attentional model, only leaving pointwise convolutions and the stem unchanged . Table 6 presents the performance","performance, fully-attentional, vision, models, attentional, channels"
84,"AA-ResNet-50 with K=v=1 is only 2.5% worse in accuracy than its fully convolutional counterpart . Notably, this fully attentional architecture outperforms ResNet-34 while being more parameter and flops efficient .","model, image, mechanism, self-attention, attentional, classification"
90,"In Figure 4, we show the effect of our proposed two-dimensional relative position encodings as a function of the fraction of attentional channels . In particular, the fully self-attentional ResNet-50 gains 2.8% top-1 ImageNet accuracy .","position, encodings, relative, attentional, channels"
96,Attention Augmentation is applied using the same hyperparameters as 4.2 with the following different position encoding schemes . We apply the same position-unaware version of self-attention . CoordConv for which we concatenate coordinate channels to the inputs of the attention function .,"function, self-attention, attention, self-atten"
97,"In Table 7 and 8, we present the results on ImageNet classification and the COCO object detection task respectively . Attention Augmentation without position encodings already yields improvements over the fully convolutional non-augmented variants .","coordinate, variants, augmentation, convolution, attention"
99,"In this work, we consider the use of self-attention for vision models as an alternative to convolutions . We introduce a novel two-dimensional relative self attention mechanism for images that enables training of competitive fully selfattentional vision models on image classification .","image, self-attention, convolution, classification"
101,"In future work, we will focus on the fully attentional regime and explore how different attention mechanisms trade off computational efficiency versus representational power . For example, identifying a local attention mechanism may result in an efficient and scalable computational mechanism that could prevent the need for downsampling with average pooling .","tasks, visual, mechanism, regime, fully, attention, attentional"
105,"Irwan Bello, Barret Zoph, Pieter-Jan Kindermans, Prajit Ramachandran, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate . In International Conference on Learning Representations, 2015.","irwan, bello"
107,"A2 -nets: Double attention networks. CoRR, abs/1810.11579, 2018. 3 Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Imagenet: A large-scale hierarchical image","image-, image, vision, recognition, and, pattern, network, transformer, computer"
108,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks . Proceedings of the IEEE, 1998. 1 Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bhara","visual, vision, recognition, coordconv, computer"
109,"QAnet: Combining local convolution with global selfattention for reading comprehension . In International Conference on Learning Representations Workshop Track, 2016. 2 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent","inception-resnet, inception-res, attention, inception-v4"
110,"Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Raz","conference, reinforcement, learning, on, representations, works, ieee, net-"
114,"CIFAR-100 We train all networks for 500 epochs using synchronous SGD with momentum 0.9 distributed across 8 TESLA V100 GPUs . The learning rate is linearly scaled from 0 to 0.2B /256, where B is the total batch size, for the first 5%","cifar, preprocessing, smaller, architecture"
115,"ImageNet classification with ResNet We train all ResNet architectures for 100 epochs using synchronous SGD with momentum 0.9 across 8 TESLA V100 GPUs . We use the largest batch size per worker B E 32, 64, 128, 256 that fits in a minibatch","resnet, architecture, classification, imagenet"
129,our implementation relies on non-trivial operations because no low-level kernels exist for hardware platforms . Current latency times reflect the lack of dedicated engineering as opposed to inefficiency in the proposed method.,"softmax, matrix, multiplication, addition, operation"
