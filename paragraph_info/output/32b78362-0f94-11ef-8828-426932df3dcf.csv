element_idx,keywords,summarized_text
4,"math, diverse, gsm8k, school, word, grade, state-of-the-art, language, models, problems","GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems . We find that even the largest transformer models fail to achieve high test performance . At test time, we generate many candidate solutions and select the one ranked highest by the verifier ."
6,"multi-step, reasoning, large, language, mathematical, models","Kaplan et al. describe the consistent benefits of increasing model size, characterizing scaling trends that hold across many orders of magnitude . However, even the largest models falter when required to perform multi-step mathematical reasoning ."
11,"a, beth, in, bakes, week, cookies, 4","Beth bakes 4 2 dozen batches of cookies for a total of 4*2 = 4*2=8>>8 dozen cookies . If these cookies are shared equally, how many cookies does each person consume? Beth divides the 96 cookies equally amongst 16 people so they each eat 96/16 ="
12,"cows, revenue, crown, milk","Yesterday morning, Mrs. Lim got 68 gallons of milk and in the evening, she got 82gallons fewer than she had yesterday morning . So she was able to get a total of 68gallons + 82 Gallons + 50gallons = 68+82+50=200>"
13,"left, tina, party, soda","Tina buys 3 12-packs of soda for a party . Half of people at the party have 3 sodas each, 2 of the people have 4, and 1 person has 5 . How many sodas are left over when the party is over?"
17,"school, math, grade, gsm8k",GSM8K is a dataset of 8.5K high quality problems at grade school math level . State-of-the-art language models struggle to achieve high performance on this dataset .
19,"nat-ural, dropout, solutions, language, models, acts","we present a dataset of 8.5K grade school math questions and natural language solutions . We show that dropout acts as a strong regularizer, significantly improving performance ."
22,"math, gsm8k, school, grade, high, quality, problems",GSM8K consists of 8.5K high quality grade school math problems created by human problem writers . These problems take between 2 and 8 steps to solve . A bright middle school student should be able to solve every problem .
24,"language, natural, state-of-the-, model","High Quality We strive for high diversity among problems . We actively avoid designing problems that are drawn from the same linguistic template . By creating each individual problem to be relatively unique, held-out test performance becomes a far more relevant metric ."
30,"mathqa, mathq, aqua-rat","AQuA-RAT contains 100K problems, but this dataset suffers from both a high degree of problem templatization and poor quality control of the natural language solutions . Ape210K is the largest publicly available dataset, consisting of 210K Chinese elementary school-level math problems ."
31,"gsm8k, average, asdiv, natural, state-of-the-art, language, dataset, models","ASDiv dataset , which contains 2.3K math word problems, addresses common flaws in prior datasets . The MATH dataset is larger and significantly more complex than GSM8K, but the high difficulty makes it challenging to accurately measure progress ."
32,"symbolic, math, senseqa, reasoning, common, logical","Similar to CommonsenseQA, GSM8K includes questions that require basic background knowledge, like the number of days in a week . The main difficulty lies in properly interpreting a question and reasoning through the steps to solve it ."
34,"problem, math, bert, classic, word, benchmarks, seq2seq, family, models",Previous work has attempted to solve classic math word problem benchmarks with recurrent seq2seq models and closely related variants . More recent work has improved performance by designing specialized encoder-decoder architectures with the strongest results often relying on large pretrained encoders from the BERT family 
35,"transformer-based, amps, pretraining, pmps, corpus, models",Hendrycks et al. propose pretraining models on a new AMPS corpus . Shen and Peng propose pre-K to college level curricula extracted from the internet .
40,"completions, analysis, verbal, model, natural, language","Nichols et al. proposed a sampleand-rank approach to improve the collaborative storytelling ability of large language models, with the training signal coming from the preferences of human workers . We focus attention on the space of natural language solutions, as this is a richer and more general solution format than pure mathematical expressions "
42,"gpt-3, verification, finetuning","Finetuning, our baseline method, uses the same language modeling objective as the generative pretraining in GPT-3 . At test time, we judge performance by autoregressively sampling a single low temperature solution . In contrast, verification consists of sampling multiple high temperature solutions ."
47,"appendix, c, accurately","To mitigate this issue, we train all models to use a calculator by injecting calculation annotations into the training set . Details can be found in Appendix C ."
49,"smaller, models, finetuning","Figure 2 shows test performance after finetuning on training sets of varying sizes for 20 epochs . Test performance is determined by a single low temperature sample for each test problem . Unsurprisingly, the 175B model significantly outperforms the smaller models ."
54,"test@n, test@100","Test@1 performance improves approximately monotonically, even though we quickly begin overfitting on test loss . Test@100 performance degrades much more sharply than test@1 as we increase the number of epochs ."
55,"appendix, d, natural, verifiers, training, language","test@100 performance peaks within the first few epochs . For this reason, we use models trained for 2 ePochs to generate samples . We provide several examples from 6B and 175B models in Appendix D."
57,"baseline, solutions, model-generated, finetuning","training solutions are labeled as correct or incorrect based solely on whether they reach the correct final answer . In practice, some solutions will achieve the correct answer using flawed reasoning, leading to false positives."
62,"auxiliary, objective, generator, modeling, verifier","Training for 2 epochs is enough for the generator to learn basic skills in this domain . We choose not to train for longer, since the diversity of generated solutions begins to collapse after this point . In principle, it should be possible to combine these models ."
63,"answer, correct, verification, finetuning","At test time, we sample 100 completions to each test problem, rank them with the verifier, and then return the one with the highest verifier score . We find that it is not beneficial to use verification at low dataset sizes ."
70,"value, token-level, function","We can either train verifiers to make a single scalar prediction conditioned on the entire generated solution . By default, we choose the latter . This can be viewed as a token-level value function ."
71,"full, token-level, value, verifier, function","token-level verifier is still improving late in training, whereas solution level verifier quickly shows signs of overfitting . we hypothesize that the full value function provides a useful auxiliary signal that encourages the model to judge the reasoning throughout solutions, rather than memorizing the correct final answer ."
80,"generator, verification, model","In Figure 6c, we ablate the model size of the generator and the verifier . Verification is still remarkably effective, even when the Verifier is much smaller than the generator . This suggests that the verification may be relying on relatively coarse heuristics to discriminate between solutions from a given"
83,"compute, 6b, verifier, verification, cost","Figure 7a shows how 6B verifier performance varies with the number of completions per test problem . Beyond this point, performance start to decrease . This suggests the benefits of search are eventually outweighed by the risk of finding adversarial solutions that fool the verifier ."
88,"final, process, answer, voting","Figure 7b shows how performance varies as we allow a greater number of top samples to cast a vote . When we have only 100 samples, it is optimal to allow only the top 3-5 samples ."
90,"gpt-3, dropout, residual, regularizer","We use 20% dropout for all dropout experiments, chosen based on the results of a hyperparameters sweep . For experiments involving dropout, we perform additional pretraining with dropout before finetuning the models ."
91,"dropout, token-level, verifiers, variants","Figure 8a shows that dropout leads to a significant improvement over baseline . We next investigate the effect of dropout on verifiers, considering both the solution-level and token-level variants . Notably, using dropout with solutionlevel Verifiers reaches a similar level of performance ."
94,"gsm8k, finetuning, finetuned, token-level, model, baseline, performance, verification, boost","6B verification slightly outperforms a finetuned 175B model, thereby offering a boost approximately equivalent to a 30x model size increase . We expect verification to scale well to problem distributions that require more complex mathematical reasoning, and hope GSM8K supports the development of new methods that scale even"
96,"team, openai, supercomputing","We thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Parascandolo for their valuable feedback on this paper . The OpenAI Supercomputing team for the infrastructure that made these experiments possible; and the team at Surge AI for performing the"
98,"problem, math, analysis, word, reasoning, mathematical","K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. D. Forbus, and J. Gao. Mapping natural-language problems to formal-language solutions using structured neural representations ."
100,"formula, word, model, emnlp, mathematical, problems","D. Huang, J. Liu, C.-Y. Lin, and J. Y. Ma. How well do computers solve math word problems? Large-scale dataset construction and evaluation . In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 887-896, 2016."
102,"math, analysis, information, processing, word, neural, systems, mathematics, problems","In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics. doi: 10.18653/v1 /D15-1202. J. Shen, Y. Yin, L. Li"
105,"guage, lan-, upwork, natural, contractors, solutions, freelance","We initially collected a starting set of a thousand problems by hiring freelance contractors on Upwork . After collecting the full dataset, we asked workers to re-solve all problems . We then performed another round of agreement checks on a smaller subset of problems, finding 1.7% of problems still produce disagreements among contractors "
106,"problem, templates, questions, contractors, seed","to assist contractors with writing questions, we provided seed questions automatically generated from a few-shot prompted 175B GPT-3 model . Contractors were allowed to use those seed questions directly, to use them as inspiration and make modifications, or to come up with their own questions entirely ."
109,"hyperparameters, of, table, below",We performed sweeps of the learning rate and batch size by an order of magnitude . Other reasonable choices for both the verifier temperature and objective also had negligible effect in our ablations .
141,"scents, company, fruity, perfume","A perfume company is trying to create new scents . They already have 4 vanilla scents and 8 fruity scents available . By the end of the day, they sell 5 of each of the vanilla fragrances and 2 of each ."
204,"science, math, and, weight, cindyâ€™s, book, history","Cindy's math and science books weigh 2 pounds each for a total of 2*2 = 2*2=4>>4 pounds Her French book weighs 4 pounds and her English book is 3 pounds . If you add up all of the books weights, the total weight is 4+7+6 = "
246,"token, modeling, special, logit, language",We implement this scalar head as a single bias parameter and single gain parameter . The bias and gain shift and scale the logit corresponding to a special token in the vocabulary .
247,"generator, model, verifier, language, distribution",We can choose to initialize the verifier from the same pretrained language model the generator was finetuned from . In our ablations the latter performed slightly better .
248,"objective, data, verifier, joint","Using an equal mix of language data and verifier data . To form the joint objective, we simply add the verifier loss and language modeling loss unweighted . With both objectives, we only train on tokens in the solutions ."
255,"questions, token-level, cherry-picked, verifier",token-level verifiers can visualize the predicted value for each token and better understand how the verifier makes decisions on judging samples . Above we present a visualization of the predicted values for five different cherry-picked questions and model completions .
