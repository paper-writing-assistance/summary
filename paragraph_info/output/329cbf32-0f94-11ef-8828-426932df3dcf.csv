element_idx,keywords,summarized_text
7,"answer, answer-eviced",TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts . We present two baseline algorithms: a featurebased classifier and a state-of-the-art neural network that performs well on SQuAD reading comprehension .
9,"correct, text, answer, reference, rc, comprehension",Reading comprehension systems aim to answer any question that could be posed against the facts in some reference text . This goal is challenging for a number of reasons: the questions can be complex .
13,"ii, campaign, war, dodecanese, islands, world",The Dodecanese Campaign of World War II inspired the 1957 novel The Guns of Navarone and the successful 1961 movie of the same name . The failed campaign was an attempt by Allied forces to capture the Italianheld Dodekanesese islands in the Aegean Sea .
19,"lenges, chal-, pairs, answer, question","Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above . In general, system performance has improved rapidly as each resource is released ."
24,"triples, question-answer-, evidence","TriviaQA is the first dataset where full-sentence questions are authored organically and evidence documents are collected retrospectively from Wikipedia and the Web . This decoupling of question generation from evidence collection allows us to control for potential bias in question style or content, while offering organically generated questions from various topics ."
27,"triples, question-answer-evidence, question-document-answer",The dataset and code are available at http :/ / nlp  CS  washington. edu . We present a manual analysis quantifying the quality of the dataset and the challenges involved in solving the task .
31,"documents, supervision, evidence, distant","Data and Distant Supervision Our evidence documents are automatically gathered from either Wikipedia or more general Web search results . This assumption is valid over 75% of the time, making evidence documents a strong source of distant supervision ."
32,"supervision, question-, distant, answer-document","For web search results, we expect the documents that contain the correct answer a to be highly redundant . In Wikipedia we generally expect most facts to be stated only once, SO we pool all of the evidence documents and never repeat the same question in the dataset ."
41,"search, web, answer, questions, bing","We posed each question4 as a search query to the Bing Web search API . To exclude trivia websites, we removed all pages from the trivia websites we scraped . We then pruned PDF and other ill formatted documents ."
49,"evidence, combined, open, domain, documents","ing a single evidence document, and 78K examples for the Wikipedia reading comprehension domain, containing on average 1.8 evidence documents per example . Table 2 contains the dataset statistics."
55,"organization, answer, organiz, pairs",Sis sampled 200 question answer pairs and manually analysed their properties . About 73.5% of these questions contain phrases that describe a fine grained category to which the answer belongs . 15.5% hint at a coarse category .
56,"synsets, in, triviaqa, wordnet, answers","Answers in TriviaQA belong to a diverse set of types . 92.85% of the answers are titles in Wikipedia,5 4.17% are numerical expressions while the rest are open ended noun and verb phrases ."
61,"young, city, brigham, questions, salt, lake, quality","The annotator was asked to answer a question if the minimal set of facts required to answer the question are present in the document, and abstain otherwise . For example, it is possible to answer question, Who became president of the Mormons in 1844, organised settlement of the Baptists in Utah 1847 and founded"
62,"question-evidence, pairs","To compare our dataset against previous datasets, we classified 100 question-evidence pairs each from Wikipedia and the Web according to the form of reasoning required to answer them . categories are not mutually exclusive: single example can fall into multiple categories."
64,"triviaqa, question-evidence, pairs",17% of the examples required some form of world knowledge . Question-evidence pairs in TriviaQA display more lexical and syntactic variance than SQuAD .
68,"candidate, answer, random, question, entity, set","In this approach, we first construct a candidate answer set using the entities associated with the provided Wikipedia pages for a given question . If no such candidate exists, we pick any random candidate from the candidate set ."
76,"words, mechanism, model, bidaf, rnn, question","Recurrent neural network models have been very effective for reading comprehension . The BiDAF model uses an RNN at the character level, token level, and phrase level to encode context and question ."
77,"string, answer, evidence, document, long, model, bidaf","TriviaQA does not contain the exact spans of the answers . To overcome this, we truncate the evidence document to the first 800 words ."
88,"clean, dev","Wikipedia and the web are vastly different in terms of style and content . We evaluate at the question level since facts needed to answer a question are generally stated only once . On the other hand, we report document level accuracy and F1 when evaluating on web documents ."
90,"bidaf, test, set, development","In addition to distant supervision evaluation, we also evaluate baselines on verified subsets of the dev and test partitions . For training BiDAF on the web domain, we first randomly sampled 80,000 documents ."
92,"bidaf, table, 7","The poor performance of the random entity baseline shows that the task is not already solved by information retrieval . For both Wikipedia and web documents, BiDAF outperforms the classifier ."
95,"qualitative, analysis, wars, lexical, variation, bidaf, napoleonic, error","We randomly sampled 100 incorrect BiDAF predictions from the development set . 19 examples lacked evidence in any of the provided documents, 3 had incorrect ground truth, and 12 predictions were partially correct ."
107,"cloze-style, questions, natural, language, dataset",Reading comprehension tasks aims to test the ability of a system to understand a document using questions based upon its contents . Cloze-style datasets do not contain natural language questions .
108,"mctest, triviaqa, questions, natural, language, newsqa",SQuAD contains 100K crowdsourced questions and answers paired with short Wikipedia passages . NewsQA uses crowdsourcing to create questions solely from news article summaries .
113,"answer, phases, selection","where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset for answer sentence selection, and the Chinese language WebQA dataset . TriviaQA contains examples that could be used for both stages of the pipeline ."
114,"openie, tri, triples","TriviaQA, which has Wikipedia entities as answers, makes it possible to leverage structured KBs like Freebase . About 7% of the questions have answers in HTML tables and lists ."
115,"questions, answering",Trivia questions from quiz bowl have been previously used in other question answering tasks . Quiz bowl questions are paragraph length and pyramidal .
117,"triples, question-document-evidence",TriviaQA is the first dataset where questions are authored by trivia enthusiasts . The evidence documents come from two domains - Web search results and Wikipedia pages - with highly different levels of information redundancy .
124,"semantic, parsing, emnlp, free-base","Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs ."
126,"computational, games, classification","Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daume III. 2012. Besting the quiz master: Crowdsourcing incremental classification games."
145,"german, kruszewski","Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda ."
160,"chical, networks, act-, actweb, hierar-, network, attention","Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy."
