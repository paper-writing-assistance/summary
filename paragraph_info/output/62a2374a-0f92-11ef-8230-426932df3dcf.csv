element_idx,summarized_text,keywords
3,"We present a residual learning framework to ease training of networks that are substantially deeper than those used previously . We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions . An ensemble of these residual nets achieves 3.57% error on the ImageNet test set","2015, inputs, task, imagenet, layer, residual, ilsvrc, networks, classification"
4,We obtain a 28% relative improvement on the COCO object detection dataset . Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1 .,"visual, recognition, representations, deep, ilsvrc"
6,Deep convolutional neural networks have led to a series of breakthroughs for image classification . Deep networks naturally integrate low/mid/highlevel features and classifiers in an end-to-end multilayer fashion .,"image, depth, breakthroughs, network, classification"
11,vanishing/exploding gradients hamper convergence from the beginning . This problem has been addressed by normalized initialization and intermediate normalization layers . Normalization layers enable networks with tens of layers to start converging .,"better, depth, stacking, descent, networks, gradient"
12,"Unexpectedly, such degradation is not caused by overfitting . adding more layers to a suitably deep model leads to higher training error .","problem, deeper, error, train-ing, degradation, networks"
13,"degradation indicates that not all systems are similarly easy to optimize . There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model .","shallower, training, accuracy, architecture, degradation, of"
19,"In this paper, we address the degradation problem by introducing a deep residual learning framework . Formally, we let the stacked nonlinear layers fit another mapping of F := H - X. The original mapping is recast into F +x .","problem, framework, learning, residual, degradation"
20,Shortcut connections are those skipping one or more layers . Identity shortcut connections add neither extra parameter nor computational complexity . The entire network can still be trained end-to-end by SGD with backpropagation .,"+, identity, f(x), connections, shortcut, x"
21,"We present comprehensive experiments on ImageNet to show the degradation problem . Our extremely deep residual nets are easy to optimize, but the counterpart ""plain"" nets exhibit higher training error when the depth increases.","imagenet, problem, degradation"
24,ImageNet test set won the 1st place in the ILSVRC 2015 classification competition . The highly deep representations also have excellent generalization performance on other recognition tasks . This strong evidence shows that the residual learning principle is generic .,"2015, learning, competition, principle, residual, ilsvrc"
26,Fisher Vector can be formulated as a probabilistic version of VLAD . Both are powerful shallow representations for image retrieval and classification .,"image, vector, recognition, representations, residual, fisher"
27,"In low-level vision and computer graphics, the widely used Multigrid method reformulates the system as subproblems at multiple scales . Each subproblem is responsible for the residual solution between a coarser and a finer scale . It has been shown that these solvers converge much faster than standard solve","solution, graphics, multigrid, residual, computer"
28,Shortcut Connections has been studied for a long time . An early practice of training multi-layer perceptrons is to add a linear layer connected from the network input to the output .,"branch, mlps, shortcut, connections, perceptrons"
29,"""highway networks"" are data-dependent and have parameters . When a gated shortcut is ""closed"", the layers in highway networks represent non-residual functions . Our formulation always learns residual functions; our identity shortcuts are never closed .","functions, networks, data-dependent, highway"
34,stacked layers can asymptotically approximate complicated functions2 . x denotes the inputs to the first of these layers .,"mapping, functions, h(x), underlying"
35,degradation problem suggests the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers . The residual learning reformulation is motivated by the counterintuitive phenomena about the degradation problem .,"problem, nonlinear, layers, multiple, identity, mapping, degradation"
36,"if the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations . We show by experiments that the learned residual functions in general have small responses .","function, optimal, identity, residual, map"
42,The shortcut connections in Eqn. introduce neither extra parameter nor computation complexity . This is not only attractive in practice but also important in our comparisons between plain and residual networks .,"eqn.(1), computation, connections, shortcut, complexity"
49,Convolutional layers have a stride of 2. The network ends with a global average pooling layer . The total number of weighted layers is 34 in Fig. 3 .,"nets, baselines, plain, network, vgg"
54,"Residual Network is based on the above plain network . We insert shortcut connections which turn the network into residual version . The shortcut still performs identity mapping, with extra zero entries padded .","mapping, identity, network, residual, connections, shortcut"
56,Our implementation for ImageNet follows the practice in . The image is resized with its shorter side randomly sampled in for scale augmentation .,"image, color, augmentation, imagenet"
71,"We argue that this optimization difficulty is unlikely to be caused by vanishing gradients . These plain networks are trained with BN , which ensures forward propagated signals to have non-zero variances. So neither forward nor backward signals vanish . In fact, the 34-layer plain net is still able","forward, bn, plain, gradients, optimization, propagated, difficulty, networks"
73,Residual Networks. Next we evaluate 18-layer and 34layer residual nets . Expect that a shortcut connection is added to each pair of 3x3 filters .,"resnets, residual, nets"
74,the 34-layer ResNet is better than the 18-layer . This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.,"gains, accuracy, learn-ing, residual"
89,In Table 3 we compare three options: zero-padding shortcuts for increasing dimensions . projection shortcuts are used for increasing dimension .,"shortcuts, identity, training"
90,Table 3 shows that all three options are considerably better than the plain counterpart . B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning . We attribute this to the extra parameters introduced by many projection shortcuts .,"shortcuts, architecture, projection, bottleneck"
91,"Deeper Bottleneck Architectures. Next we describe our deeper nets for ImageNet . For each residual function F, we use a stack of 3 layers instead of 2 . Both designs have similar time complexity .","deeper, nets, time, bottleneck, design, train-, ing"
99,In Table 4 we compare with the previous best single-model results . Our baseline 34-layer ResNets have achieved very competitive accuracy . We combine six models of different depth to form an ensemble .,"top-5, ensemble, error, validation"
102,"The network inputs are 32x32 images, with the per-pixel mean subtracted . The first layer is 3x3 convolutions, with 2n layers for each feature map size. The numbers of filters are 16, 32, 64 respectively .","softmax, network, inputs, architecture"
108,"We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in and BN but with no dropout . These models are trained with a minibatch size of 128 on two GPUs .","bn, weight, initialization, decay"
111,"n = 18 leads to a 110-layer ResNet . In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% . The rest of the learning schedule is as","110-layer, training, error, rate, resnet, converging"
119,"Analysis of Layer Responses. Fig. 7 shows the standard deviations of the layer responses . The responses are the outputs of each 3x3 layer, after BN and before other nonlinearity . This analysis reveals the response strength of the residual functions .","response, strength, resnet, layer, responses"
126,We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large for this small dataset. Strong regularization such as maxout or dropout is applied to obtain the best results on this dataset.,"regularization, architecture, stronger, maxout"
128,Table 7 and 8 shows the object detection baseline results on PASCAL VOC 2007 and 2012 and COCO . Here we are interested in the improvements of replacing VGG-16 with ResNet-101 .,"method, recognition, voc, fem, r-cnn, rescal"
132,"Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult . In CVPR, 2015. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object","convolutional, networks, visual, recognition, deep, backpropagation"
133,"In NIPS, 2014. V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines . In CVPR, 2007. S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Faster R-CNN: Towards","visual, neural, image, recognition, deep, large-scale, networks"
137,ResNet and VGG-16 have no hidden fc layers . We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels . The final classification layer is replaced by two sibling layers.,"feature, fc, f, conv, layer, maps"
138,"for the usage of BN layers, after pre-training, we compute the BN statistics for each layer on the ImageNet training set . The BN layer becomes linear activations with constant offsets and scales . As such, BN stats are not updated by fine-tuning.","imagenet, memory, training, set, consumption"
140,The hyper-parameters for training Faster R-CNN are the same as in Table 7 shows the results . ResNet-101 improves the mAP by >3% over VGG-16. This gain is solely because of the improved features learned by ResNet.,"pascal, training, test, 2012, voc, set, map"
142,The MS COCO dataset involves 80 object categories . We evaluate the PASCAL VOC metric and the standard COCO metric .,"pascal, voc, metric"
144,"ResNet-101 has a 6% increase of mAP@ over VGG-16 . This is a 28% relative improvement, solely contributed by the features learned by the better network .","map@[.5, set, validation, ms, recognition, coco, map@95]"
148,"In Faster R-CNN, the final output is a regressed box that is different from its proposal box . We combine these 300 new predictions with the original 300 predictions . Non-maximum suppression is applied on the union set of predicted boxes .","refinement, box, voting"
149,"Global context can be implemented as ""RoI"" pooling using the entire image's bounding box as the RoI . Global context improves mAP@.5 by about 1 point .","spatial, context, global, pyramid, pooling"
150,"Multi-scale training/testing has been developed in by selecting a scale from a feature pyramid, and in by using maxout layers . In our current implementation, we have performed multi-scale testing following .","multi-scale, training/testing, testing, f-cnn, step"
160,"In Faster R-CNN, the system is designed to learn region proposals and also object classifiers . The mAP is 59.0% and 37.4% on the test-dev set . This result won the 1st place in the detection task in COCO 2015.","r-cnn, faster, ensemble, proposals"
167,"The accuracy is evaluated by mAP@.5. Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9. The networks are pretrained on the 1000-class ImageNet classification set, and are fine-tuned on the DET data . We split the validation set into two parts","validation, imagenet, detection, set, classification"
174,The ImageNet Localization task requires to classify and localize objects . We assume that the image-level classifiers are first adopted for predicting the class labels of an image . The localization algorithm only accounts for predicted bounding boxes based on the predicted classes .,"localization, strategy, algorithm, imagenet"
175,Our RPN for localization is designed in a per-class form . This RPN ends with two sibling 1 x 1 convolutional layers for binary classification and box regression .,"binary, layer, localization, reg, classification"
176,"To avoid negative samples being dominate, 8 anchors are randomly sampled for each image . The sampled positive and negative anchors have a ratio of 1:1 .","augmentation, imagenet, image, training, classification"
180,our RPN method using ResNet-101 net significantly reduces the center-crop error to 13.3% . This comparison demonstrates the excellent performance of our framework . The top-5 localization error is 14.4% .,"classes, resnet-101, ground, error, top-5, localization, truth"
181,"The results are only based on the proposal network in Faster R-CNN . On this dataset, one image usually contains a single dominate object . The proposal regions highly overlap with each other and thus have very similar RoI-pooled features .","training, image-centric, faster, network, proposal, r-cnn"
182,"Our R-CNN implementation is as follows . For each training image, the highest scored 200 proposals are extracted as training samples . This network is fine-tuned on the training set using a mini-batch size of 256 .","class, network, r-cnn, truth, classification, round"
183,This method reduces the top-5 localization error to 10.6% . This result significantly outperforms the ILSVRC 14 results .,"top-5, localization, error, imagenet"
