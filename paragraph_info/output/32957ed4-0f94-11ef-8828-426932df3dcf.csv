element_idx,keywords,summarized_text
7,"system, translation, model, statistical, rnn, log-linear, machine",RNN encodes a sequence of symbols into a fixedlength vector representation . The encoder and decoder of the proposed model are jointly trained to maximize conditional probability of a target sequence given a source sequence .
10,"nlp, deep, embedding, (nlp), translation, extraction, networks, word, (smt), neural, natural, statistical, language, processing, machine","Deep neural networks have shown great success in various applications such as speech recognition . These include language modeling , paraphrase detection and word embedding extraction ."
14,"neural, smt, network, architecture",RNN Encoder-Decoder consists of two recurrent neural networks that act as an encoder and a decoder pair . The encoder maps a variable-length source sequence to a fixed-length vector .
15,"translation, rnn, encoder-decoder",The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French . We train the model to learn the translation probability of an English phrase to a corresponding French phrase . The model is then used as a part of a standard phrase-
16,"translation, rnn, encoder-decoder, model","the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance . The further analysis of the model reveals that the Rencoder Decoder learns a continuous space representation of a phrase that preserves both the"
26,"fixed-length, variable-length, sequence, representation, vector",this paper proposes a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation . This new model is a general method to learn the conditional distribution .
59,"smt, translation, networks, neural, hypotheses","In many cases, neural networks have been used to rescore translation hypotheses . Recently, there has been interest in training neural networks to score the translated sentence using a representation of the source sentence ."
62,"pro, translation, rnn, encoder-decoder, pair, phrase, probability",RNN Encoder-Decoder ignores the frequencies of each phrase pair in the original corpora . This measure was taken in order to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies .
66,"phrase, rnn, encoder-decoder, table",Schwenk pointed out that it is possible to completely replace the existing phrase table with the proposed RNN EncoderDecoder . This requires an expensive sampling procedure to be performed repeatedly . We only consider rescoring the phrase pairs in the phrase table .
69,"neural, rnn, network, scoring, phrase","Schwenk proposed a similar approach of scoring phrase pairs . Instead of the RNN-based neural network, he used a feedforward neural network . The proposed RNN Encoder-Decoder is well-suited for these applications."
72,"context, bag-of-words, representation, source, sentence",a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase . This is closely related to both the proposed RNN Encoder-Decoder and the model proposed in .
73,"words, information, encoder-decoder, rnn, order",the RNN Encoder-Decoder naturally distinguishes between sequences that have the same words but in a different order . The aforementioned approaches effectively ignore order information .
74,"system, smt, conventional, translation, model, recurrent, n-gram, continuous, convolutional",The closest approach related to the proposed RNN Encoder-Decoder is the Recurrent Continuous Translation Model proposed in their paper . The difference with our model is that they used a convolutional n-gram model for the encoder and the hybrid of an inverse CGM and a recurrent neural network
78,"smt, communication, model, tokenization, english/french, language","The bilingual corpora include Europarl , news commentary , UN , and two crawled corpors of 90M and 780M words respectively . The last two corpore are quite noisy ."
79,"weight, selection, rnn, encoder-decoder, data, tuning",We used the test set newstest2012 and 2013 for data selection and weight tuning . Each set has more than 70 thousand words and a single reference translation .
86,"hyperbolic, activation, rank-100, tangent, matrices, function","We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word . The activation function used for  in Eq. is a hyperbolic tangent function ."
91,"smt, pairs, cslm, networks, neural, rnn, encoder-decoder, scoring","In order to assess the effectiveness of scoring phrase pairs with the proposed RNN EncoderDecoder, we also tried a more traditional approach of using a neural network for learning a target language model . Specifically, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by"
93,"softmax, ing, layer, cslm, dur-, model",We trained the CSLM model on 7-grams from the target corpus . Each input word was projected into the embedding space R512 . The vector was fed through two rectified layers .
106,"cslm, model, rnn, encoder-decoder, linear, log-",the best performance was achieved when we used both the CSLM and the phrase scores from the RNN Encoder-Decoder . This suggests that the contributions of the CCLM and RNN are not too correlated and that one can expect better results by improving each method independently .
111,"improvement, translation, model, rnn, encoder-decoder, performance",RNN Encoder-Decoder is trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences .
126,"target, phrases, table, rnn, encoder-decoder, phrase","In Table 3, we show the generated samples from the RNN Encoder-Decoder . For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores . Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table ."
132,"space, encoder-decoder, rnn, language, continuous, models",the proposed RNN Encoder-Decoder projects to and maps back from a sequence of words into a continuous space vector . We expect to see a similar property with the proposed model .
136,"similar, bottom-left, plot, syntactically, rnn, encoder-decoder","RNN Encoder-Decoder captures both semantic and syntactic structures of phrases . For example, most of the phrases are about the duration of time . On the other hand, the top-right plot shows the phrases that are semantically similar ."
139,"sequence, source, encoder-decoder, rnn, length, arbitrary","RNN Encoder-Decoder is able to either score a pair of sequences or generate a target sequence given a source sequence . Along with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or"
141,"smt, bleu, scores, rnn, encoder-decoder","scores by the RNN Encoder-Decoder were found to improve the overall translation performance in terms of BLEU scores . Also, we found that the contribution by RNNEncoder Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system ."
143,"target, phrases, rnn, encoder-decoder, propose, proposed, architecture","the proposed architecture has large potential for further improvement and analysis . One approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the RNN Encoder-Decoder propose target phrases ."
