element_idx,summarized_text,keywords
5,"Vision transformers have been successfully applied in image classification tasks recently . In this paper, we show that the performance of ViTs saturate fast when scaled to be deeper . This fact demonstrates that in deeper layers, the selfattention mechanism fails to learn effective concepts for representation learning .","imagenet, image, vit, deep, transformer, classification"
7,Recent studies have demonstrated that transformers can be successfully applied to vision tasks with competitive performance compared with convolutional neural networks . Vision transformers take advantage of the self-attention mechanism to capture spatial patterns and non-local dependencies . This allows ViTs to aggregate rich global information by stacking multiple convolutions .,"convolutional, neural, transformers, vision, networks"
12,"The recent progress of deep CNN models is largely driven by training very deep models with a large number of layers which is enabled by novel model architecture designs . This is because a deeper CNN can learn richer and more complex representations for the input images and provide better performance on vision tasks . Thus, how to effectively scale CNN","learning, deep, cnn, residual"
15,"a natural question arises: can we further improve performance of ViTs by making it deeper, just like CNNs? To settle the question, we investigate in detail the scalability along depth in this work.","question, natural, mechanism, vit, self-attention, cnn"
16,"In Fig. 1, we show the performance of ViTs with different block numbers . The ViT model with 32 transformer blocks performs even worse than the one with 24 blocks . This means that directly stacking more transformer blocks as performed in CNNs is inefficient at enhancing ViT models .","collapse, imagenet, vit, attention, maps"
17,Our Re-attention takes advantage of the multihead self-attention structure . Experiments show that simply replacing the MHSA module in ViTs with Reattention allows us to train very deep vision transformers with even 32 transformer blocks with consistent improvements .,"collapse, mechanism, multi-head, self-attention, attention, module"
19,"Re-attention is a simple yet effective attention mechanism that considers information exchange among different attention heads . To the best of our knowledge, we are the first to successfully train a 32-block ViT on ImageNet-1k .","collapse, transformers, vision, imagenet-1k, attention"
23,"The vision transformer is among the first attempt that uses the pure transformer architecture to achieve competitive performance with CNNs on the image classification task . Due to the large model complexity, ViT needs to be pre-trained on largerscale datasets for performing well on ImageNet-1k . In this manner, vision transformer can perform well on","imagenet-1k, transformer, vision"
25,"Increasing the network depth of a CNN model is deemed to be an effective way to improve the model performance . However, very deep CNNs are generally harder to train to perform significantly better than the shallow ones in the past . How to effectively scale up the CNNs in depth was a long-standing and challenging problem ","depth, network, cnn"
30,vision transformer model is composed of three main components: a linear layer for patch embedding . a stack of transformer blocks with multi-head self-attention and feed-forward layers for feature encoding .,"self-attention, transformer, vision"
32,"Transformers were extensively used in natural language for encoding input word tokens into a sequence of embeddings . To comply with such sequenceto-sequence learning structure when processing images, ViTs first divide an input image into multiple patches uniformly . Then, all these tokens, together with","transformer, blocks, vits"
35,"In the above self-attention, Q and K are multiplied to generate the attention map . It represents the correlation between all the tokens within each layer . The attention map is computed as Ah,:,: = Softmax with Qh and Kh .","map, factor, attention, scaling"
37,"We conduct systematic study in the changes of the performance of ViTs as depth increases . Without loss of generality, we first fix the hidden dimension and the number of heads to 384 and 12 respectively . The overall performances for image classification are evaluated on ImageNet .","image, deep, architecture, cnn, classification"
38,"We would like to highlight that the self-attention mechanism plays a key role in ViTs, which makes it significantly different from CNNs . We start with examining how the generated attention map A varies as the model goes deeper .","deeper, model, transfromer, architecture, cnn"
43,"Mp,q is the cosine similarity matrix between layers p and q . Consider is one specific self-attention layer and its h-th head, Ah,:,t a T-dimensional vector representing how much the input token t contributes .","similarity, token, cosine, attention, t, map"
44,"We then train a ViT model with 32 transformer blocks on ImageNet-1k . This indicates that the learned attention maps afterwards are similar and the transformer block may degenerate to an MLP . As a result, further stacking such degenerated MHSA may introduce the model rank degeneration issue and limits the","collapse, imagenet-1k, similar, attention, maps"
46,Cross layer similarity of attention map and features for ViTs . The black dotted line shows the cosine similarity between feature maps of the last block and each of the previous blocks .,"similarity, feature, self-attention, attention, map"
48,Attention collapse may hurt the ViT model performance . We compare the final output features with the outputs of each intermediate transformer block . There is a close correlation between the increase of attention similarity and feature similarity .,"similarity, collapse, fire, attention, similar"
57,This will increase the representation capability of each token embedding to encode more information . The resultant attention maps can be more diverse and the similarity between each block's attention map could be reduced .,"collapse, attention, dimension, embedding"
58,Attention collapse is alleviated by increasing the embedding dimension . This validates our hypothesis-the attention collapse is the main bottleneck for scaling ViT.,"collapse, similar, maps, attention"
61,"Similarity of attention maps from different heads of different transformer blocks is high, especially for deep layers . However, we find the similarity between attention maps is quite small . Based on this observation, we propose to establish cross-head communication to re-generate the attention maps .","layer, deep, maps, attention, transformer, block"
64,Re-attention is effective and easy to implement . It needs only a few lines of code and negligible computational overhead compared to the original self-attention .,"diversity, attention, map"
69,"To make a fair comparison, we first tuned a set of parameters for training the ViT base model and then use the same set of hyper-parameters for all the ablation experiments . We use AdamW optimizer and cosine learning rate decay policy with an initial learning rate of 0.0005 . The batch","optimizer, warm-up, size, image, rate, learning, adamw, decay"
74,"We design an experiment to reuse the attention maps computed at an early block of ViT to replace the ones after it . The ""unique"" block is defined as the block whose attention map's similarity ratio with adjacent layers is smaller than 90% . This implies that adding more blocks may not improve the model performance .","similarity, model, vit, attention, reuse, ratio, blocks"
78,Visualization of the attention maps with original MHSA and Re-attention is shown in Fig. 6 . It can be observed that the original ViT learns the local relationship between the adjacent patches in the shallow blocks .,"attention, maps, re-attention, map"
81,We directly replace the self-attention module in ViT with Reattention and show the results in Tab. 4 with different number of transformer blocks . This phenomenon coincides with our observations that the number of blocks with similar attention maps increases with the depth as shown in Fig. 3 . The performance gain is especially significant for deep ViT,"vit, self-attention, module"
92,"We design two sets of experiments on a ViT model with 32 transformer blocks . We first check the impact of the SoftMax temperature on reducing the attention map similarity . As shown in Fig. 8, the number of similar blocks are still large .","similarity, model, training, attention, transformer, block, map"
98,the impacts on the attention maps and the output features of each block are shown in Fig. 8 . This is because the difference between attention maps comes from the zero positions in the generated mask . The improvement is still not significant as shown in Tab. 5.,"similarity, dropping, attention, maps"
99,our proposed Re-attention mechanism uses different heads as basis and re-generate the attention maps via the transformation matrix O . This process incorporates the interhead information communication and the generated attention maps can encode richer information .,"self-, re-attention, attention-dropping, attention"
102,Re-attention creates two ViT variants based on the ViT with 16 and 32 transformer blocks respectively . The hidden dimensions of DeepViT-S and Deep ViT-L models are set as 396 and 408 respectively. The results are shown in Tab. 6.,"self-attention, model, deepvit-l, deepvit-s"
104,"In this work, we found the attention collapse problem of vision transformers as they go deeper and propose a novel Re-attention mechanism to solve it with minimal amount of computation and memory overhead . We hope our observations and methods could facilitate the development of Vision transformers in future.","transformers, attention, vision"
108,"IEEE Transactions on pattern analysis and machine intelligence, 35:1798-1828, 2013. Language models are few-shot learners . arXiv preprint arxiv:2005.14165, 2020 . Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kiril","workshops, conference, image, vision, re-resentation, recognition, on, learning, and, pattern, processing, ieee/cvf, transformer, computer"
109,"Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks . In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. Kaiming He, Xiangyu Zhang, Shaoq","mobile, self-calibrated, image, convolution, vision, recognition, network, computer"
110,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov . Ilya Loshchilov and Frank","visual, conference, vision, recognition, on, design, and, pattern, network, ieee, computer"
111,"Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks . In International Conference on Machine Learning, pages 6105-6114 . PMLR, 2019. Hugo Touvron, Matthieu Cord . Matthijs Douze,","conference, vision, recognition, on, qong-, and, pattern, international, le, qongy, ieee/cvf, qong, qoc, computer"
112,"Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, and Yi Yang. Random erasing data augmentation .","mobile, network, augmentation, transformers"
114,"Vision transformers with 24 blocks and 32 blocks have 11 and 15 blocks with similar attention maps . To verify the effectiveness of the attention maps from those blocks, we directly force those blocks to share the same attention map as the last 'unique' block . For all the following blocks, the attention output is calculated by:","transformers, vision, attention, reuse, aunique, map"
115,"Norm is batch normalization used to adjust the variance . For a ViT with 32 blocks, forcing the top 15 blocks to share the same attention map causes negligible degradation on the classification accuracy on ImageNet .","map, variance, attention"
124,"To study the optimal number of blocks with re-attention, we conduct a set of experiments on a ViT model with 16 transformer blocks . For each experiment, we only apply reattention on top K blocks where K ranges from 5 to 15 . The rest of the blocks are using the original transformer block structure ","re-attention, imagenet, vit, transformer, block"
125,"The highest accuracy appears at the position where the number of re-attention blocks is the same . Based on this observation, we define the architecture of Deep ViT-S and DeepViT-L .","re-attention, deepvit-l, deepvit-s, attention, block, map"
