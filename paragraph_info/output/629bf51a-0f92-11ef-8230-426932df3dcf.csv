element_idx,summarized_text,keywords
4,"The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification . However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet . To overcome such limitations, we propose a new","t2t-vit, transformer, vision, imagenet"
10,Vision Transformer is the first full-transformer model that can be directly applied for image classification . ViT splits each image into 14x 14 or 16 x 16 patches with fixed length .,"super-, image, vision, processing, resolution, transformer"
11,ViT proves the full-transformer architecture is promising for vision tasks . but its performance is still inferior to that of similar-sized CNN counterparts .,"tasks, image, structure, vision, cnn, local"
16,the vanilla ViT ignores the local structure when directly splitting images to tokens with fixed length . This means that the backbone of ViT is not efficient as ResNets .,"vit, layer, bottom, structure"
17,"We propose a progressive tokenization module to aggregate neighboring Tokens to one Token . In each Token-to-Token step, the tokens output by a transformer layer are reconstructed as an image which is then split into tokens with overlapping . Thus the local structure from surrounding patches is embedded","full-transformer, vision, design, architecture, tokenization"
18,"Wide-ResNets , DenseNet , ResneXt structure , Ghost operation and channel attention . We find deep-narrow structure is the most efficient and effective for ViT .","transformers, structure, vision, deep-narrow, design, backbone"
19,"Our T2T-ViT with 21.5M parameters and 4.8G MACs can achieve 81.5% top-1 accuracy on ImageNet, much higher than that of ViT with 48.6M parameters . This result is also higher than the popular CNNs of similar size, like ResNet50 with 25.5","module, t2t-vit, imagenet"
20,Visual transformers can outperform CNNs at different complexities on ImageNet without pretraining on JFT-300M . We develop a novel progressive tokenization for ViT .,"t2t, imagenet, visual, transformers, module"
23,Transformers in Vision Transformers are the models that rely on the self-attention mechanism to draw global dependencies between input and output . Recent works have explored applying transformers to various vision tasks . The Vision Transformer proves that a pure Transformer architecture can also attain state-of-the-art performance on image classification .,"mlp, imagenet, image, vision, classification, transformer, block"
24,"the SE block applies attention to channel dimensions and non-local networks . Compared with most of the works exploring global attention on images, some works also explore self-attention in a local patch to reduce the memory and computation cost .","san, module, cnn"
28,"To overcome the limitations of simple tokenization and inefficient backbone of ViT, we propose Tokens-to-Token Vision Transformer which can progressively tokenize the image to tokens . We adopt a deep-narrow structure for the backbone to reduce redundancy .","tokenization, t2t-vit, backbone"
35,"Soft Split After obtaining the image I, we apply the soft split on it to model local structure information and reduce length of tokens . To avoid information loss in generating tokens from the restructurizated image, we split it into patches with overlapping . Each patch is correlated with surrounding patches .","image, split, soft, re-structurized"
40,"To address the limitations, we set the channel dimension of the T2T layer small to reduce MACs . We optionally adopt an efficient Transformer such as Performer layer to reduce memory usage at limited GPU memory.","mac, limitation, layer, macs, transformer"
42,"We explore different architecture designs for ViT and borrow some designs from CNNs . Each transformer layer has skip connection as ResNets, or apply Wide-ResNets or ResNeXt structure to change the channel dimension and head number in the backbone of vanilla ViT .","backbone, architecture, design"
44,We conduct extensive experiments on the structures transferring in Sec. 4.2 . We empirically find that 1) by adopting a deep-narrow structure that simply decreases channel dimensions to reduce the redundancy in channels . 2) the channel attention as SE block also improves ViT but is less effective than using the deepn,"transferring, structure, deep-narrow"
49,"T2T-ViT-14/19/24 have comparable model size with ResNet50/101/152 . For ViT, S means Small, 'B' is Base and 'L' is Large . ViT-S/16 has smaller MLP size and layer depth .","t2t-vit-14/19/24, t2t-vit-7/12, t2t-vit, t2t-vit-7/"
52,"The T2T-ViT module has two parts: the Tokens-to-Token module and the tokens to Token module . Here, we set n = 2 as shown in Fig. 4 . The patch size for the three soft splits is P = , and the overlapping","t2t, t2t-vit, backbone, module, back"
53,The T2T-ViT backbone takes tokens with fixed length as input . It has a deep-narrow architecture design with smaller hidden dimensions and MLP size than ViT . ViT-B/16 has 12 transformer layers and 768 hidden dimensions .,"t2, t2t-vit-14, t2t-vit, backbone"
55,"T2T-ViT-7, T2TViT-12 and MibileNetV1 are comparable models . The two lite TiT-viT have no special designs or tricks like efficient convolution .","mobilenetv2, resnetv2, mobilenets, tit-vit-12, resnetv1, resnet152, resnet"
60,"All experiments are conducted on ImageNet dataset . We use batch size 512 or 1024 with 8 NVIDIA GPUs for training . For fair comparison, we implement the same training scheme for the CNN models, ViT, and our T2T-ViT .","t2, t2t-vit, imagenet"
61,T2T-ViT vs. ViT We first compare performance of T2TViT and ViT on ImageNet . The results are given in Tab. 2. We compare performance on Imagenet . We also adopt higher image resolution as 384x384 and get 83.3% accuracy .,"t2t-vitt-14, t2t-vit, imagenet, vit-l/16, vit-s/16, vit-"
62,"T2TViT achieves 1.4%-2.7% performance gain over ResNets . For example, our T2T-ViT-14 have 21.5M parameters and 4.8G MACs obtain 81.5% accuracy on ImageNet .","resnet152, resnet, resnet101"
68,"Our T2TViT-12 with 6.9M parameters achieves 76.5% top1 accuracy, which is higher than MobileNetsV21.4cc by 0.9% . However, there are no special operations or tricks like efficient convolution . We only reduce model size by reducing the hidden dimension, MLP ratio","bilenets, distillation, bilenetsv2, bilenetv2"
76,"We apply DenseNet structure, Wide-ResNet structure , ResNeXt structure . The most effective structure is deep-narrow structure, which decreases model size and MACs nearly 2x .","to, structure, ghost, vit, densenet, operation, vit-s/, vit-s/16, cnn"
77,"We further apply these structures from CNN to our T2TViT . We take ResNet50 as the baseline for CNN, ViT-S/16 for ViT, and T2T-ViT-14 .","vit-sw, t2t-vit, vit, t2t-vit-14, vit-s/16, cnn"
83,"Ghost can further compress model and reduce MACs of T2T-ViT . For the original ViT, it would cause more decrease than ResNet .","t2t-vit, ghost"
93,the T2T-ViTc-14 is worse than T2 T-viT-14 . The convolution layers can model both the global relation and the structure information of images .,"t2t, module, layers, convolution"
97,T2T-ViT effectively models the structure information of images and enhances feature richness . It introduces the novel tokens-to-token process to progressively tokenize images to tokens and structurally aggregate tokens . Our T2TViT achieves superior performance to ResNets and comparable performance,"tokens-to-token, tasks, t2t-vit, development, vision, architecture"
99,"In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, pages 3286-3295, 2019. T. B. Brown, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le. Attention augmented convolutional networks .","image, recognition, attention, processing, transformer"
100,"A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand and M. Andreetto . Mobilenets: Efficient convolutional neural networks for mobile vision applications. J. Hu, L. Shen, S","mobile, network, vision, models"
101,"V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Attention is all you need . Advances in neural information processing systems, 30:5998-6008, 2017. X. Wang, R. Girshick, A. Gupta, and K","question, image, vision, recognition, super-resolution, computer"
102,"L. Yuan, S. Chang, Z. Huang, Y. Chen, X. Nie, F. E. Tay, J. Feng, and S. Yan. A simple baseline for pose tracking in videos of crowed scenes . In Proceedings of the 28th ACM International Conference on Multimedia, pages","change, c., loy"
