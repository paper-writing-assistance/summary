element_idx,keywords,summarized_text
5,"trpo, monotonic, policies, improvement","We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement . By making several approximations to the theoretically-justified procedure, we develop a practical algorithm called Trust Region Policy Optimization ."
7,"gradient, iteration, covariance, methods, policy, adaptation, matrix",policy iteration methods alternate between estimating the value function under the current policy and improving the policy . policy gradient methods use an estimator of the gradient of the expected return obtained from sample trajectories .
10,"optimization, namic, locomotion, dy-, gradient-based, programming, approximate","Tetris is a classic benchmark problem for approximate dynamic programming methods, stochastic optimization methods are difficult to beat on this task . Continuous gradient-based optimization has been very successful at learning function approximators for supervised learning tasks with huge numbers of parameters ."
11,"model-free, optimization, region, trust, trpo, pol-, setting, icy","We describe two variants of this algorithm: the single-path method, which can be applied in the model-free setting . The vine method requires the system to be restored to specific states . These algorithms are scalable and can optimize nonlinear policies with tens of thousands of parameters, which have previously posed a"
24,"optimal, maxa, arg, policy"," = arg maxa A , improves the policy if there is at least one state-action pair with a positive advantage value and nonzero state visitation probability . However, in the approximate setting, it will typically be unavoidable, due to estimation and approximation error, that there"
33,"improvement, equation, bound, stochastics","policy improvement bound in Equation can be extended to general stochastic policies, rather than just mixture polices, by replacing a with a distance measure between and  . This result is crucial for extending the improvement guarantee to practical problems ."
59,"13, equation","We first replace Es Poold in the objective by the expectation E sPoold . Next, we replace the advan1- 2 tage values Aoold by the Q-values Quold in Equation ."
64,"gradient, it-, eration, policy, rollout, methods, set, estimation","the first sampling scheme, which we call single path, is the one that is typically used for policy gradient estimation . The second scheme, vine, involves constructing a rollout set and then performing multiple actions from each state in each state ."
68,"oi, set, (Â·|sn), rollout","In this estimation procedure, we first sample SO  Po and simulate the policy Oi . We then choose a subset of N states along these trajectories . For each state Sn in the rollout set, we sample K actions according to an,k  q ."
71,"qui, carlo, state, monte, rollout, estimation","mate QUi by performing a rollout starting with state Sn and action an,k. can We greatly reduce the variance of the Q-value differences between rollouts by using the same random number sequence for the noise in each rollout ."
76,"single, rollout, set, path, method",the vine method gives much better estimates of the advantage values . The downside is that we must perform far more calls to the simulator for each of these advantage estimates .
79,"problem, optimization, gradient, carlo, estimates, algorithm, monte, conjugate, constrained","Approximately solve this constrained optimization problem to update the policy's parameter vector 0. By averaging over samples, construct the estimated objective and constraint in Equation ."
80,"information, state, covariance, sn, matrix, fim",The ana1 V En=1 dui log lytic estimator integrates over the action at each state Sn . It does not depend on the action an sampled . The rate of improvement in the policy is similar to the empirical FIM .
82,"dmax, dkl, penalty, error, estimation","The theory justifies optimizing a surrogate objective with a penalty on KL divergence . However, the large penalty coefficient C leads to prohibitively small steps, so we would like to decrease this coefficient . Our theory ignores estimation error for the advantage function ."
90,"search, policy, marginals, equa-tion, state-action, entropy","Relative entropy policy search constrains the state-action marginals p, while TRPO constrains conditionals p . Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop ."
99,"ablated, single, and, vine, variants, locomotion, controllers, path","We compare the performance of the single path and vine variants of TRPO, several ablated variants, and a number of prior policy optimization algorithms . We also show that these algorithms produce competitive results when learning policies for playing Atari games ."
104,"space, state, progress, gait, forward, swimmer","Swimmer. 10-dimensional state space, linear reward for forward progress and a quadratic penalty on joint effort to produce the reward r = Vx - 10-5 ||u||2 . The swimmer can propel itself forward by making an undulating motion . We ended episodes when the hopper fell over,"
105,"appendix, problem, balancing, cart-pole, policy, architecture","We used 8 = 0.01 for all experiments . See Table 2 in the Appendix for more details on the experimental setup and parameters used . To establish a standard baseline, we also included the classic cartpole balancing problem ."
106,"gradient, covariance, gradient-free, algorithm, adaption, natural, matrix, method","natural gradient, the classic natural policy gradient algorithm, differs from single path by the use of a fixed penalty coefficient instead of the KL divergence constraint . empirical FIM, identical to single path, except that the FIM is estimated using the covariance matrix rather than the analytic estimate ."
107,"kl, progress, divergence, forward","Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress . These results provide empirical evidence that constraining the KL divergence is a more robust way to choose step sizes and make fast, consistent progress compared to using a fixed path ."
110,"cma, cem","CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters . The max KL method learned somewhat more slowly than our final method, due to the more restrictive form of the constraint . overall the result suggests that the average KL divergence constraint"
113,"com-, plex, observations, raw, statistics, games, atari, image","Atari games require learning a variety of behaviors, such as dodging bullets and hitting balls with paddles . Atari uses raw images to evaluate TRPO on a partially observed task ."
119,"search, tree, algorithms, image-based, game, playing, multi-path, monte-carlo",the results of the vine and single path algorithms are summarized in Table 1 . The 500 iterations of our algorithm took about 30 hours on a 16-core computer . Our method only outperformed the prior methods on some of the games .
121,"kl, divergence, penalty, region, trust, methods",We proposed and analyzed trust region methods for optimizing stochastic control policies . We showed monotonic improvement for an algorithm that optimizes a local approximation to the expected return of the policy with a KL divergence penalty .
124,"learning, model, estimation, state","the method we proposed is scalable and has strong theoretical foundations . We hope it will serve as a jumping-off point for future work on training large, rich function approximators for a range of challenging problems . The use of more sophisticated policies, including recurrent policies with hidden state, could further make it"
126,"young, investigator, program, award, simulator, faculty, mast, mujoco","We thank Emo Todorov and Yuval Tassa for providing the MuJoCo simulator; Bruno Scherrer, Tom Erez, Greg Wayne, and the anonymous ICML reviewers for insightful comments . This research was funded in part by the Office of Naval Research through a Young Investigator Award and"
163,"theorem, of, 1, coupling, disagreement, probability","This proof uses techniques from the proof of Theorem 4.1 in . Our proof relies on the notion of coupling, where we jointly define the policies and ' so that they choose the same action with high probability = ."
181,"coupled, trajectory, policy","We can also obtain a coupling over the trajectory distributions produced by and  respectively . We will consider the advantage of  over at timestep t, and decompose this expectation based on whether agrees with  ."
199,"p, space, state, density, reward","We want to bound 7 -7 =rpo . Note that 7 = rGpo, 7 = P - P . We start with some standard perturbation theory manipulations."
214,"search, information, products, matrix-vector, fisher, direction, matrix","In large-scale problems, itis prohibitively costly to Aij = dui doj form the full matrix A . Appendix C.1 describes the most efficient way to compute matrix-vector products with the Fisher information matrix ."
215,"kl, search, divergence, direction, dynamics",The term sT As can be computed through a single Hessian vector product . We need to compute the maximum step length B so that 0 + Bs will satisfy the KL divergence constraint .
216,"kl, search, divergence, line, approximations, linear, constraint","X equals zero when its argument is true and +00 when it is false . Without this line search, the algorithm occasionally computes large steps that cause a catastrophic degradation of performance ."
218,"gradient, information, matrix-vector, fisher, product, algorithm, matrix","This matrix-vector product enables us to perform the conjugate gradient algorithm .Suppose the parameterized policy maps from the input x to ""distribution parameter"" vector en . Now the KL divergence for a given input X can be written as follows ."
220,"in, information, fisher, dui, differentiation, matrix, distribution, terms","The second term vanishes, leaving just the first term . Let J := , then the Fisher dui information matrix can be written in matrix form as JT MJ ."
221,"multiplication, backprop, fisher-vector, product, operation",Fisher-vector product can now be written as a function y  JT M Jy . Multiplication by JT and J can be performed by most automatic differentiation and neural network packages .
222,"mode, hessian, automatic, differentiation, reverse",a generic method could be used for calculating Hessian-vector products using reverse mode automatic differentiation . This method would be slightly less efficient as it does not exploit the fact that the second derivatives of) can be ignored .
225,"gradient, step, fisher-vector, product, a-1g","k is the number of iterations of the conjugate gradient algorithm we perform . We found k = 10 to be quite effective, and using higher k did not result in faster policy improvement . a naive implementation would spend more than 90% of the computational effort on these Fisher-vector products ."
228,"gaussian, state, log, deviation, standard, distribution","a neural network with several fully-connected layers maps from the input features to the mean of a Gaussian distribution . A separate set of parameters specifies the log standard deviation of each element . Then, the policy is defined by the normal distribution N , stdev = exp)."
229,"categorical, categor, distribution, atari","ak E 1, 2,  , Nk, and each of these components is assumed to have a categorical distribution . Hence, H is defined to be the concatenation of the factors' parameters: H = and has dimension dim H = EK=1 Nk ."
