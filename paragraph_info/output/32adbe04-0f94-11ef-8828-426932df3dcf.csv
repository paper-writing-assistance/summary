element_idx,keywords,summarized_text
5,"spatial, question-answering, reasoning, visual, scenes","This paper proposes a benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work . Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs . Experiments show that further pretraining LMs on these automatically generated"
7,"spatial, ing, natural, understand-, reasoning, language","Spatial reasoning is a cognitive process based on the construction of mental representations for spatial objects, relations, and transformations . it is necessary for many natural language understanding tasks such as natural language navigation , human-machine interaction , dialogue systems , and clinical analysis ."
8,"albert, bert",Modern language models have seen great success in natural language processing . bAbI is the only dataset with direct textual spatial question answering .
11,"find, structures, spatial, blocks, spartqa, relation","SPARTQA is built on NLVR's images containing more objects with richer spatial structures . The questions require deeper reasoning and have four types: find relation, find blocks, choose object, and yes/no ."
12,"nlvr, spatial, low-resource, reasoning, setting, annotators","In total, we obtained 1.1k unique QA pair annotations . Similar to bAbI, we keep this dataset in relatively small scale . Experiments show that modern LMs do not perform well in this low-resource setting ."
13,"supervision, spatial, reasoning, signals, distant",This paper thus proposes a way to obtain distant supervision signals for spatial reasoning . We can automatically generate stories for NLVR images via context free grammars and context-sensitive rules .
17,"a, b, c, block","We have three blocks, A, B and C. Block B is to the right of block C and it is below block A . Block C contains one medium blue square and one medium black square ."
19,"two, blue, square, c, number, block","A, B, C FB: Which block doesn't have blue square that is to the left of a medium square? The medium black square which is in block C is below a middle black square ."
25,"spartqa-h, spartqa-human","Experiments show that by further pretraining on SPARTQA-AUTO, we improve LMs' performance by a large margin . BERT further pretrained only requires half of the training data to achieve 99% accuracy on bAbI as compared to the original BERT ."
35,"appendix, f","Task 17 of the bAbI project is the only QA dataset focused on textual spatial reasoning . Solving Task 17 does not require sophisticated reasoning, which is an important capability emphasized by more recent works ."
36,"qa, gqa, multi-modal, bench, benchmark",Spatial reasoning is arguably more prominent in multi-modal QA benchmarks . Some other works on visual-spatial reasoning are based on geographical information inside maps and diagrams and navigational instructions .
42,"nlvr, relationships, qualitative, relative, image, distance, relationship","the blocks are always horizontally aligned in each NLVR image to allow for more flexibility . Relationships between objects within the same block can take the forms of relative direction , qualitative distance , and topological relationship ."
51,"spatial, spartqa-human, data, reasoning, training, large",Annotators spent 45-60 mins on average to create a single story with 8-16 QA pairs . SPARTQA-HUMAN costed about 100 human hours in total .
53,"supervision, spatial, spartqa-human, reasoning, distant","We take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SPARTQAAUTO, and below we describe its generation process in detail ."
68,"correct, cfg, sentences, visual, scenes",Being grounded to visual scenes guarantees spatial coherency in a story . We design context-sensitive rules to limited options for each CFG's variable .
75,"phrase, description, mention","Describe-Objects generates a mention phrase for an object using parts of its full name . To describe a or unique object, it chooses an attribute or a group of attributes that apply to a unique object . For example, ""the circle which is above the black triangle"""
79,"fr, find-all-relations, question, questions","In FB Q-TYPE, we mention an object by its indirect relation to another object using the nested relation in Describe-objects module . The CO question selects an anchor object and specifies a relationship in the question . Two other objects are chosen as candidates to check whether the specified relationship holds between them"
88,"cls, representation, q-type, model","All QTYPES can be cast into a sequence classification task . The three transformer-based LMs tested in this paper, BERT , ALBERT and XLNet , can all handle this type of tasks ."
89,"candidate, fr, answers, questions, question","FR and YN both have a predefined label set as candidate answers . The answer to a question is a single label chosen from Yes, No, and DK . Therefore, we treat each candidate answer as an independent binary classification problem, and take the union as the final answer ."
90,"output, layer, co, q-types, fb, model, fr, lstm","YN and FR model has moderately less accurate results on FB and CO Q-TYPES . To find the final answer, we add a LSTM layer to improve it . The final answers are selected based on Eq."
96,"fem, flnet, ing, spatial, fine-tuning, reason-",We study the capability of spatial reasoning of modern LMs . This fine-tuning process is also known as further pretraining . It is an open problem to find out better transfer learning techniques .
114,"spartqa-, auto, spart, bert","the proposed System 5 overall performs better than the other three baseline systems, but one exception is its accuracy on YN, which is lower than that of System 3 . To verify it, we compute the F1 score for Q-TYPE in Table 3 where we see all systems effectively achieve better scores than the majority baseline ."
121,"albert, spartqa-human, spart, spartqa-auto","We further pretrain these LMs on SPARTQA-AUTO, Seen and Unseen . In addition to BERT, we continue to test another two held-out test sets . We use the unseen set to test to what extent baseline models use shortcuts ."
122,"xnet, spatial, model, human, roles, models",XLNet performs the best on all Q-TYPES except its accuracy on SPARTQA-HUMAN's YN section . The drops in Unseen and human suggest overfitting on the training vocabulary .
124,"contrast, reliability, test, set","To evaluate the reliability of the models, we also provide two extra consistency and contrast test sets . Given a pivot question and answer of a specific consistency set, answering other questions in the set does not need extra reasoning over the story ."
125,"contrast, minimal, its, answer, to, modification, question, set, change",Contrast set is made by minimal modification in a question to change its answer . The consistency and contrast sets are evaluated only on the correctly predicted questions to check if the actual understanding and reasoning occurs . This ensures the reliability of the models.
142,"pre-trained, spatial, lm, reasoning, state-of-the-art, language, models","Spatial reasoning is an important problem in natural language understanding . We propose the first human-created QA benchmark on spatial reasoning . To improve LMs' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules ."
160,"contrast, decision, boundaries, sets, 2020, local, emnlp",In Findings of the Association for Computational Linguistics: EMNLP 2020. Evaluating models' local decision boundaries via contrast sets .
169,"qu, xinhua, yuzhong, emnlp",GeoSQA: A benchmark for scenario-based question answering in the geography domain at high school level . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing .
205,"questions, question, triangle, indefinition","Articles and pronouns in each template play an essential role in understanding the question's objective . For example, ""Are there any blue circles near to a triangle?"" is different from ""a"" or ""the"" in proper places ."
212,"shortcuts, modification, spartqa-auto",Wir propose an unseen test set alongside the seen test of SPARTQA-AUTO to check whether a model is using shortcuts in the language surface . This set has minor modifications that should not affect the performance of a consistent and reliable model . The modifications are randomly applied on a number of generated stories and questions
216,"questionsâ€™, answer, conscience, questions, set",Consistency set is made by changing parts of the question in a way that it still asks about the same information . Answering these questions around a pivot question is possible for human without the need for extra reasoning over the story and based on the main questions' answer .
217,"contrast, minor, set, changes","In the question ""Is the blue circle below the black triangle? Yes,"" we create a contrast question . The evaluation on this set shows the robustness of the model and its sensitivity to the semantic changes ."
221,"human, annotations, scene-graph, spartqa-auto","In SPARTQA-AUTO, we generated fine-grained scene-graph based on the story . This scene-graphic contains blocks' description, their relations, and the objects' attributes alongside their direct relations with each other . Figure 7 shows an example of this scene . It can provide strong supervision for question answering challenges"
230,"spatial, and, labeling, expressions, arguments, role, (sprl)",SpRL is used for recognizing spatial expressions and arguments in a sentence . We also provided spatial annotations for each sentence and question . This annotation is generated by hand-crafted rules during the main data generation .
244,"correct, label, triangle",Table 8: The percentage of each correct label in all samples . The candidate answers for the FB Q-TYPE can be varied based on its story .
246,"bert, huggingface, implementation, training","We use a boolean classifier on the output of """" token from the last layer of BERT model . We train the model on the 10k, 5k, 2k, 1k, 500, and 100 portion of bAbI's training questions . The model yields 100% accuracy on 10"
251,"of, great, hurricane, 1989, storm","The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands  A: Yes. Q: Does France have a Prime Minister and a President? P:   The extent to which those decisions lie with the Prime Minister or President?"
