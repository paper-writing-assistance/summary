element_idx,keywords,summarized_text
6,"graphical, sequence, to-sequence, modeling, sequences",Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks . Many complex tasks require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence framework which employs the chain rule to efficiently represent the joint probability of
8,"captioning, recognition, speech, lstm, image","Deep architectures have shown in the last few years that they often yield state-of-the-art performance on several inherently sequential tasks . Such examples range from machine translation , to image captioning , speech recognition , constituency parsing and learning to compute . These approaches all follow a simple architecture, "
9,"framework, sequence-to-sequence, data, lstm","how should we represent data, either inputs or outputs, for problems where an obvious order cannot be determined? How should we encode a set of detected objects when there is no specific known order among them?"
15,"computation, sequence-to-sequence, map, models","sequence-to-sequence models were proposed for machine translation . For example, image captioning maps from an image to a sentence . models for computation map from problem statements to their solutions ."
16,"neural, rnnsearch, machines, turing","More recently, many related models and key contributions have been proposed, that utilize the concept of external memories . The key element that these models utilize is a reading mechanism to read these external memories in a fully differentiable way ."
17,"algorithms, networks, prediction, structured, lstm","our approach relies on the chain rule to serialize output random variables through the strong capabilities of LSTM networks . Similarly, we do not want to assume a known structured input ."
19,"probabilit, probability, dynamic, conditional","a generic supervised task with a given training set of n pairs n=1 corresponds to tasks where both Xi and Yv are represented by sequences . In this case, it is reasonable to model each example using the conditional probability P and to use the chain rule to decompose it"
27,"order, sequential, cats, sequence","""I like cats"" becomes the set . We argue that even for sequences, inputting and/or outputting them in a different order could be beneficial ."
31,"approach, ap, bag-of-words","the bag-of-words approach satisfies this, and is commonly used for encoding sentences . In this case, the representation is simply a reduction of counts, word embeddings, or similar embedded functions . For language and other domains which are naturally sequential, this is replaced with more complex"
32,"reduction, operation, addition","the model operates over a fixed dimensional embedding regardless of the length of the set . It is unlikely that such representation will succeed, as the amount of memory required to encode a length T set should increase as a function of T ."
35,"order, sequences","In this section, we highlight prior work where we observed that the order of inputs impacted the performance of seq2seq models taking sequences as input . In principle, order should not matter when using a complex encoder such as a recurrent neural network, as these are universal approximators that"
36,"order, translation, machine","In machine translation, the mapping function encodes a sentence in a source language , and decodes it to its translation in target language . Sutskever et al. got a 5.0 BLEU score improvement by reversing the order of the input English sentence ."
42,"memory, seq2seq, network, paradigm","In the next sections, we explain such a modification, which could be seen as a special case of a Memory Network or Neural Turing Machine - with a computation flow as depicted in Figure 1."
44,"handwriting, mechanism, memory, and, translation, generation, recognition, attention",Neural models with memories coupled to differentiable addressing mechanism have been successfully applied to handwriting generation and recognition . This has the property that the vector retrieved from our memory would not change if we randomly shuffled the memory .
47,"state, read, vector, rt, lstm, index",qt is a query vector which allows us to read rt from the memories . q* is the state which this LSTM evolves . Note that permuting mi and mi' has no effect on the read vector .
54,"write, mechanism, pointer, reads, network, block, attention, lstm","The original work in Vinyals et al. used a pointer mechanism which, instead of issuing a readout of memory by a weighted sum with a soft pointer, uses the pointer as part of the loss . This is related to the process block described above, but with the difference"
55,"memory, neural, turing, network, machine",The architecture is depicted in Figure 1 and can be seen as a special case of a Neural Turing Machine or Memory Network . It satisfies the key property of being invariant to the order of the elements in X . Also note that the write component could simply be an LSTM
57,"sort, number, read, t, architecture","In order to verify if our model handles sets more efficiently than the vanilla seq2seq approach, we ran the following experiment on artificial data for the task of sorting numbers . We used the architecture defined in Figure 1, where the Read module is a small multilayer perceptron for each number, the Process module"
58,"pointer, writing, glimpses, baseline, module, network","the baseline pointer network LSTM input model is better than the Read-Process-and- Write model when no processing steps are used . As soon as at least one processing step is allowed, the performance gets worse, as expected . Also note that with 0 processing steps and 0 glimpses, the writing module"
64,"output, ordering, composition, joint, probability, lstm","So far, we have considered the problem of encoding input sets . The chain rule which describes joint probabilities over sets of random variables Y is, perhaps, the simplest decomposition of the joint probability which does not incur arbitrary restrictions ."
67,"conditional, probability, seq2seq, distribution, models","In this section, we will study the effect that ordering has on the performance of seq2seq models on several tasks . We will consider arbitrary orders over the variables in Y and model the conditional probability distribution P following that order ."
69,"sized, modeling, lstm, medium",PennTree Bank is a standard language modeling benchmark . This dataset is quite small for language modeling standards . We trained medium sized LSTMs with large amounts of regularization .
71,"3-word, probability, reversal, joint",3-word reversal destroys the structure of the sentence . For each ordering we trained a different model . The results for both natural and reverse matched each other at 86 perplexity .
74,"tree, first, linearized, schemes, depth, traversal",depth first traversal is only one of the many ways one can uniquely encode a tree onto a sequence . We therefore tried to train a small model using depth second traversal . The model trained to produce depth third traversal linearized trees obtained 89.5% F1 score .
82,"x, sorted, input","We can choose to output these indices in some order or treat them as a set . If our training setis generated with any permutations picked uniformly at random, our mapping will have to place equal probability on n! output configurations for the same input X."
83,"equiv-alence, class, ordering, lexicographical, outputs","In previous work, it was found that restricting as much as possible the equivalence class for the outputs was always better . For example, to output a tour , we started from the lower indexed city ."
88,"bayes, rule","the natural order of words gives a good clue of how to order the random variables in the model . In practice, the order should not matter because of Bayes rule which lets us reorder all the conditional probabilities as needed ."
92,"variables, head, probability, variable","we created several artificial datasets by varying the number of random variables to model . For each problem, we trained two LSTMs for 10,000 mini-batch iterations to model the joint probability ."
94,"of, variables, v, number, random, large, lstm","the LSTM is able to learn the joint probability in whichever order .  when the marginal distributions are very peaky ,  in all other cases , it was always easier to learn an ELSTM with the optimal order of random variables ."
97,"sequentially, joint, rule, probability, chain, function","Fortunately, we can apply the chain rule which decomposes this joint probability sequentially without independence assumptions . In this work, we focus on using the chains rule, discarding more naive decompositions that have strong and unrealistic assumptions."
98,"lstm, y",the chain rule violates the argument of treating y1 as a set . In the previous section we have shown that certain orderings are better than others .
99,"drawback, model","We would like to train the model as p|X . The number of possible orderings is large - n! where n is the length of the output, and the best order is unknown ."
108,"search, complexity, inexact, modeling, language","In our initial attempt to solve , we considered a simplified version of the language modeling task described in Section 5.1.1 . The simplified task consists of modeling the joint probability of 5grams without any further context . This choice allowed us to have a small enough n as initially we were trying to exactly find the best ordering"
111,"order, perplexity, matters, drops","The first experiment, which reinforces our result in Section 5.1.1, tests the hypothesis once again that order matters . If, instead of picking, we use , perplexity drops to 280 ."
114,"settles, framework, seq2seq, model","In the easy case, we restrict the search space over orderings to only 2, where one order is clearly better than the other . Very quickly, the model settles on the natural ordering, yielding a perplexity of 225 . Thus, the framework we propose is able to find good orderings without any prior knowledge"
121,"lstm, long, data, input, term, dependencies","LSTMs have shown to be powerful models to represent variable length sequential data thanks to their ability to handle reasonably long term dependencies and the use of the chain rule to efficiently decompose joint distributions . On the other hand, some problems are expressed in terms of an unordered set of elements, either as input or"
