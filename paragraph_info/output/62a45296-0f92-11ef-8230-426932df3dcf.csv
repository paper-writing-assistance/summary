element_idx,summarized_text,keywords
4,Co-scale conv-attentional image Transformers is a Transformer-based image classifier . The co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales . We design a series of serial and parallel blocks to realize the mechanism .,"image, transformers, co-scale, conv-attentional"
6,"In the past, state-of-the-art image classifiers have been built on convolutional neural networks that operate on layers of filtering processes . Recent developments however begin to show encouraging results for Transformer-based image classifications .","mechanisms, intelligence, artificial, attention, transformer"
11,"the receptive field at each token in self-attention readily covers the entire input space . the selfattention operation for each token computes a dot product between the ""query"" and the ""key""","token, tokens, convolution, cnn"
12,"convolution and self-attention operations perform weighted sums . The weights are learned during training but fixed during testing . As a consequence, the self-similarity operation in the selfattention mechanism provides modeling means that are potentially more adaptive and general than convolution operations.","operation, convolution, self-similarity"
14,"Transformers has mainly focused on natural language processing tasks since text is ""shorter"" than an image . In computer vision, self-attention has been adopted to provide modeling capability for various applications .","self-attention, operation, vision, computer"
15,"Transformers are adopted to perform object detection and panoptic segmentation, but DETR still uses CNN backbones to extract the basic image features . In this paper, we develop Co-scale conv-attentional image Transformers by introducing two mechanisms of practical significance to Transformer-based image classifiers .","image, modelling, classifiers, cnn"
16,"Two types of building blocks are developed, namely a serial and a parallel block, realizing fine-to-coarse, coarseto-fine, and cross-scale image modeling . We design a conv-attention module to realize relative position embeddings with convolutions .","image, co-scale, transformers, modeling"
17,CoaT achieves state-of-the-art classification results when compared with competitive convolutional neural networks . Our resulting image Transformers learn effective representations under a modularized architecture .,"convolutional, neural, image, transformers, competitive, networks"
21,CoaT models shown in the experiments are based on two of our new designs in Transformers: a co-scale mechanism that allows cross-scale interaction; a conv-attention module to realize an efficient self-attention operation .,"conv-like, transformers, co-scale, operations, conv-attentional"
22,Multi-scale approaches have a long history in computer vision . U-Net enforces an extra coarse-to-fine route . HRNet provides a further enhanced modeling capability . CoaT consists of a series of highly modularized serial and parallel blocks .,"modeling, multi-scale, vision, co-scale, computer"
25,"Transformers take as input a sequence of vector representations X1, ..., XN, or equivalently X E RNxC The self-attention mechanism in Transformers projects each Xi into corresponding query, key, and value vectors . The scaled dot-product attention from original Transformer","transformers, value, vectors"
26,"In vision Transformers, the input sequence of vectors is formulated by the concatenation of a class token CLS and the flattened feature vectors X1,  ., XHW as image for a total tokens from the feature maps F E RHxWxC .","image, co-scale, multi-scale, representation"
34,"the scaling factor 1 / VC is implicitly included in the weight initialization . The factorized attention follows is not a direct approximation of the scaled dot-product attention, but it can still be regarded as a generalized attention mechanism modeling the feature interactions .","lambdanets, softmax(Â·)"
40,"Without the position encoding, the Transformer is only composed of linear layers and self-attention modules . Thus, the output of a token is dependent on the corresponding input without awareness of any difference in its locally nearby features . This property is unfavorable for vision tasks such as semantic segmentation .","layers, linear, self-attention, modules, layer, transformer"
41,ViT and DeiT insert absolute position embeddings into the input . We can integrate a relative position encoding M-1 M21  with window size P = Pi E RC .,"position, convolutional, encoding, relative, embedding"
47,Convolutional Relative Position Encoding works in standard scaled dot-product attention settings since the encoding matrix E is combined with the softmax logits in the attention maps .,"relative, convolutional, position, encoding"
50,LambdaNets uses a 3-D convolution to compute EV directly and reduce channels of queries and keys to CK where CK  C is large . Our factorized attention computes EV with only O space complexity and O time complexity .,"time, convolution, lambdanets, depth-wise, complexity"
51,"Convolutional relative position encoding models local position-based relationships between queries and values . In each conv-attentional module, we insert a depthwise convolution into the input features X and concatenate the resulting position-aware features back to the output features .","position, convolutional, encoding, relative, conditional"
52,"coaT and CoaT-Lite share the convolutional position encoding weights . Convolution kernel size to 3, 5 and 7 for image features from different attention heads .","position, convolutional, encoding, parallel, coat-lite, modules, weights"
57,"CoaT Serial Block models image representations in a reduced resolution . In a typical serial block, we first down-sample input feature maps by a certain ratio . We then concatenate image tokens with an additional CLS token, a specialized vector to perform image classification .","token, image, tokens, representations, cls"
58,"In a typical parallel group, we have sequences of input features from serial blocks . To enable fine-to-coarse, coarseto-fine, and cross-scale interaction, we develop two strategies: direct cross-layer attention .","mechanism, parallel, group, co-scale, block"
64,"vectors to match the resolution of other scales . We then perform cross-attention, which extends the conv-attention with queries from the current scale with keys and values from another scale .","conv-attention, cross-layer, cross-scale, attention, information"
65,"Attention with feature interpolation . The input image features from different scales are processed by independent conv-attention modules . Next, we down-sample or up-semple images from each scale to match the dimensions of other scales .","conv-attention, feature, with, interpolation, attention, module"
68,"CoaT-Lite processes input images with a series of serial blocks following a fine-to-coarse pyramid structure . Each serial block down-samples the image features into lower resolution, resulting in a sequence of four resolutions .","image, coat-lite, input, coating"
73,"Tiny, Mini, Small and Medium models follow the same architecture design . In CoaT-Lite Tiny architectures, the hidden dimensions of the attention layers increase in later blocks .","variants, model, mechanism, coat-lite, co-scale"
85,COCO2017 benchmark contains 118K training images and 5K validation images . We perform object detection based on Deformable DETR following its data processing settings .,"coco2017, segmentation, instance, detection, object"
91,Tables 3 and 4 demonstrate CoaT object detection and instance segmentation results under the Mask R-CNN and Cascade Mask R CNN frameworks on the COCO val2017 dataset . Our co-scale mechanism is essential to improve the performance of Transformer-based architectures for downstream tasks .,"cad, model, segmentation, mask, mechanism, cascade, co-scale, instance, r-cnn"
94,Our CoaT-Lite without any position encoding results in poor performance . The combination of CRPE and CPE leads to the best results .,"position, crpe, convolutional, encoding, relative"
101,"CoaT models attain higher accuracy than similar-sized Swin Transformers . The current parallel groups are more computationally demanding . We report FLOPs, FPS, latency, and GPU memory usage in Table 8 .","fps, coat, computational, memory, flop, gpu, cost, latency"
105,Co-scale conv-attentional image Transformer is a Transformer based image classifier . CoaT models attain strong classification results on ImageNet .,"image, transformer, vision, computer"
108,"In ICLR, 2021. Zhaowei Cai, Nuno Vasconcelos, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. Kai Chen, Jiaqi Wang, Yuhang","image, transformers, vision, recognition, iccv"
109,"IJCV, 60:91-110, 2004. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Stand-alone self-attention in vision models . In CVPR, 2016. Christian Szegedy, Wei Liu, Yang","image, vision, large-scale, attention, transformer"
110,"Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Attentional shapecontextnet for point cloud recognition . In CVPR, 2017. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ru","image, attention, visual"
