element_idx,keywords,summarized_text
7,"dynamic, lstm-rnn, network, classifiers",Long-Term Memory Recurrent Neural Networks are one of the most powerful dynamic classifiers publicly known . The network itself and the related learning algorithms are reasonably well documented to get an idea how it works . This paper will shed more light into understanding how LSTM-RNNs evolved and why they work impressively well
9,"memory, long-term, re-, networks, intelligence, neural, current, artificial",This article is a tutorial-like introduction initially developed as supplementary material for lectures focused on Artificial Intelligence . The interested reader can deepen his /her knowledge by understanding Long Short- Term Memory Recurrent Neural Networks considering its evolution since the early nineties .
14,"feed-forward, interconnected, static, neurons, networks, neural, classification",Neurons are simple units accepting a vector of real-valued inputs . Feed-forward neural networks are limited to static classification tasks . To model time prediction tasks we need a dynamic classifier .
15,"feed-forward, dynamic, networks, timesteps, neural, lstm-rnn, classification","We can extend feed-forward neural networks towards dynamic classification . This is due to the fed back signal is either vanishing or exploding . LSTM networks are to a certain extend biologically plausible and capable to learn more than 1,000 timesteps ."
17,"social, networks, neural, lstm-rnns, evolution","In this article, we help beginners in the machine learning community to understand how LSTM works with the intention motivate its further development . This article explains how neural networks evolved from a single perceptron to something as powerful ."
23,"of, weight, the, value, activation, u, unit, function","The output of a unit u is Yu, and unlike the input, it is a single value .  The set of units with connections from a Unit u; i.e., its successors, is Suc ."
28,"perceptron, threshold, neuron, artificial","Perceptrons consist of a number of external input links, a threshold, and a single external output link . Additionally, perceptrons have an internal input, 6, called bias . The perceptron learns these weights on the basis of training data . It sums all weighted input values and"
32,"nand, boolean, nor, functions, n","Single perceptrons can already represent a number of useful functions . Examples are the Boolean functions AND, OR, NAND and NOR . Other functions are only representable using networks of neurons ."
37,"separability, boolean, two-dimensional, graph, functions, linear","Figure 2 shows representations of the Boolean functions OR and XOR . The OR function is linearly separable, whereas the Xor function is not ."
43,"perceptron, imitation, training","Perceptron training is learning by imitation, which is called supervised learning . In cases of misclassification, it then modifies the weights accordingly . Convergence is not assured if the training data is not linearly separable."
44,"perceptron, rule, learning, delta","the perceptron learns from a set of samples A sample is a pair x, d> where x is the input and d is its label . The old weight vector W = W1 is updated to the new vector W' using the rule ."
46,"rate, learning, examples, training",the learning rate is a constant that controls the degree to which the weights are changed . The perceptron rule fails if the training examples are not linearly separable .
47,"optim, gradient, optimisation, learning, delta, descent, algorithm, rule","delta learning rule was specifically designed to handle linearly separable and linearly non-separable training examples . It also calculates the errors between calculated output and output data from training samples . The modification of weights is achieved by using gradient optimisation descent algorithm, which alters them in the direction that produces the steep"
53,"threshold, unit, sigmoid","the output of the sigmoid threshold unit now has more than two possible values . The function is called the 'squashing' function because it maps a very large input domain onto a small range of outputs . For a low total input value, the output is close to zero, whereas it is"
60,"perceptron, feed-forward, networks, neural, single-layer",single-layer neural networks consist of a set of input neurons . The outputs of the input-layer neurons are directly connected to the neurons of the output layer .
62,"neural, feed-forward, multilayer, network","the input and output layers are connected via at least one hidden layer . For most problems, feed-forward neural networks with more than two layers offer no advantage ."
69,"□, h, σ, 0, i, non-input, n, units","The set of units of the network is N  I  H  0, where  is disjoint union, and I, H, 0 are the sets of input, hidden and output units respectively . For convenience, we define the set of non-input units U E U, the input to "
76,"output, learning, eo, network, backpropagation, error",the backpropagation learning algorithm propagates the error backwards . The algorithm compares the network output Yo with the desired target output do . It calculates the error eo for each output neuron using some error function to be minimised .
96,"dynamic, events, systems, series, rnns, time",Recurrent neural networks are dynamic systems; they have an internal state at each time step of the classification . These feedback connections enable RNNs to propagate data from earlier events to current processing steps .
98,"layer, neuron, hidden, standard, network, backpropagation, error, elman",Elman networks can be trained with standard error backpropagation . Each hidden neuron has its own context cell and receives input from the input layer . Figures 5 and 6 show a standard feed-forward network in comparison with such an Elman network.
105,"rtrl, ffnn, learning, through, recurrent, backpropagation, time","the most common and well-documented learning algorithms for training RNNs in temporal, supervised learning tasks are backpropagation through time and real-time recurrent learning . In BPTT, the network is unfolded in time to construct an FFNN . Then, the generalised delta"
111,"rtrl, networks, neural, recurrent, bptt",Backpropagation Through Time and Real-Time Recurrent Learning . The main difference between BPTT and RTRL is the way the weight changes are calculated .
113,"time, step, ffnn, bptt","Figure 9a shows a simple, fully recurrent neural network with a single two-neuron layer . The corresponding feed-forward neural network requires a separate layer for each time step with the same weights for all layers ."
114,"algorithm, backpropagation, unfolded, network","the network can be trained using the backpropagation algorithm described in Section 4 . At the end of a training sequence, the network is unfolded in time ."
115,"backpropagation, time, steps","We consider discrete time steps 1,2,3..., indexed by the variable T . This time frame between t' and t is called an epoch . Let U be the set of non input units, and let fu be the differentiable, non-linear squashing function of the unit u"
121,"(u), t(t), e, unpre, value, network","where v E UnPre and i E I, the set of input units . Note that the inputs to u at time +1 are of two types: the environmental input that arrives . At time T, the output value Yu of the unit u E T should match some target value du. The cost function"
131,"rtrl, algorithm, network, information",RTRL algorithm does not require error propagation . All the information necessary to compute the gradient is collected as the input stream is presented to the network . This makes a dedicated training interval obsolete .
149,"18, equation, t', time",Equation 19 calculates the quantities pkuv for the first and all subsequent time steps using Equation 18 . Equation 14 and 13 combine these values with the error vector e . The final weight change for W can be calculated using Equations 14 and 13.
167,"short-term, memory, carousels, long, error, constant","LSTM can learn how to bridge minimal time lags of more than 1,000 discrete time steps . The solution uses constant error carousels . Access to cells is handled by multiplicative gate units ."
173,"short-term, identity, memory, =, fu, carousel, storage, id, error, constant, function","this preservation of error is called the constant error carousel . It is the central feature of LSTM, where short-term memory storage is achieved for extended periods of time."
175,"inputs, weighted, cec, neuron, cell, u","In the absence of new inputs to the cell, we now know that the CEC's backflow remains constant . Incoming connections to neuron u can have conflicting weight update signals because the same weight is used for storing and ignoring inputs ."
179,"state, cec, cell, threshold, sigmoid, simple, units","input gates are simple sigmoid threshold units with an activation function range of . They control signals from the network to the memory cell by scaling them appropriately . When the gate is closed, activation is close to zero . The output gates can learn how to control access to memory cell contents ."
181,"rtrl, cell, train, network, components, bptt","the original formulation of LSTM used a combination of two learning algorithms . The latter units work with RTRL because there are some partial derivatives that need to be computed during every step . For now, we only allow the gradient of the cell to be propagated through time, truncating the rest of"
185,"output, mc, memory, gate, block, lstm","In the original formulation of LSTM, each memory block m is associated with one input gate inm and one output gate outm . The internal state of a memory cell mc at time T + 1 is updated according to its state Smc and according to the weighted input gate Zmc multiplie"
214,"state, cell, network, self-connection, lstm","cell states Sm tend to grow linearly during the progression of a time series presented in a continuous input stream . The main negative effect is that the entire memory cell loses its memorising capability, and begins to function like an ordinary RNN network neuron ."
216,"adaptive, self-connection, forget, gate","Forget gates can learn to reset the internal state of the memory cell when the stored information is no longer needed . To this end, we replace the weight '1.0' of the self-connection from the CEC with a multiplicative, forget gate activation ."
222,"memory, forget, cell, activation, gate, forgetting, lstm",bias weights of input and output gates are initialised with negative values . The memory cell will behave like a standard LSTM memory cell without a forget gate . This prevents the memory cell from forgetting .
245,"cells, complexity, memory, number, gers, block, measure","In this section, we present a complexity measure following the same principles that Gers used in . Let B, C,In, Out be the number of memory blocks, memory cells in each block, input units and output units . For each memory block we need to resolve the connections for each cell, input gate, forget gate and"
252,"blocks, memory, forget, gate, block","memory blocks also have a forget gate which weights the information inside the cells . Forget gates also enable continuous prediction, because they can make cells completely forget their previous state ."
253,"lstm, modularisation, network",LSTM requires the topology of the network to be fixed a priori . The number of memory blocks in networks does not change dynamically .
255,"(gru), unit, recurrent, lstm-rnn, gated",LSTM-RNN permits many different variants and topologies . These are partially problem specific and can be derived from the basic method . The basic method is referenced to as 'vanilla' the most common in use . In the following sections we cover the most popular variants of Grid LS
257,"lstm, phoneme, training, bidirectional, phone","Conventional RNNs analyse, for any given point in a sequence, just one direction during processing: the past . At a very high level, bidirectional means that the input is presented forwards and backwards to two separate LSTM networks, both of which are connected to the same output layer ."
259,"connectionist, (ctc), (ctc, classification, temporal",In 2006 introduced an RNN objective function named Connectionist Temporal Classification . The advantage of CTC is that it enables the LSTM-RNN to handle input data not segmented into sequences .
263,"multidimensional, grid, lstm","Grid LSTM is analogous to the stacked lsTM . It adds cells along the depth dimension too, i.e., in-between layers . Additionally, N-LSTM networks with N > 2 are analogous ."
264,"lstm, cells, memory, hidden, network, signals, units","Whenever this LSTM network is provided an input vector x, there is a change in the signals emitted by both hidden units and memory cells . Let P be a projection matrix, the concatenation of the new input signals and the recurrent signals is given by weights W ."
277,"multidimensional, network, lstm","a Grid LSTM block outputs N hidden vectors hidden Sm1 and SmN . To do so, the model concatenates the vectors from the N dimensions ."
298,"lstm, non-markovian, reinforcement, learning","LSTM was successfully introduced to meta-learning with a program search tasks to approximate a learning algorithm for quadratic functions . This included recalling real numbers over extended noisy sequences , learning context free languages , and various tasks that require precise timing and counting ."
302,"networks, neural, recognition, speech, hmm",LSTM-RNN networks with a mix of sigmoidal units to speech recognition tasks were obtained by . Better results comparable to Hidden-Markov-Model -based systems were achieved using bidirectional training with BLSTM .
304,"learning, sequence-to-sequence, framework, recognition, attention-based, speech",LSTM was improving results using the sequence-to-sequence framework and attention-based learning . In 2015 introduced an specialised architecture for speech recognition with two functions . The 'listener' function uses BLSTM with a pyramid structure similar to clockwork RNNs introduced by and .
306,"handwriting, model, online, hidden-markov, blstm-ctc, data, raw, recognition","In 2007 introduced BLSTM-CTC and applied it to online handwriting recognition, with results later outperforming Hidden-Markov-based recognition systems . In another approach combined BLSTC with multidimensional LSTM and by this developed a system capable of directly transcribing raw online handwritten data ."
308,"translation, statistical, rnn, encoder-decoder, architecture, machine",In 2014 the authors applied the RNN encoder-decoder neural network architecture to machine translation and improved the performance of a statistical machine translation system . The architecture was further improved by addressing issues related to the translation of long sentences by implementing an attention mechanism into the decoder .
310,"interpretation, feature, extractor, bsltm, visual, image",In 2012 BSLTM was applied to keyword spotting and mode detection distinguishing different types of content in handwritten documents . In 2015 the more recent LSTM variant was successfully trained by to generate natural sentences in plain English describing images .
316,"lem, prob-, time-lag, rnn, lstm","In this article, we covered the derivation of LSTM in detail . We highlighted the vanishing error problem, which is a serious shortcoming of RNNs ."
318,"ralf, national, university, rhodes, africa, foundation, staudemeyer, african, south, research, c.","This work was mainly pushed as a private project from Ralf C. Staudemeyer . During the time 2013-15 it was partially supported by post-doctoral fellowship research funds provided by the South African National Research Foundation, Rhodes University, the University of South Africa ."
319,"short-term, memory, long-term, janza, arne, long, networks, neural, recurrent",We acknowledge support for this work from Ralf's Ph.D. supervisor Christian W. Omlin for raising the authors interest to investigate the capabilities of Long Short-Term Memory Recurrent Neural Networks . Arne Janza for doing the internal review.
323,"long-term, networks, long, neural, recurrent, attention-based, dependencies","Int. Conf. on Artificial Neural Networks, pages 755-764, 2009. Yoshua Bengio, Patrice Simard, Paolo Frasconi, and Christian Janvin. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3:1137-1155, 2003. N Beringer, A Grav"
325,"long-term, networks, neural, recurrent, asr, based, grapheme, lstm","LSTM recurrent networks learn simple context-free and context-sensitive languages . In Proc. of the Conf. on Computer Vision and Pattern Recognition, pages 2625-2634, jun 2015. Douglas Eck and Jurgen Schmidhuber. From speech to letters - using novel neural network architecture for grapheme"
327,"handwriting, phoneme, recognition, classification, lstm","Alex Graves, Nicole Beringer, and Jurgen Schmidhuber. Rapid retraining on speech data with LSTM recurrent networks . In Proc. of the 23rd IASTED Int. Conf. on Modelling, Identification, and Control, Grindelwald, 2003. Alex Grav"
329,"memory, blstm, long-term, depen-dencies, lag, dynamics, problems","Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber. LSTM can solve hard long time lag problems . Neural computation, 9:1735-1780, 1997 ."
331,"short-term, handwriting, memory, networks, long, recognition","In Proc. of the 31st Int. Conf. on Machine Learning . Volume 32, pages 1863-1871, 2014. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton . ImageNet Classification with Deep Convolutional Neural Networks . In"
333,"networks, neural, network, time, intrusions","Ilya Sutskever, James Martens, and Quoc V Le. Sequence to Sequence learning with neural networks . In Proc. of the 4th Int. Conf. on Learning Representations, pages 1-11, 2016. Oriol Vinyals, Meire Fortunato, and"
335,"ieee, networks, neural, on, transactions","Martin Woellmer, Florian Eyben, Stephan Reiter, Bjorn Schuller, Cate Cox, Ellen Douglas-Cowie, and Roddy Cowie. Abandoning emotion classes Towards continuous emotion recognition with modelling of long-range dependencies . In Proc. of the Ann."
