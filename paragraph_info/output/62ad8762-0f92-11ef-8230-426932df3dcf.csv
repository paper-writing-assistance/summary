element_idx,summarized_text,keywords
4,"Compared to convolutional networks, Transformers enjoy the ability of referring to global features at each stage . The attention module brings higher computational overhead that obstructs the application of Transformers to process highresolution visual data . We then integrate the MSG token into a multi-scale architecture named MSG-","msg-transformer, visual, recognition"
6,"The past decade has witnessed the convolutional neural networks dominating the computer vision community . As one of the most popular models in deep learning, CNNs construct a hierarchical structure to learn visual features . In each layer, local features are aggregated using convolutions to produce features of the next layer .","convolutional, neural, language, natural, vision, communication, networks, process-, ing"
9,"The Transformer module works by using a token to formulate the feature at each spatial position . The features are then fed into self-attention computation and each token, according to the vanilla design, can exchange information with all the others at every single layer . To reduce computational costs, researchers propose to compute attention in local windows of the 2D","visual, additional, attention, cost, information, local"
13,"MSG tokens serve as the hub of information exchange . This brings two-fold benefits . First, our design is friendly to implementation since it does not create redundant copies of data like . Second, the flexibility of design is largely improved .","multi-scale, feature, design, tokens, extraction, multi-level, msg"
14,We instantiate MSG-Transformer as a straightforward case that the features of MSG tokens are shuffled and reconstructed with splits from different locations . This can effectively exchange information from local regions and delivered to each other in the next attention computation .,"msg-transformer, visual, recognition"
16,Vision Transformer computes attention in non-overlapped windows . Communications are achieved via the proposed MSG tokens . The proposed shuffle operation effectively exchanges information with negligible cost .,"vision, tokens, local-attention, transformer, msg"
17,"In experiments, MSG-Transformers show promising results on ImageNet classification, 84.0% Top-1 accuracy, and MS-COCO object detection, i.e., 52.8 mAP, which consistently outperforms recent state-of-the-art Swin Transformer .","swin, transformer, classification, imagenet"
20,"Convolutional Neural Networks CNNs have been a popular and successful algorithm in a wide range of computer vision problems . As AlexNet shows strong performance on ImageNet classification, starting the blooming development of CNNs . A series of subsequent methods emerge and persist in promoting CNN performance on vision tasks .","vision, recognition, backbone, scenarios, networks, cnn"
21,Vision Transformer Networks Transformers have been widely used in natural language processing . The core idea of Transformers lies in the self-attention mechanism . Some works integrate modules from CNNs into vision Transformer networks .,"natural, language, vision, processing, transformer, networks"
26,HaloNet and Swin Transformer propose to compute attention in a local window . To overcome the contradiction that non-overlapped windows lack communication while overlapped windows introduce additional memory/computation cost .,"token, windows, relations, local-global, msg"
32,"In vision Transformers , image features are commonly projected into patch tokens . Each local window is attached with one MSG token as X'w E R kw x W x R2 xxC Then a layer normalization is applied .","token, self-, multi-head, shuffle, attention, region, msg"
34,"Local Multi-head Self-Attention Different from previous vision Transformers, we compute selfattention within each local window . Taking a window of w x w for example, the attention is computed on the token sequence of X = , where tMSG denotes the MSG token associated","self-, window, attention, msg, local"
41,the relative position biases between patch tokens in B are taken from the bias parameter brel E R x according to the relative token distances . matrix B are computed as the same as the manner dealing with the token in .,"position, token, channel, biases, relative, patch, dimension"
43,"Exchanging Information by Shuffling MSG Tokens The MSG tokens allow us to exchange visual information flexibly . Here we instantiate an example using the shuffling operation, while we emphasize that the framework easily applies to other operations .","exchange, shuffling, tokens, information, msg"
47,"Extensions There are other ways of constructing and manipulating MSG tokens . For example, one can extend the framework SO that neighboring MSG tones overlap . One can program the propagation rule SO that the MSG . tokens are not fully-connected to each other .","mapping, identity, tokens, msg, shuffle-based"
53,"tokens are split into windows with the shape of w x w, and each window is attached with one MSG token, which has an equal channel number with the patch token . To obtain features under various spatial resolutions, we downsample features by merging both patch and MSG . Blocks under the","token, image, classification, msg-transformer, merging, block"
54,"MSGTransformer-T, -S and -B represent tiny, small, and base architectures with different channel numbers, attention head numbers and layer numbers . The window size is set as 7 for all architectures . We prefer deeper and narrower architecture scales than Swin Transformer .","scale, msg-transformer, architecture, variant"
62,"All the linear projection parameters are shared between patch and MSG tokens . All the parameters are shuffle regions, only taking 42C = 16C, i.e.,  0.0015M for the 96 input channel dimension .","projection, linear, tokens, parameters, msg"
63,"the huge feature matrix of patch tokens only needs to be window-partitioned once in a stage if the input images have a regular size . With MSG tokens assisting, cost from frequent 2D-to-1D matrix transitions can be saved, which causes additional latencies especially on computation-","patch, communication, tokens, region, msg, local"
78,"MSG-Transformer-T achieves 2.6 Top-1 accuracy promotion over DeiT-S with 0.8G smaller FLOPs . MSG - Transformer -B achieves an 84.0% top-1 accuracy, beating larger-resolution DeiTB by 0.9 with only 25.6%","swin, imagenet, transformer-b, transformer-t, transformer, transformer-s"
80,"The training and evaluation are performed based on the MMDetection toolkit . For training, we use the AdamW optimizer with 0.05 weight decay, 1 x 10-4 initial learning rate and a total batch size of 16 . The learning rate is decayed by 0.1 at the 27 and 33 ","msg-transformer, networks"
85,"MSG-Transformers achieve significantly better performance than CNN-based models . Swin Transformers still achieve significant promotion by 0.9, 0.7, 0.9 APbox .","swin, msg-transformers, transformers"
88,"Effects of MSG Tokens & Shuffle Operations We study the effects of both MSG tokens and shuffle operations, providing the results in Tab. 4. As shown in Row 1, the performance is promoted by 0.3 compared to that without both . The Top-1 accuracy drops to 0.9 .","tokens, shuffle, operations, msg"
93,"Input Parameters of MSG Tokens To further understand the role MSG tokens play in Transformers, we study impacts caused by parameters of input MSG tokens . In Row 2 of Tab. 5, we randomly re-initialize parameter of input CLS tokens for evaluation, which are learn","input, transformers, tokens, parameters, msg"
94,the proposed MSG tokens play a different role than conventional CLS tokens . The input parameters of their own matter little in latter information delivering as they absorb local features layer by layer via attention computing .,"cls, tokens, patch, conventional"
95,"Swin- and MSG-Transformer scales are evaluated as follows . As shown in Tab. 6, two scales were evaluated where one is shallow and wide . One is deep and narrow with 64 dimension and blocks .","scales, msg-transformer, network"
103,"In Swin Transformer, each patch token is involved in two different windows between layers . This requires a wider channel dimension with more attention heads to support the variety . MSGTransformer uses MSG tokens to extract window-level information and transmit to patch tokens .","patch, deeper-narrower, tokens, scale"
104,"Shuffle Region Sizes We study the impacts of shuffle region sizes on the final performance . As shown in Tab. 7, the final accuracy increases with the region enlarged . It is reasonable that larger receptive fields are beneficial for tokens capturing substantial spatial information.","size, shuffle, final, accuracy, region"
105,"Attention Map Visualization of MSG Tokens We visualize attention maps computed between each MSG token and its associated patch tokens within the local window in different blocks . The local window size to the token features is constant, i.e. 7 in our settings .","window, tokens, attention, maps, local"
108,"This paper proposes MSG-Transformer, a novel Transformer architecture . The core innovation is to introduce the MSG token which serves as the hub of collecting and propagating information . Our approach achieves competitive performance on standard image classification and object detection tasks with reduced implementation difficulty .","exchange, msg-transformer, infor-, architecture, mation, transformer"
109,"Limitations We would analyze limitations from the perspective of the manipulation type for MSG tokens . Although shuffling is an efficient communication operation, the specificity of shuffled tokens is not so well .","manipulation, type, msg, shuffling"
115,"In CVPR, 2018. 2, 6 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille . Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected cr","atrous, convolution, vision, bridge, transformer"
116,"ICLR, 2021. 1, 2, 3, 6 Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu, and Xinggang Wang. Densely connected search space for more flexible neural architecture search . In CVPR, 2020. 2, 6","mobile, image, vision, recognition, cvpr"
118,"In ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 11 Zili Liu, Tu Zheng, Guodong Xu, Haifeng Liu . Training-time-friendly network for real-time object detection . In CVPR, 2016. 1, 2, 6 Joseph Redmon, San","network, transformer, vision, design"
119,"ICCV, 2021. 2 Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In NeurIPS, 2017. 1, 2 We","mobile, image, vision, devices, transformer"
125,"MSG-Transformers in main text do not include depth-wise convolutions to make the designed model a purer Transformer . As in Tab. 8, MSG -Tdwc shows promising performance with low FLOPs .","depth-wise, msg-tdwc, transformer, convolutions"
129,"In Swin, each patch token obtains information from other patch tokens in different windows . In MSG, information from one window is summarized by a MSG token . This manner eases the difficulty of patches obtaining information from others .","information, windows, exchange, msg"
133,"How to manipulate MSG tokens is not limited to the adopted shuffle operation . We study two additional manipulations, namely the 'average' and 'shift'","msg, tokens, shuffle, shifted"
